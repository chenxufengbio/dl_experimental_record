# 特征提取

目前主要使用的方法是**方法七**

## 方法合集

**参考文献**：

[Deep Learning for Genomics: A Concise Overview](https://arxiv.org/abs/1802.00810) 3.1Model Interpretation

**方法一：**

 Zeiler and Fergus (2014) gave insights into the function of intermediate features by mapping hidden layers back
to input through deconvolution

Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pages 818–833. Springer, 2014.

**方法二：**

 Simonyan et al. (2013) linearly approximate the network by first-order Taylor expansion and obtained Saliency Maps from a ConvNet by projecting back from the dense layers of the network. People also searched for an understanding of genes by deep networks

Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutionalnetworks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013.

**方法三：**

 Denas and Taylor (2013) managed to pass the model knowledge back into the input space through the inverse of the activa-
tion function, so that biologically-meaningful patterns can be highlighted.

Olgert Denas and James Taylor. Deep modeling of gene expression regulation in an ery-thropoiesis model. In Representation Learning, ICML Workshop, 2013.

**方法四：**saliency map（显著性图)

[一阶泰勒展开和梯度下降的推导](https://zhuanlan.zhihu.com/p/82757193)

https://mathpretty.com/10683.html

 Lanchantin et al. (2016b, Dashboard) adopted Saliency Maps to measure nucleotide importance. Their work provided a series of visualization techniques to detect motifs, or sequence patterns from deep learning models, and went further to discuss about the features extracted by CNNs and RNNs

Jack Lanchantin, Ritambhara Singh, Beilun Wang, and Yanjun Qi. Deep gdashboard: Visualizing and understanding genomic sequences using deep neural networks. CoRR, abs/1608.03644, 2016b. URL http://arxiv.org/abs/1608.03644.

![image-20210322175439956](https://img.imgdb.cn/item/6092930fd1a9ae528f4983de.png)

**which which parts of the sequence are most influential for the classification?**

sequence：X0  

length：|X0|

class : c

score: Sc(X0) 衡量碱基重要性的指标，根据碱基变化对Sc的影响来表示重要性

![image-20210322190357603](https://img.imgdb.cn/item/60929328d1a9ae528f4a60ad.png)

w是神经网络反向传播的一步

This derivative is simply one step of backpropagation in the DNN model, and is therefore easy to compute.

 We do a pointwise multiplication of the saliency map with the one-hot encoded sequence to get the derivative values for the actual nucleotide characters of the sequence (A,T,C, or G) so we can see the influence of the character at each position on the output score.

 Finally, we take the element-wise magnitude of the resulting derivative vector to visualize how important each character is regardless of derivative direction.

**Saliency.py**

```python
import torch
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import re
from sklearn.preprocessing import LabelEncoder,OneHotEncoder
import torchvision
from model_dna import NetDeepHistone

model= NetDeepHistone()
model.load_state_dict(torch.load('D:/桌面/model.txt',map_location=torch.device('cpu')))



#停止梯度更新
for param in model.parameters():
    param.requires_grad = False


#读入X，y
X = pd.read_csv('D:/桌面/data_motif.txt',header = None)
Y = pd.read_csv('D:/桌面/target_motif.txt',header = None)



#转成onehot
# function to convert a DNA sequence string to a numpy array
# converts to lower case, changes any non 'acgt' characters to 'n'
def string_to_array(my_string):
    my_string = my_string.lower()
    my_string = re.sub('[^acgt]', 'z', my_string)
    my_array = np.array(list(my_string))
    return my_array

label_encoder = LabelEncoder()
label_encoder.fit(np.array(['a','c','g','t','z']))

def one_hot_encoder(my_array):
    integer_encoded = label_encoder.transform(my_array)
    onehot_encoder = OneHotEncoder(sparse=False, dtype=int)
    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)
    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)
    return onehot_encoded

one_hot_matrix = []
for i in range(2):
    one_hot_matrix.append(one_hot_encoder(string_to_array(X[0][i])))

print(one_hot_matrix)

X_tensor = torch.Tensor(one_hot_matrix)
X_tensor = X_tensor.reshape(len(X),4,500)
print(X_tensor.shape)


#开启测试模式
model.eval()

#确定梯度
X_tensor.requires_grad_()

y_tensor = torch.LongTensor([1,1])
print(y_tensor.shape)

saliency = None    
logits = model.forward(X_tensor)
print(logits)
logits = logits.gather(1, y_tensor.view(-1, 1)).squeeze()
logits.backward(torch.FloatTensor([1., 1.]))
saliency = abs(X_tensor.grad.data)
print(saliency[0])
PWM = pd.DataFrame(saliency[0].numpy())
print(PWM)
PWM.to_csv('C:/Users/DELL/Documents/PWM1.txt',sep='\t',index=False)
```

**ggseqlogo**

```R
> library(ggseqlogo)
> matrix <- read.table("./PWM.txt",header = T)
> row.names(matrix) <- c("A","T","G","C")
> matrix <- as.matrix(matrix)
> motif <- matrix[,245:254]
> ggseqlogo(motif)
```



**方法五：**Mutation Map

 Alipanahi et al. (2015) visualized the sequence specificities deter- mined by DeepBind through mutation maps that indicate the effect of variations on bound sequences.

Babak Alipanahi, Andrew Delong, Matthew T. Weirauch, and Brendan J. Frey. Predicting the sequence specificities of dna- and rna-binding proteins by deep learning. Nat Biotech, 33(8):831–838, Aug 2015.  ISSN 1087-0156.  URL http://dx.doi.org/10.1038/nbt.3300. Computational Biology

**Mutation map**

1.通过给定序列碱基的高度来表示在DeepBind分析中的重要性

2.heat map （4 * n）（n是序列长度）表示每个可能出现的突变对结合能力的影响

![Figure 4](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fnbt.3300/MediaObjects/41587_2015_Article_BFnbt3300_Fig4_HTML.jpg)

**绘制方法**

通过计算原始序列（参考基因组）的binding score p(s) 然后依次让每个位点都发生突变生成新的序列，计算p(s)^ 

ΔSij = （p(s)^ - p(s)) * max(0,p(s),p(s)^)



**方法六：**[Deepmotif](file:///D:/Zotero%E6%96%87%E7%8C%AE/Deepmotif.pdf)

https://github.com/bakirillov/deepmotif4pytorch

Method for motif generation via class optimization. We find the input matrix which corresponds to the highest locally optimum TFBS probability via backpropagation, and generate a PWM from the matrix.

![](https://img.imgdb.cn/item/60929348d1a9ae528f4b8b6a.png)

**方法七：**

![](https://pic.imgdb.cn/item/609521cdd1a9ae528f17a18b.png)



1.将预训练好的模型调整为预测模式，停止梯度更新

2.将预测集投入到模型当中进行计算，然后输出Conv1(第一层卷积层)对应每个Kernel的位置权重矩阵（PWMs）

3.然后将每个样本的矩阵叠加，设置阈值进行判断将保守性矩阵输出

4.将PWMs矩阵的形式转换为meme的形式

5.使用TomTom工具进入JASPAR数据库进行相似性比对

**参考文献：**

[DeepHistone: a deep learning approach to predicting histone modifications](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-5489-4)

[TSPTFBS: a Docker image for trans-species prediction of transcription factor binding sites in plants](https://academic.oup.com/bioinformatics/article/37/2/260/6069568)

github：https://github.com/liulifenyf/TSPTFBS

**get_PWM.py**

```python
import numpy as np
import pandas as pd
import re
import torch
from model_dna import NetDeepHistone

def NUMPY2STRING(input_array):

    # convert numpy to string for 2 dimension numpy array.

    output_str = ""

    for i in range(input_array.shape[0]):
        for j in range(input_array.shape[1]):
            output_str = output_str + str(input_array[i, j]) + "\t"
        output_str += "\n"

    return output_str


def matrix2meme(pwm_txt_name, pwm_meme_name, pwm_len):

    # convert PWM to meme format used in tomtom.

    write_ofl = open(pwm_meme_name, "w")  #####

    write_ofl.write("MEME version 5.0.4\n\n")
    write_ofl.write("ALPHABET= ACGT\n\n")
    write_ofl.write("strands: + -\n\n")
    write_ofl.write("Background letter frequencies\n")
    write_ofl.write("A 0.25 C 0.25 G 0.25 T 0.25\n\n")

    read_ofl = open(pwm_txt_name)  #####
    oflst = read_ofl.readlines()
    read_ofl.close()

    count = 0
    for line in oflst:
        line = line.strip()
        if re.search(">", line):
            write_ofl.write("\n")
            write_ofl.write("MOTIF" + "\t" + "filter" + str(count + 1) + "\n")
            write_ofl.write("letter-probability matrix: alength= 4 w= " +
                            str(pwm_len) + "\n")  #######
            count += 1
        else:
            write_ofl.write(line + "\n")
    write_ofl.close()
    
# 用于存储pwm矩阵的文件
input_path = "onehot_test.npy" # DNA one-hot encoding with .npy foramt.
model_path = "D:/桌面/model.txt" # trained model path
X = np.load(input_path) 
X = torch.Tensor(X).reshape(len(X),4,500)

pwm_meme_name = "pwm.meme"
pwm_txt_name = "pwm.txt"
model = NetDeepHistone()
model.load_state_dict(torch.load(model_path,map_location=torch.device('cpu')))
 # check out the name of first conv_layer



parm = {}
for name,parameters in model.seq_map.named_parameters():
    print(name,':',parameters.size())
    parm[name]=parameters.detach().numpy()



'''According to your model, return the output of first conv_layer. 
    please change the "outputs" para according to your model construction.
    And the bias and weights of first conv_layer.'''
WEIGHTS, BIAS = parm['conv1.0.weight'],parm['conv1.0.bias']
WEIGHTS = WEIGHTS.squeeze()
INSTANCE_LENGTH = WEIGHTS.shape[2] # return the length of filter
print(WEIGHTS.shape)

conv_out = model.seq_map.get_conv1(X)
conv_out = conv_out.squeeze()

print(conv_out.shape)



'''
layer_output = Model(inputs=model.input, outputs=model.layers[1].output)
conv_out = layer_output.predict(X)
conv_out = conv_out.squeeze()# 根据模型卷积核数目调整最后一位
print(conv_out.shape)
'''


# 用于提取pwm矩阵
'''
WEIGHTS(128,4,9)(Kernels_num,(width,length))
BIAS(128)
THRESHOLD
INSTANCE_FILTERED_NUMBER
INSTANCE_FILTERED
INSTANCE_LENGTH

'''
motif_ofl = open(pwm_txt_name, "w")

for i in range(WEIGHTS.shape[0]):
    one_filter_weight = WEIGHTS[i,: ,: ]
    THRESHOLD = (np.sum(np.max(one_filter_weight, 1))+ BIAS[i]) * 0.9
    #阈值可以调整0.5-1.0
    
    model_c = conv_out[:,i,:] - BIAS[i]

    position_m = np.where(model_c >= THRESHOLD)
    print(position_m[1].shape[0])
    #(samples,position)
    INSTANCE_FILTERED_NUMBER = position_m[1].shape[0]
    print(INSTANCE_FILTERED_NUMBER)
    INSTANCE_FILTERED = np.zeros(
        [INSTANCE_FILTERED_NUMBER, 4, INSTANCE_LENGTH])
    for j in range(INSTANCE_FILTERED_NUMBER):
        if position_m[1][j] <= 491:
            INSTANCE_FILTERED[j] = X[position_m[0][j], : , 
                                 (position_m[1][j]):(position_m[1][j] +
                                                   INSTANCE_LENGTH)]
    pwm_matrix = np.mean(INSTANCE_FILTERED, 0)
    print(pwm_matrix.shape)
    pwm_sum = np.sum(pwm_matrix, 0)
    for col in range(pwm_matrix.shape[0]):
        pwm_matrix[col,:]  = pwm_matrix[col ,:] / pwm_sum
    outline = ">MOTIF" + str(i + 1) + "\n"
    motif_ofl.write(outline)
    outline = NUMPY2STRING(pwm_matrix.T)
    motif_ofl.write(outline)
    motif_ofl.flush()
motif_ofl.close()
print("PWM-txt Done")

matrix2meme(pwm_txt_name, pwm_meme_name, INSTANCE_LENGTH)
print("PWM-meme Done")
```





## 结果展示

**方法七**

![](https://pic.imgdb.cn/item/609b5375d1a9ae528f8907ad.png)





![](https://pic.imgdb.cn/item/609b53b4d1a9ae528f8a6ec1.jpg)






## 构建模型

### 构建基本模型（pytorch）

初步实验在恒源云服务器上进行

#### **导入模块**

```python
import numpy as np
import pandas as pd
import torch
from torch.utils import data
import time
from torch import nn, optim
import sys
```

#### **设置GPU**

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

#### **数据导入及格式转换**

```python
data_np = np.load('./one_hot.npy')
target_pd =pd.read_csv('./target6.txt',sep = '\n')
#转换成tensor
data_tensor = torch.from_numpy(data_np)
target_array = np.array(target_pd)
target_tensor = torch.tensor(target_array)
#转置成(4，500)
data_tensor = data_tensor.reshape(10000,4,500)
#转换成float（后续模型输入需要）
data_tensor = data_tensor.float()
target_tenor = target_tensor.float()
```

#### **构建训练集和验证集**（TensorDataset类和DataLoader类）

```python
class TensorDataset(data.Dataset):
    """Dataset wrapping data and target tensors.
    Each sample will be retrieved by indexing both tensors along the first
    dimension.
    Arguments:
        data_tensor (Tensor): contains sample data.
        target_tensor (Tensor): contains sample targets (labels).
    """

    def __init__(self, data_tensor, target_tensor):
        assert data_tensor.size(0) == target_tensor.size(0)
        self.data_tensor = data_tensor
        self.target_tensor = target_tensor

    def __getitem__(self, index):
        return self.data_tensor[index], self.target_tensor[index]

    def __len__(self):
        return len(self.data_tensor)
  
```

```python
train_data = TensorDataset(data_tensor[0:9000], target_tensor[0:9000])
val_data = TensorDataset(data_tensor[9000:10000], target_tensor[9000:10000])
train_dataloader = data.DataLoader(train_data,batch_size = 200,
                        shuffle=True,num_workers=0)
val_dataloader = data.DataLoader(val_data,batch_size = 200,
                        shuffle=False,num_workers=0)
```



### DeepSEA

为防止过拟合增大泛化能力，在卷积层后加入BN层（归一化）

```python
class DeepSEA(nn.Module):
    def __init__(self, sequence_length=500, n_genomic_features=2):
        """
        Parameters
        ----------
        sequence_length : int
        n_genomic_features : int
        """
        super(DeepSEA, self).__init__()
        conv_kernel_size = 8
        pool_kernel_size = 4

        self.conv_net = nn.Sequential(
            nn.Conv1d(4, 320, kernel_size=conv_kernel_size),
            nn.BatchNorm1d(320),
            nn.ReLU(inplace=True),
            nn.MaxPool1d(
                kernel_size=pool_kernel_size, stride=pool_kernel_size),
            nn.Dropout(p=0.2),

            nn.Conv1d(320, 480, kernel_size=conv_kernel_size),
            nn.BatchNorm1d(480),
            nn.ReLU(inplace=True),
            nn.MaxPool1d(
                kernel_size=pool_kernel_size, stride=pool_kernel_size),
            nn.Dropout(p=0.2),

            nn.Conv1d(480, 960, kernel_size=conv_kernel_size),
            nn.BatchNorm1d(960),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5))

        reduce_by = conv_kernel_size - 1
       
        self.n_channels = int(
            np.floor(
                (np.floor(
                    (sequence_length - reduce_by) / pool_kernel_size)
                 - reduce_by) / pool_kernel_size)
            - reduce_by)
        self.classifier = nn.Sequential(
            nn.Linear(960 * self.n_channels, n_genomic_features),
            nn.ReLU(inplace=True),
            nn.Linear(n_genomic_features, n_genomic_features),
            nn.Sigmoid())

    def forward(self, x):
        """Forward propagation of a batch.
        """
        out = self.conv_net(x)
        reshape_out = out.view(out.size(0), 960 * self.n_channels)
        predict = self.classifier(reshape_out)
        return predict
```

**创建训练函数**

损失函数：CrossEntropyLoss

```python
def train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):
    net = net.to(device)
    print("training on ", device)
    loss = torch.nn.CrossEntropyLoss()
    for epoch in range(num_epochs):
        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()
        for X, y in train_iter:
            X = X.to(device)
            y = y.to(device)
            y_hat = net(X)
            y = y.squeeze()
            l = loss(y_hat, y)
            optimizer.zero_grad()
            l.backward()
            optimizer.step()
            train_l_sum += l.cpu().item()
            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()
            n += y.shape[0]
            batch_count += 1
        test_acc = evaluate_accuracy(test_iter, net)
        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'
              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))
```

**创建准确率计算函数**

```python
def evaluate_accuracy(data_iter, net, device=None):
    if device is None and isinstance(net, torch.nn.Module):
        device = list(net.parameters())[0].device
    acc_sum, n = 0.0, 0
    with torch.no_grad():
        for X, y in data_iter:
            if isinstance(net, torch.nn.Module):
                net.eval() # 评估模式, 这会关闭dropout
                acc_sum += (net(X.to(device)).argmax(dim=1)==y.to(device).squeeze()).float().sum().cpu().item()
                net.train() # 改回训练模式
            else: 
                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数
                    # 将is_training设置成False
                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() 
                else:
                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() 
            n += y.shape[0]
    return acc_sum / n

```

**开启训练**

| 超参数     |          |
| ---------- | -------- |
| lr         | 学习率   |
| num_epochs | 训练批次 |

优化器：Adam

优化器：SGD

```python
lr, num_epochs = 0.001, 10
optimizer = torch.optim.SGD(net.parameters(), lr=lr)
train(net, train_dataloader, val_dataloader, 256, optimizer, device, num_epochs)
```

**训练结果可视化**

更新train函数

恒源云

```python
logger.log_value('loss', train_l_sum/batch_count,  epoch*len(train_iter) + batch_count)
logger.log_value('train_acc', 100. *train_acc_sum / n, epoch*len(train_iter) + batch_count)
logger.log_value('val_acc',test_acc,epoch)
```

**TensorBoard_logger**

```python
from tensorboard_logger import Logger
logger = Logger(logdir="./tb_logs", flush_secs=10)#设置输出的log文件位置

def train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs):
    net = net.to(device)
    print("training on ", device)
    loss = torch.nn.CrossEntropyLoss()
    for epoch in range(num_epochs):
        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()
        for X, y in train_iter:
            X = X.to(device)
            y = y.to(device)
            y_hat = net(X)
            y = y.squeeze()
            
            l = loss(y_hat, y)
            optimizer.zero_grad()
            l.backward()
            optimizer.step()
            train_l_sum += l.cpu().item()
            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()
            n += y.shape[0]
            batch_count += 1


        test_acc = evaluate_accuracy(test_iter, net)
        logger.log_value('loss', train_l_sum/batch_count,  epoch*len(train_iter) + batch_count)
        logger.log_value('train_acc', 100. *train_acc_sum / n, epoch*len(train_iter) + batch_count)
        logger.log_value('val_acc',test_acc,epoch)
        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'
              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))

```







#### **组织代码框架**

/public/home/xwli/xwzhang/deeplearningDATA/pytorch_deepsea

**文件组织架构**

```text
├── checkpoints/
├── data/
│   ├── __init__.py
│   ├── dataset.py
│  
├── models/
│   ├── __init__.py
│   ├── DeepSEA.py
│   
│  
└── utils/
│   ├── __init__.py
│   └── visualize.py
├── config.py
├── main.py
├── README.md
```

**数据加载模块**

```python
#dataset.py
from torch.utils import data 

class TensorDataset(data.Dataset):
    """Dataset wrapping data and target tensors.
    Each sample will be retrieved by indexing both tensors along the first
    dimension.
    Arguments:
        data_tensor (Tensor): contains sample data.
        target_tensor (Tensor): contains sample targets (labels).
    """

    def __init__(self, data_tensor, target_tensor):
        assert data_tensor.size(0) == target_tensor.size(0)
        self.data_tensor = data_tensor
        self.target_tensor = target_tensor

    def __getitem__(self, index):
        return self.data_tensor[index], self.target_tensor[index]

    def __len__(self):
        return len(self.data_tensor) 
        

```

**模型定义模块**

```python
# coding: utf-8


import torch
import time


class BasicModule(torch.nn.Module):
    '''
    封装了nn.Module，主要提供save和load两个方法
    '''

    def __init__(self,opt=None):
        super(BasicModule,self).__init__()
        self.model_name = str(type(self)) # 模型的默认名字

    def load(self, path):
        '''
        可加载指定路径的模型
        '''
        self.load_state_dict(torch.load(path))

    def save(self, name=None):
        '''
        保存模型，默认使用“模型名字+时间”作为文件名，
        如AlexNet_0710_23:57:29.pth
        '''
        if name is None:
            prefix = 'checkpoints/' + self.model_name + '_'
            name = time.strftime(prefix + '%m%d_%H:%M:%S.pth')
        torch.save(self.state_dict(), name)
        return name


```

```python
#__init__.py
from .DeepSEA import DeepSEA
#from .new_module import NewModule
```

```python
# coding: utf-8

import numpy as np
from torch import nn
from .BasicModule import BasicModule

class DeepSEA(nn.Module):
    def __init__(self, sequence_length=500, n_genomic_features=2):
        """
        Parameters
        ----------
        sequence_length : int
        n_genomic_features : int
        """
        super(DeepSEA, self).__init__()
        conv_kernel_size = 8
        pool_kernel_size = 4

        self.conv_net = nn.Sequential(
            nn.Conv1d(4, 320, kernel_size=conv_kernel_size),
            nn.ReLU(inplace=True),
            nn.MaxPool1d(
                kernel_size=pool_kernel_size, stride=pool_kernel_size),
            nn.Dropout(p=0.2),

            nn.Conv1d(320, 480, kernel_size=conv_kernel_size),
            nn.ReLU(inplace=True),
            nn.MaxPool1d(
                kernel_size=pool_kernel_size, stride=pool_kernel_size),
            nn.Dropout(p=0.2),

            nn.Conv1d(480, 960, kernel_size=conv_kernel_size),
            nn.ReLU(inplace=True),
            nn.Dropout(p=0.5))

        reduce_by = conv_kernel_size - 1

        self.n_channels = int(
            np.floor(
                (np.floor(
                    (sequence_length - reduce_by) / pool_kernel_size)
                 - reduce_by) / pool_kernel_size)
            - reduce_by)
        self.classifier = nn.Sequential(
            nn.Linear(960 * self.n_channels, n_genomic_features),
            nn.ReLU(inplace=True),
            nn.Linear(n_genomic_features, n_genomic_features),
            nn.Sigmoid()
        )

    def forward(self, x):
        """Forward propagation of a batch.
        """
        out = self.conv_net(x)
        reshape_out = out.view(out.size(0), 960 * self.n_channels)
        predict = self.classifier(reshape_out)
        return predict



```

**工具函数**

```python
#coding:utf8
#visualize.py
import visdom
import time
import numpy as np

class Visualizer(object):
    '''
    封装了visdom的基本操作，但是你仍然可以通过`self.vis.function`
    或者`self.function`调用原生的visdom接口
    比如 
    self.text('hello visdom')
    self.histogram(t.randn(1000))
    self.line(t.arange(0, 10),t.arange(1, 11))
    '''

    def __init__(self, env='default', **kwargs):
        self.vis = visdom.Visdom(env=env, **kwargs)

       # 画的第几个数，相当于横坐标
       # 比如（’loss',23） 即loss的第23个点
        self.index = {}
        self.log_text = ''
    def reinit(self, env='default', **kwargs):
        '''
        修改visdom的配置
        '''
        self.vis = visdom.Visdom(env=env, **kwargs)
        return self

    def plot_many(self, d):
        '''
        一次plot多个
        @params d: dict (name, value) i.e. ('loss', 0.11)
        '''
        for k, v in d.iteritems():
            self.plot(k, v)

    def img_many(self, d):
        for k, v in d.iteritems():
            self.img(k, v)

    def plot(self, name, y, **kwargs):
        '''
        self.plot('loss', 1.00)
        '''
        x = self.index.get(name, 0)
        self.vis.line(Y=np.array([y]), X=np.array([x]),
                     win=unicode(name),
                     opts=dict(title=name),
                     update=None if x == 0 else 'append',
                     **kwargs
                     )
        self.index[name] = x + 1

    def img(self, name, img_, **kwargs):
        '''
        self.img('input_img', t.Tensor(64, 64))
        self.img('input_imgs', t.Tensor(3, 64, 64))
        self.img('input_imgs', t.Tensor(100, 1, 64, 64))
        self.img('input_imgs', t.Tensor(100, 3, 64, 64), nrows=10)
        '''
        self.vis.images(img_.cpu().numpy(),
                      win=unicode(name),
                      opts=dict(title=name),
                      **kwargs
                      )

    def log(self, info, win='log_text'):
        '''
        self.log({'loss':1, 'lr':0.0001})
        '''

        self.log_text += ('[{time}] {info} <br>'.format(
                           time=time.strftime('%m%d_%H%M%S'),\
                           info=info))
        self.vis.text(self.log_text, win)

    def __getattr__(self, name):
        '''
        self.function 等价于self.vis.function
        自定义的plot,image,log,plot_many等除外
        '''
        return getattr(self.vis, name)

                                                                      
```

**配置文件**

```python
#config.py
class DefaultConfig(object):
    env = 'default' # visdom 环境
    model = 'DeepSEA' # 使用的模型，名字必须与models/__init__.py中的名字一致

    train_data_root = './data/one_hot_10w_sh.npy' # 训练集存放路径
    test_data_root = './data/test1' # 测试集存放路径
    target_data_root = './data/target_10w_sh.txt'
    load_model_path = 'checkpoints/model.pth' # 加载预训练的模型的路径，为None代表不加载

    batch_size = 256 # batch size
    use_gpu = True # use GPU or not
    num_workers = 2 # how many workers for loading data
    print_freq = 20 # print info every N batch
    #debug_file = '/tmp/debug' # if os.path.exists(debug_file): enter ipdb
    result_file = 'result.csv'

    num_epochs = 10
    lr = 0.01 # initial learning rate
    lr_decay = 0.95 # when val_loss increase, lr = lr*lr_decay
    weight_decay = 1e-4 # 损失函数
def parse(self, kwargs):
        '''
        根据字典kwargs 更新 config参数
        '''
        # 更新配置参数
        for k, v in kwargs.items():
            '''
            if not hasattr(self, k):
                # 警告还是报错，取决于你个人的喜好
               
                warnings.warn("Warning: opt has not attribut %s" %k)
            '''
            setattr(self, k, v)

        # 打印配置信息  
        print('user config:')
        for k, v in self.__class__.__dict__.items():
            if not k.startswith('__'):
                print(k, getattr(self, k))

DefaultConfig.parse = parse
opt =DefaultConfig()
```

**主函数**

```python
#main.py
from config import opt
import os
import torch
from torch.utils import data
import models
import numpy as np
import pandas as pd
from data.dataset import TensorDataset
from torch.utils.data import DataLoader
#from torch.autograd import Variable
#from torchnet import meter
#from utils.visualize import Visualizer
#from tqdm import tqdm
import time




def evaluate_accuracy(data_iter, net, device=None):
    if device is None and isinstance(net, torch.nn.Module):
        # 如果没指定device就使用net的device
        device = list(net.parameters())[0].device
    acc_sum, n = 0.0, 0
    with torch.no_grad():
        for X, y in data_iter:
            '''
            if isinstance(net, torch.nn.Module):
                net.eval() # 评估模式, 这会关闭dropout
                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()
                net.train() # 改回训练模式
            else: 
                if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数
                    # 将is_training设置成False
                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() 
                else:
                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()
            '''
            acc_sum += (net(X.to(device)).argmax(dim=1) == (y.to(device).squeeze())).float().sum().cpu().item()
            n += y.shape[0]

    return acc_sum / n



def train(**kwargs):
    opt.parse(kwargs)
    #vis = Visualizer(opt.env)

    model = getattr(models, opt.model)()

    '''
    if opt.load_model_path:
        model.load(opt.load_model_path)
    if opt.use_gpu: model.cuda()
    '''
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print("training on ", device)

    data_np = np.load(opt.train_data_root)
    data_tensor = torch.from_numpy(data_np)

    target_pd =pd.read_csv(opt.target_data_root,sep = '\n',header = None)
    target_array = np.array(target_pd)
    target_tensor = torch.tensor(target_array)

    data_tensor = data_tensor.reshape(100000,4,500)
    data_tensor = data_tensor.float()
    target_tenor = target_tensor.float()

    train_data = TensorDataset(data_tensor[0:99000], target_tensor[0:99000])
    val_data = TensorDataset(data_tensor[99000:100000], target_tensor[99000:100000])
    train_dataloader = data.DataLoader(train_data,batch_size = 200,
                        shuffle=True,num_workers=0)
    val_dataloader = data.DataLoader(val_data,batch_size = 200,
                        shuffle=False,num_workers=0)

    loss = torch.nn.CrossEntropyLoss()
    lr = opt.lr
    optimizer = torch.optim.SGD(model.parameters(),
                                lr=lr,
                                weight_decay = opt.weight_decay)




    for epoch in range(opt.num_epochs):
        model = model.to(device)
        train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time()
        for X, y in train_dataloader:
            X = X.to(device)
            y = y.to(device)
            y_hat = model(X)
            y = y.squeeze()
            l = loss(y_hat, y)
            optimizer.zero_grad()
            l.backward()
            optimizer.step()
            train_l_sum += l.cpu().item()
            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()
            n += y.shape[0]
            batch_count += 1
        test_acc = evaluate_accuracy(val_dataloader, model)
        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'
              % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))


        #model.save()

def val(model, dataloader):
    '''
    计算模型在验证集上的准确率等信息，用以辅助训练
    '''
    pass

def test(**kwargs):
    '''
    测试（inference）
    '''
    pass

def help():
    '''
    打印帮助的信息 
    '''
    print('help')

if __name__=='__main__':
    import fire
    fire.Fire()

```







单个组蛋白修饰数据集构建

组蛋白修饰文献阅读















### DeepHistone

https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-5489-4#Sec9

![Fig. 1](https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12864-019-5489-4/MediaObjects/12864_2019_5489_Fig1_HTML.png)



<img src="https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12864-019-5489-4/MediaObjects/12864_2019_5489_Fig2_HTML.png" alt="Fig. 2" style="zoom:50%;" />

DenseNet

labels onehot encoder

可以将此函数加入OneHotEncoder

```python
import numpy as np
def dense_to_one_hot(labels_dense, num_classes):
    """Convert class labels from scalars to one-hot vectors."""
    num_labels = labels_dense.shape[0]
    index_offset = np.arange(num_labels) * num_classes
    labels_one_hot = np.zeros((num_labels, num_classes))
    labels_one_hot.flat[index_offset+labels_dense.ravel()] = 1
    return labels_one_hot

labels_dense = np.array([0,1,2,3,4]) 
num_classes  = 5
dense_to_one_hot(labels_dense,num_classes)


```

继续改进target_mark : 将组蛋白修饰（m）与数字标签对应，同时**改良每种组蛋白的标记方法**

npz封装：将npz封装也加入OneHotEncoder，加入到make

```python
np.savez('C:/Users/12394/PycharmProjects/Spyder/data.npz',a = a, b = b)
```

bash脚本

1.将5种组蛋白修饰阳性样本集中在一个数据集

2.reshape(len(data),1,4,500)

3.先拿一个品种制作数据集

labels_make.sh

OneHotEncoder3.0.py完成

加入删除（n,1,6)中的第一列，变为（n,1,5)



```shell
bsub -q high -e 12.err -o 12.out 'python3 scripts/py_scripts/OneHotEncoder3.0.py -i dataset_ex1_10w.txt -c make -m ex1 -n 10w' 
```

| Keys                          | DNA seq              | labels       |
| ----------------------------- | -------------------- | ------------ |
| (nums,)                       | (nums, 1 , 4 , 500 ) | (nums ,1, 5) |
| species_histone_chr_start_end | onehot               | onehot       |
|                               |                      |              |



#### **DNA-only**

```

```

geno_sub3.0.py(加上keys)

.npz (['keys', 'dna', 'labels'])

**DNase**（openness score）

![image-20210331205505975](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210331205505975.png)

**model_dna.py**

https://ieeexplore.ieee.org/document/8099726

三个模块：DNA module ，DNase module， joint module

DNA and DNase module： **densely connected** convolutional neural network

joint module： distinguish histone modification sites of a marker from those of other markers

```python
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.autograd import Variable
from sklearn import metrics
from torch.optim import  Optimizer
import math 
from torch.nn.parameter import Parameter

class BasicBlock(nn.Module):
	def __init__(self, in_planes, grow_rate,):
		super(BasicBlock, self).__init__()
		self.block = nn.Sequential(
			nn.BatchNorm2d(in_planes),
			nn.ReLU(),
			nn.Conv2d(in_planes, grow_rate, (1,9), 1, (0,4)),
			#nn.Dropout2d(0.2)
		)
	def forward(self, x):
		out = self.block(x)
		return torch.cat([x, out],1)

class DenseBlock(nn.Module):
	def __init__(self, nb_layers, in_planes, grow_rate,):
		super(DenseBlock, self).__init__()
		layers = []
		for i in range(nb_layers):
			layers.append(BasicBlock(in_planes + i*grow_rate, grow_rate,))
		self.layer = nn.Sequential(*layers)
	def forward(self, x):
		return self.layer(x)


class ModuleDense(nn.Module):
	def __init__(self):
		super(ModuleDense, self).__init__()


		self.conv1 = nn.Sequential(
			nn.Conv2d(1,128,(4,9),1,(0,4)),
			#nn.Dropout2d(0.2),
			)
	
		self.block1 = DenseBlock(3, 128, 128)	
		self.trans1 = nn.Sequential(
			nn.BatchNorm2d(128+3*128),
			nn.ReLU(),
			nn.Conv2d(128+3*128, 256, (1,1),1),
			#nn.Dropout2d(0.2),
			nn.MaxPool2d((1,4)),
		)
		self.block2 = DenseBlock(3,256,256)
		self.trans2 = nn.Sequential(
			nn.BatchNorm2d(256+3*256),
			nn.ReLU(),
			nn.Conv2d(256+3*256, 512, (1,1),1),
			#nn.Dropout2d(0.2),
			nn.MaxPool2d((1,4)),
		)
		self.out_size = 500 // 4 // 4  * 512

	def forward(self, seq):
		n, h, w = seq.size()
		
		seq = seq.view(n,1,4,w)
	
		out = self.conv1(seq)
		out = self.block1(out)
		out = self.trans1(out)
		out = self.block2(out)
		out = self.trans2(out)
		n, c, h, w = out.size()
		out = out.view(n,c*h*w) 
		return out


#NetDeepHistone类才是model
class NetDeepHistone(nn.Module):
	def __init__(self):
		super(NetDeepHistone, self).__init__()
		print('DeepHistone(Dense) is used.')
		self.seq_map = ModuleDense()
		self.seq_len = self.seq_map.out_size
		seq_len = self.seq_len

		self.linear_map = nn.Sequential(
			nn.Dropout(0.5),
			nn.Linear(int(seq_len),925),
			nn.BatchNorm1d(925),
			nn.ReLU(),
			#nn.Dropout(0.1),
			nn.Linear(925,5),
			nn.Sigmoid(),
		)

	def forward(self, seq):
		flat_seq = self.seq_map(seq)	
		out = self.linear_map(flat_seq)
		return out

#封装了训练验证和测试，保存加载模型的函数
class DeepHistone():
	def __init__(self,use_gpu,learning_rate=0.001):
		self.forward_fn = NetDeepHistone()
		self.criterion  = nn.BCELoss()
		self.optimizer  = optim.Adam(self.forward_fn.parameters(), lr=learning_rate, weight_decay = 0)
		self.use_gpu    = use_gpu
		if self.use_gpu : self.criterion,self.forward_fn = self.criterion.cuda(), self.forward_fn.cuda()

	def updateLR(self, fold):
		for param_group in self.optimizer.param_groups:
			param_group['lr'] *= fold

	def train_on_batch(self,seq_batch,lab_batch,): 
		self.forward_fn.train()
		seq_batch  = Variable(torch.Tensor(seq_batch))
		lab_batch  = Variable(torch.Tensor(lab_batch))
		if self.use_gpu: seq_batch, lab_batch = seq_batch.cuda(), lab_batch.cuda()
		output = self.forward_fn(seq_batch)
		loss = self.criterion(output,lab_batch)
		self.optimizer.zero_grad()
		loss.backward()
		self.optimizer.step()
		return loss.cpu().data

	def eval_on_batch(self,seq_batch,lab_batch,):
		self.forward_fn.eval()
		seq_batch  = Variable(torch.Tensor(seq_batch))
		lab_batch  = Variable(torch.Tensor(lab_batch))
		if self.use_gpu: seq_batch,  lab_batch = seq_batch.cuda(), lab_batch.cuda()
		output = self.forward_fn(seq_batch)
		loss = self.criterion(output,lab_batch)
		return loss.cpu().data,output.cpu().data.numpy()
			
	def test_on_batch(self, seq_batch):
		self.forward_fn.eval()
		seq_batch  = Variable(torch.Tensor(seq_batch))
		if self.use_gpu: seq_batch = seq_batch.cuda()
		output = self.forward_fn(seq_batch)
		pred = output.cpu().data.numpy()
		return pred
	
	def save_model(self, path):
		torch.save(self.forward_fn.state_dict(), path)


	def load_model(self, path):
		self.forward_fn.load_state_dict(torch.load(path))

```

**untils_dna.py**

在原函数的基础上增加了MCC，Specificity和Sensitivity的计算函数，现在有三个具体评估指标

```python
from sklearn.metrics import auc,roc_auc_score,precision_recall_curve
import numpy as np
histones=['H3K4me3','H3K27ac','H3K4me1','H3K27me3','H3K9me2']

def loadRegions(regions_indexs,dna_dict,label_dict,):
	if dna_dict is not None:
		dna_regions = np.concatenate([dna_dict[meta]  for meta in regions_indexs],axis=0)
	else: dna_regions =[]
	
	label_regions = np.concatenate([label_dict[meta] for meta in regions_indexs],axis=0).astype(int)
	return dna_regions,label_regions
 	
def model_train(regions,model,batchsize,dna_dict,label_dict,):
	train_loss = []
	regions_len = len(regions)
	for i in range(0, regions_len , batchsize):
		regions_batch = [regions[i+j] for j in range(batchsize) if (i+j) < regions_len]
		seq_batch ,lab_batch = loadRegions(regions_batch,dna_dict,label_dict)
		_loss= model.train_on_batch(seq_batch, lab_batch)
		train_loss.append(_loss)
	return np.mean(train_loss) 

def model_eval(regions,model,batchsize,dna_dict,label_dict,):
	loss = []
	pred =[]
	lab =[]
	regions_len = len(regions)
	for i in range(0, regions_len , batchsize):
		regions_batch = [regions[i+j] for j in range(batchsize) if (i+j) < regions_len]
		seq_batch ,lab_batch = loadRegions(regions_batch,dna_dict,label_dict)
		_loss,_pred = model.eval_on_batch(seq_batch, lab_batch)
		loss.append(_loss)
		lab.extend(lab_batch)
		pred.extend(_pred)
	return np.mean(loss), np.array(lab),np.array(pred)

def model_predict(regions,model,batchsize,dna_dict,label_dict,):
	lab  = []
	pred = []
	regions_len = len(regions)
	for i in range(0, len(regions), batchsize):
		regions_batch = [regions[i+j] for j in range(batchsize) if (i+j) < regions_len]
		seq_batch ,lab_batch = loadRegions(regions_batch,dna_dict,label_dict)
		_pred = model.test_on_batch(seq_batch)
		lab.extend(lab_batch)
		pred.extend(_pred)		
	return np.array(lab), np.array(pred) 


def ROC(label,pred):
	if len(np.unique(np.array(label).reshape(-1)))  == 1:
		print("all the labels are the same !")
		return 0
	else:
		label = np.array(label).reshape(-1)
		pred = np.array(pred).reshape(-1)
		return roc_auc_score(label,pred)
def auPR(label,pred):
	if len(np.unique(np.array(label).reshape(-1)))  == 1:
		print("all the labels are the same !")
		return 0
	else:
		label = np.array(label).reshape(-1)
		pred = np.array(pred).reshape(-1)
		precision, recall, thresholds = precision_recall_curve(label,pred)
		return auc(recall,precision)
def metrics(lab,pred,Type='test',loss=None):
		if Type == 'Valid':
			training_color = '\033[0;34m'
		elif Type == 'Test':
			training_color = '\033[0;35m'
		else:
			training_color = '\033[0;36m'

		auPRC_dict={}
		auROC_dict ={}
		for i in range(len(histones)):
			auPRC_dict[histones[i]] = auPR(lab[:,i],pred[:,i])
			auROC_dict[histones[i]] = ROC(lab[:,i],pred[:,i])

		print_str = training_color + '\t%s\t%s\tauROC : %.4f,auPRC : %.4f\033[0m'
		print('-'*25+Type+'-'*25)
		if loss is not None: loss_str = ',Loss : %.4f'%loss
		else :loss_str =''
		print('\033[0;36m%s\tTotalMean\tauROC : %.4f,auPRC : %.4f%s\033[0m'%(Type,np.mean(list(auROC_dict.values())),np.mean(list(auPRC_dict.values())),loss_str) )
		for histone in histones:
				print(print_str%(Type,histone.ljust(10),auROC_dict[histone],auPRC_dict[histone]))
		return auPRC_dict,auROC_dict
[xwli@mn02 deepHistone]$ cat utils_dna.py 
from sklearn.metrics import auc,roc_auc_score,precision_recall_curve
import numpy as np
histones=['H3K4me3','H3K27ac','H3K4me1','H3K27me3','H3K9me2']

def loadRegions(regions_indexs,dna_dict,label_dict,):
	if dna_dict is not None:
		dna_regions = np.concatenate([dna_dict[meta]  for meta in regions_indexs],axis=0)
	else: dna_regions =[]
	
	label_regions = np.concatenate([label_dict[meta] for meta in regions_indexs],axis=0).astype(int)
	return dna_regions,label_regions
 	
def model_train(regions,model,batchsize,dna_dict,label_dict,):
	train_loss = []
	regions_len = len(regions)
	for i in range(0, regions_len , batchsize):
		regions_batch = [regions[i+j] for j in range(batchsize) if (i+j) < regions_len]
		seq_batch ,lab_batch = loadRegions(regions_batch,dna_dict,label_dict)
		_loss= model.train_on_batch(seq_batch, lab_batch)
		train_loss.append(_loss)
	return np.mean(train_loss) 

def model_eval(regions,model,batchsize,dna_dict,label_dict,):
	loss = []
	pred =[]
	lab =[]
	regions_len = len(regions)
	for i in range(0, regions_len , batchsize):
		regions_batch = [regions[i+j] for j in range(batchsize) if (i+j) < regions_len]
		seq_batch ,lab_batch = loadRegions(regions_batch,dna_dict,label_dict)
		_loss,_pred = model.eval_on_batch(seq_batch, lab_batch)
		loss.append(_loss)
		lab.extend(lab_batch)
		pred.extend(_pred)
	return np.mean(loss), np.array(lab),np.array(pred)

def model_predict(regions,model,batchsize,dna_dict,label_dict,):
	lab  = []
	pred = []
	regions_len = len(regions)
	for i in range(0, len(regions), batchsize):
		regions_batch = [regions[i+j] for j in range(batchsize) if (i+j) < regions_len]
		seq_batch ,lab_batch = loadRegions(regions_batch,dna_dict,label_dict)
		_pred = model.test_on_batch(seq_batch)
		lab.extend(lab_batch)
		pred.extend(_pred)		
	return np.array(lab), np.array(pred) 


def ROC(label,pred):
	if len(np.unique(np.array(label).reshape(-1)))  == 1:
		print("all the labels are the same !")
		return 0
	else:
		label = np.array(label).reshape(-1)
		pred = np.array(pred).reshape(-1)
		return roc_auc_score(label,pred)
def auPR(label,pred):
	if len(np.unique(np.array(label).reshape(-1)))  == 1:
		print("all the labels are the same !")
		return 0
	else:
		label = np.array(label).reshape(-1)
		pred = np.array(pred).reshape(-1)
		precision, recall, thresholds = precision_recall_curve(label,pred)
		return auc(recall,precision)
def metrics(lab,pred,Type='test',loss=None):
		if Type == 'Valid':
			training_color = '\033[0;34m'
		elif Type == 'Test':
			training_color = '\033[0;35m'
		else:
			training_color = '\033[0;36m'

		auPRC_dict={}
		auROC_dict ={}
		for i in range(len(histones)):
			auPRC_dict[histones[i]] = auPR(lab[:,i],pred[:,i])
			auROC_dict[histones[i]] = ROC(lab[:,i],pred[:,i])

		print_str = training_color + '\t%s\t%s\tauROC : %.4f,auPRC : %.4f\033[0m'
		print('-'*25+Type+'-'*25)
		if loss is not None: loss_str = ',Loss : %.4f'%loss
		else :loss_str =''
		print('\033[0;36m%s\tTotalMean\tauROC : %.4f,auPRC : %.4f%s\033[0m'%(Type,np.mean(list(auROC_dict.values())),np.mean(list(auPRC_dict.values())),loss_str) )
		for histone in histones:
				print(print_str%(Type,histone.ljust(10),auROC_dict[histone],auPRC_dict[histone]))
		return auPRC_dict,auROC_dict

```

**train_dna.py**

```python
from model_dna import DeepHistone
import copy
import numpy as np
from utils_dna import metrics,model_train,model_eval,model_predict
import torch
#setting 
batchsize=20
data_file = 'data/onehot_all_100w.npz'

model_save_file = 'results/model.txt'
lab_save_file ='results/label.txt'
pred_save_file ='results/pred.txt'

print('Begin loading data...')
with np.load(data_file) as f:
	indexs = f['keys']
	dna_dict = dict(zip(f['keys'],f['DNAseq']))
	lab_dict = dict(zip(f['keys'],f['labels']))
np.random.shuffle(indexs)
idx_len = len(indexs)
train_index=indexs[:int(idx_len*3/5)]
valid_index=indexs[int(idx_len*3/5):int(idx_len*4/5)]
test_index=indexs[int(idx_len*4/5):]


use_gpu = torch.cuda.is_available()
model = DeepHistone(use_gpu)
print('Begin training model...')
best_model = copy.deepcopy(model)
best_valid_auPRC=0
best_valid_loss = np.float('Inf')
for epoch in range(50):
	np.random.shuffle(train_index)
	train_loss= model_train(train_index,model,batchsize,dna_dict,lab_dict,)
	valid_loss,valid_lab,valid_pred= model_eval(valid_index, model,batchsize,dna_dict,lab_dict,)
	valid_auPRC,valid_auROC= metrics(valid_lab,valid_pred,'Valid',valid_loss)

	if np.mean(list(valid_auPRC.values())) >best_valid_auPRC:
		best_model = copy.deepcopy(model)

	if valid_loss < best_valid_loss: 
		early_stop_time = 0
		best_valid_loss = valid_loss	
	else:
		model.updateLR(0.1)
		early_stop_time += 1
		if early_stop_time >= 5: break


print('Begin predicting...')
test_lab,test_pred = model_predict(test_index,best_model,batchsize,dna_dict,lab_dict,)	
test_auPR,test_roc= metrics(test_lab,test_pred,'Test')


print('Begin saving...')
np.savetxt(lab_save_file, test_lab, fmt='%d', delimiter='\t')
np.savetxt(pred_save_file, test_pred, fmt='%.4f', delimiter='\t')
best_model.save_model(model_save_file)
torch.save(model,'model1.pth')

print('Finished.')

```



#### 确定上标签方法

将H3K4me3和H3K27ac按照同一种标签计算

将H3K4me1和H3K9me2按照同一种标签计算



计算每种组蛋白修饰peak的中位数，平均数

| histone modifications | mid  | average |
| --------------------- | ---- | ------- |
| H3K4me3（1）          | 880  | 950.52  |
| H3K27ac（2）          | 799  | 854.329 |
| H3K4me1（3）          | 2694 | 3446.12 |
| H3K27me3（4）         | 1586 | 3149.9  |
| H3K9me2（5）          | 2158 | 4094.97 |

| histone modifications | function                                                     |
| --------------------- | ------------------------------------------------------------ |
| H3K4me3（1）          | 取peak中央**250bp**作为标准，win在其中的比例占到**10%**则标记为阳性 |
| H3K27ac（2）          | 250                                                          |
| H3K4me1（3）          | 750                                                          |
| H3K27me3（4）         | 750                                                          |
| H3K9me2（5）          | 1000                                                         |

target_mark5.0.py

![image-20210406221649915](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210406221649915.png)











#### 构建总数据集

20个品种5种组蛋白修饰所有阳性

| histone modifications | nums   |
| --------------------- | ------ |
| H3K4me3（1）          | 256721 |
| H3K27ac（2）          | 227526 |
| H3K4me1（3）          | 152355 |
| H3K27me3（4）         | 136028 |
| H3K9me2（5）          | 118142 |
| total                 | 890772 |
| total（after check）  | 887724 |

```shell
#!/bin/bash

for n in $(cat histone)
do
    for i in $(cat id.txt)
    do
    python3 target_mark5.0.py -f ${i}.fasta.fai -b data/signal_area/sort_${i}.bed -o target_${i}_${n}.bed -s ${i} -m ${n} && paste $[i}.fasta target_${i}_${n}.bed > data/dataset/ex1/${n}.csv && awk '$3 != 0 {print}' data/dataset/ex1/${n}.csv >> dataset_all.csv | awk '{print $3}' | uniq -c  
    done
done

```



```
#!/bin/bash


```

```shell
bsub -q gpu -o result1.out -e 14.err 'python3 train_dna.py'
```







结果展示

箱线图

```

```

![image-20210415140355121](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\image-20210415140355121.png)







### Transformer







<!DOCTYPE HTML>
<html lang="zh-hans" >
    <head>
        <meta charset="UTF-8">
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <title>构建模型 · Experimental Record</title>
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="description" content="">
        <meta name="generator" content="GitBook 3.2.3">
        <meta name="author" content="xfchen">
        
        
    
    <link rel="stylesheet" href="../gitbook/style.css">

    
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-back-to-top-button/plugin.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-chapter-fold/chapter-fold.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-code/plugin.css">
                
            
                
                <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-sectionx/sectionx.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-page-treeview/style.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-highlight/website.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-splitter/splitter.css">
                
            
                
                <link rel="stylesheet" href="../gitbook/gitbook-plugin-fontsettings/website.css">
                
            
        

    

    
        
    
        
    
        
    
        
    
        
    
        
    

        
    
    
    <meta name="HandheldFriendly" content="true"/>
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <link rel="apple-touch-icon-precomposed" sizes="152x152" href="../gitbook/images/apple-touch-icon-precomposed-152.png">
    <link rel="shortcut icon" href="../gitbook/images/favicon.ico" type="image/x-icon">

    
    <link rel="next" href="模型评估.html" />
    
    
    <link rel="prev" href="数据获取与预处理.html" />
    

    </head>
    <body>
        
<div class="book">
    <div class="book-summary">
        
            
            
                <nav role="navigation">
                


<ul class="summary">
    
    

    

    
        
        
    
        <li class="chapter " data-level="1.1" data-path="../">
            
                <a href="../">
            
                    
                    Introduction
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2" data-path="./">
            
                <a href="./">
            
                    
                    实验流程
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.2.1" data-path="实验流程概述.html">
            
                <a href="实验流程概述.html">
            
                    
                    实验流程概述
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.2" data-path="数据获取与预处理.html">
            
                <a href="数据获取与预处理.html">
            
                    
                    数据获取与预处理
            
                </a>
            

            
        </li>
    
        <li class="chapter active" data-level="1.2.3" data-path="构建模型.html">
            
                <a href="构建模型.html">
            
                    
                    构建模型
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.4" data-path="模型评估.html">
            
                <a href="模型评估.html">
            
                    
                    模型评估
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.2.5" data-path="特征提取.html">
            
                <a href="特征提取.html">
            
                    
                    特征提取
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.3" data-path="../结果分析.html">
            
                <a href="../结果分析.html">
            
                    
                    结果分析
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.4" data-path="../训练日志.html">
            
                <a href="../训练日志.html">
            
                    
                    训练日志
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5" data-path="../背景知识/READ.html">
            
                <a href="../背景知识/READ.html">
            
                    
                    背景知识
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1" data-path="../背景知识/生物学/">
            
                <a href="../背景知识/生物学/">
            
                    
                    生物学
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.1.1" data-path="../背景知识/生物学/数据来源.html">
            
                <a href="../背景知识/生物学/数据来源.html">
            
                    
                    数据来源
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.1.2" data-path="../背景知识/生物学/表观遗传.html">
            
                <a href="../背景知识/生物学/表观遗传.html">
            
                    
                    表观遗传
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    
        <li class="chapter " data-level="1.5.2" data-path="../背景知识/深度学习/">
            
                <a href="../背景知识/深度学习/">
            
                    
                    深度学习
            
                </a>
            

            
            <ul class="articles">
                
    
        <li class="chapter " data-level="1.5.2.1" data-path="../背景知识/深度学习/Pytorch.html">
            
                <a href="../背景知识/深度学习/Pytorch.html">
            
                    
                    Pytorch
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2.2" data-path="../背景知识/深度学习/CNN.html">
            
                <a href="../背景知识/深度学习/CNN.html">
            
                    
                    CNN
            
                </a>
            

            
        </li>
    
        <li class="chapter " data-level="1.5.2.3" data-path="../背景知识/深度学习/Transformer.html">
            
                <a href="../背景知识/深度学习/Transformer.html">
            
                    
                    Transformer
            
                </a>
            

            
        </li>
    

            </ul>
            
        </li>
    

            </ul>
            
        </li>
    

    

    <li class="divider"></li>

    <li>
        <a href="https://www.gitbook.com" target="blank" class="gitbook-link">
            本书使用 GitBook 发布
        </a>
    </li>
</ul>


                </nav>
            
        
    </div>

    <div class="book-body">
        
            <div class="body-inner">
                
                    

<div class="book-header" role="navigation">
    

    <!-- Title -->
    <h1>
        <i class="fa fa-circle-o-notch fa-spin"></i>
        <a href=".." >构建模型</a>
    </h1>
</div>




                    <div class="page-wrapper" tabindex="-1" role="main">
                        <div class="page-inner">
                            
                                <section class="normal markdown-section">
                                
                                <div class="treeview__container"><ul>
<li> 
<ul>
<li><div><a href="#&#x6784;&#x5EFA;&#x57FA;&#x672C;&#x6A21;&#x578B;&#xFF08;pytorch&#xFF09;">&#x6784;&#x5EFA;&#x57FA;&#x672C;&#x6A21;&#x578B;&#xFF08;pytorch&#xFF09;</a><i class="level__parent level__item level__parent--opened" state="opened" onclick="var curState = this.getAttribute(&apos;state&apos;);var nextState = curState === &apos;opened&apos; ? &apos;hidden&apos; : &apos;opened&apos;;this.setAttribute(&apos;state&apos;, nextState);this.className = this.className.split(curState).join(nextState);var list = this.parentNode.nextElementSibling;if (nextState === &apos;hidden&apos;) {    list.style.display = &apos;none&apos;;} else {    list.style.display = &apos;block&apos;;}"></i></div>
<ul>
<li><div><a href="#&#x5BFC;&#x5165;&#x6A21;&#x5757;">&#x5BFC;&#x5165;&#x6A21;&#x5757;</a><i></i></div></li>
<li><div><a href="#&#x8BBE;&#x7F6E;gpu">&#x8BBE;&#x7F6E;GPU</a><i></i></div></li>
<li><div><a href="#&#x6570;&#x636E;&#x5BFC;&#x5165;&#x53CA;&#x683C;&#x5F0F;&#x8F6C;&#x6362;">&#x6570;&#x636E;&#x5BFC;&#x5165;&#x53CA;&#x683C;&#x5F0F;&#x8F6C;&#x6362;</a><i></i></div></li>
<li><div><a href="#&#x6784;&#x5EFA;&#x8BAD;&#x7EC3;&#x96C6;&#x548C;&#x9A8C;&#x8BC1;&#x96C6;&#xFF08;tensordataset&#x7C7B;&#x548C;dataloader&#x7C7B;&#xFF09;">&#x6784;&#x5EFA;&#x8BAD;&#x7EC3;&#x96C6;&#x548C;&#x9A8C;&#x8BC1;&#x96C6;&#xFF08;TensorDataset&#x7C7B;&#x548C;DataLoader&#x7C7B;&#xFF09;</a><i></i></div></li>
</ul></li>
<li><div><a href="#deepsea">DeepSEA</a><i class="level__parent level__item level__parent--opened" state="opened" onclick="var curState = this.getAttribute(&apos;state&apos;);var nextState = curState === &apos;opened&apos; ? &apos;hidden&apos; : &apos;opened&apos;;this.setAttribute(&apos;state&apos;, nextState);this.className = this.className.split(curState).join(nextState);var list = this.parentNode.nextElementSibling;if (nextState === &apos;hidden&apos;) {    list.style.display = &apos;none&apos;;} else {    list.style.display = &apos;block&apos;;}"></i></div>
<ul>
<li><div><a href="#&#x6A21;&#x578B;&#x6846;&#x67B6;">&#x6A21;&#x578B;&#x6846;&#x67B6;</a><i></i></div></li>
</ul></li>
<li><div><a href="#deephistone">DeepHistone</a><i class="level__parent level__item level__parent--opened" state="opened" onclick="var curState = this.getAttribute(&apos;state&apos;);var nextState = curState === &apos;opened&apos; ? &apos;hidden&apos; : &apos;opened&apos;;this.setAttribute(&apos;state&apos;, nextState);this.className = this.className.split(curState).join(nextState);var list = this.parentNode.nextElementSibling;if (nextState === &apos;hidden&apos;) {    list.style.display = &apos;none&apos;;} else {    list.style.display = &apos;block&apos;;}"></i></div>
<ul>
<li><div><a href="#&#x6A21;&#x578B;&#x67B6;&#x6784;">&#x6A21;&#x578B;&#x67B6;&#x6784;</a><i></i></div></li>
<li><div><a href="#densenet">DenseNet</a><i></i></div></li>
<li><div><a href="#&#x6570;&#x636E;&#x96C6;&#x683C;&#x5F0F;&#x6539;&#x826F;">&#x6570;&#x636E;&#x96C6;&#x683C;&#x5F0F;&#x6539;&#x826F;</a><i></i></div></li>
<li><div><a href="#dna-only&#xFF08;dna-module&#xFF09;">DNA-only&#xFF08;DNA module&#xFF09;</a><i></i></div></li>
<li><div><a href="#&#x6784;&#x5EFA;&#x603B;&#x6570;&#x636E;&#x96C6;">&#x6784;&#x5EFA;&#x603B;&#x6570;&#x636E;&#x96C6;</a><i></i></div></li>
</ul></li>
<li><div><a href="#nttransformer">Nt_Transformer</a><i class="level__parent level__item level__parent--opened" state="opened" onclick="var curState = this.getAttribute(&apos;state&apos;);var nextState = curState === &apos;opened&apos; ? &apos;hidden&apos; : &apos;opened&apos;;this.setAttribute(&apos;state&apos;, nextState);this.className = this.className.split(curState).join(nextState);var list = this.parentNode.nextElementSibling;if (nextState === &apos;hidden&apos;) {    list.style.display = &apos;none&apos;;} else {    list.style.display = &apos;block&apos;;}"></i></div>
<ul>
<li><div><a href="#transformer&#xFF1A;attention-is-all-you-need">Transformer&#xFF1A;Attention is All You Need</a><i></i></div></li>
<li><div><a href="#&#x6A21;&#x578B;&#x67B6;&#x6784;">&#x6A21;&#x578B;&#x67B6;&#x6784;</a><i></i></div></li>
</ul></li>
</ul></li>
<li><a href="#"></a></li>
</ul>
</div>

<h2 id="&#x6784;&#x5EFA;&#x6A21;&#x578B;">&#x6784;&#x5EFA;&#x6A21;&#x578B;</h2>
<p>[TOC]</p>
<h3 id="&#x6784;&#x5EFA;&#x57FA;&#x672C;&#x6A21;&#x578B;&#xFF08;pytorch&#xFF09;"><strong>&#x6784;&#x5EFA;&#x57FA;&#x672C;&#x6A21;&#x578B;&#xFF08;pytorch&#xFF09;</strong></h3>
<p>&#x521D;&#x6B65;&#x5B9E;&#x9A8C;&#x5728;&#x6052;&#x6E90;&#x4E91;&#x670D;&#x52A1;&#x5668;&#x4E0A;&#x8FDB;&#x884C;</p>
<h4 id="&#x5BFC;&#x5165;&#x6A21;&#x5757;">&#x5BFC;&#x5165;&#x6A21;&#x5757;</h4>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.utils <span class="hljs-keyword">import</span> data
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn, optim
<span class="hljs-keyword">import</span> sys
</code></pre>
<h4 id="&#x8BBE;&#x7F6E;gpu">&#x8BBE;&#x7F6E;GPU</h4>
<pre><code class="lang-python">device = torch.device(<span class="hljs-string">&apos;cuda&apos;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&apos;cpu&apos;</span>)
</code></pre>
<h4 id="&#x6570;&#x636E;&#x5BFC;&#x5165;&#x53CA;&#x683C;&#x5F0F;&#x8F6C;&#x6362;">&#x6570;&#x636E;&#x5BFC;&#x5165;&#x53CA;&#x683C;&#x5F0F;&#x8F6C;&#x6362;</h4>
<pre><code class="lang-python">data_np = np.load(<span class="hljs-string">&apos;./one_hot.npy&apos;</span>)
target_pd =pd.read_csv(<span class="hljs-string">&apos;./target6.txt&apos;</span>,sep = <span class="hljs-string">&apos;\n&apos;</span>)
<span class="hljs-comment">#&#x8F6C;&#x6362;&#x6210;tensor</span>
data_tensor = torch.from_numpy(data_np)
target_array = np.array(target_pd)
target_tensor = torch.tensor(target_array)
<span class="hljs-comment">#&#x8F6C;&#x7F6E;&#x6210;(4&#xFF0C;500)</span>
data_tensor = data_tensor.reshape(<span class="hljs-number">10000</span>,<span class="hljs-number">4</span>,<span class="hljs-number">500</span>)
<span class="hljs-comment">#&#x8F6C;&#x6362;&#x6210;float&#xFF08;&#x540E;&#x7EED;&#x6A21;&#x578B;&#x8F93;&#x5165;&#x9700;&#x8981;&#xFF09;</span>
data_tensor = data_tensor.float()
target_tenor = target_tensor.float()
</code></pre>
<h4 id="&#x6784;&#x5EFA;&#x8BAD;&#x7EC3;&#x96C6;&#x548C;&#x9A8C;&#x8BC1;&#x96C6;&#xFF08;tensordataset&#x7C7B;&#x548C;dataloader&#x7C7B;&#xFF09;">&#x6784;&#x5EFA;&#x8BAD;&#x7EC3;&#x96C6;&#x548C;&#x9A8C;&#x8BC1;&#x96C6;&#xFF08;TensorDataset&#x7C7B;&#x548C;DataLoader&#x7C7B;&#xFF09;</h4>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TensorDataset</span><span class="hljs-params">(data.Dataset)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;Dataset wrapping data and target tensors.
    Each sample will be retrieved by indexing both tensors along the first
    dimension.
    Arguments:
        data_tensor (Tensor): contains sample data.
        target_tensor (Tensor): contains sample targets (labels).
    &quot;&quot;&quot;</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, data_tensor, target_tensor)</span>:</span>
        <span class="hljs-keyword">assert</span> data_tensor.size(<span class="hljs-number">0</span>) == target_tensor.size(<span class="hljs-number">0</span>)
        self.data_tensor = data_tensor
        self.target_tensor = target_tensor

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span><span class="hljs-params">(self, index)</span>:</span>
        <span class="hljs-keyword">return</span> self.data_tensor[index], self.target_tensor[index]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> len(self.data_tensor)
</code></pre>
<pre><code class="lang-python">train_data = TensorDataset(data_tensor[<span class="hljs-number">0</span>:<span class="hljs-number">9000</span>], target_tensor[<span class="hljs-number">0</span>:<span class="hljs-number">9000</span>])
val_data = TensorDataset(data_tensor[<span class="hljs-number">9000</span>:<span class="hljs-number">10000</span>], target_tensor[<span class="hljs-number">9000</span>:<span class="hljs-number">10000</span>])
train_dataloader = data.DataLoader(train_data,batch_size = <span class="hljs-number">200</span>,
                        shuffle=<span class="hljs-keyword">True</span>,num_workers=<span class="hljs-number">0</span>)
val_dataloader = data.DataLoader(val_data,batch_size = <span class="hljs-number">200</span>,
                        shuffle=<span class="hljs-keyword">False</span>,num_workers=<span class="hljs-number">0</span>)
</code></pre>
<h3 id="deepsea"><strong>DeepSEA</strong></h3>
<p>&#x4E3A;&#x9632;&#x6B62;&#x8FC7;&#x62DF;&#x5408;&#x589E;&#x5927;&#x6CDB;&#x5316;&#x80FD;&#x529B;&#xFF0C;&#x5728;&#x5377;&#x79EF;&#x5C42;&#x540E;&#x52A0;&#x5165;BN&#x5C42;&#xFF08;&#x5F52;&#x4E00;&#x5316;&#xFF09;</p>
<pre><code class="lang-python"><span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DeepSEA</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, sequence_length=<span class="hljs-number">500</span>, n_genomic_features=<span class="hljs-number">2</span>)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;
        Parameters
        ----------
        sequence_length : int
        n_genomic_features : int
        &quot;&quot;&quot;</span>
        super(DeepSEA, self).__init__()
        conv_kernel_size = <span class="hljs-number">8</span>
        pool_kernel_size = <span class="hljs-number">4</span>

        self.conv_net = nn.Sequential(
            nn.Conv1d(<span class="hljs-number">4</span>, <span class="hljs-number">320</span>, kernel_size=conv_kernel_size),
            nn.BatchNorm1d(<span class="hljs-number">320</span>),
            nn.ReLU(inplace=<span class="hljs-keyword">True</span>),
            nn.MaxPool1d(
                kernel_size=pool_kernel_size, stride=pool_kernel_size),
            nn.Dropout(p=<span class="hljs-number">0.2</span>),

            nn.Conv1d(<span class="hljs-number">320</span>, <span class="hljs-number">480</span>, kernel_size=conv_kernel_size),
            nn.BatchNorm1d(<span class="hljs-number">480</span>),
            nn.ReLU(inplace=<span class="hljs-keyword">True</span>),
            nn.MaxPool1d(
                kernel_size=pool_kernel_size, stride=pool_kernel_size),
            nn.Dropout(p=<span class="hljs-number">0.2</span>),

            nn.Conv1d(<span class="hljs-number">480</span>, <span class="hljs-number">960</span>, kernel_size=conv_kernel_size),
            nn.BatchNorm1d(<span class="hljs-number">960</span>),
            nn.ReLU(inplace=<span class="hljs-keyword">True</span>),
            nn.Dropout(p=<span class="hljs-number">0.5</span>))

        reduce_by = conv_kernel_size - <span class="hljs-number">1</span>

        self.n_channels = int(
            np.floor(
                (np.floor(
                    (sequence_length - reduce_by) / pool_kernel_size)
                 - reduce_by) / pool_kernel_size)
            - reduce_by)
        self.classifier = nn.Sequential(
            nn.Linear(<span class="hljs-number">960</span> * self.n_channels, n_genomic_features),
            nn.ReLU(inplace=<span class="hljs-keyword">True</span>),
            nn.Linear(n_genomic_features, n_genomic_features),
            nn.Sigmoid())

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Forward propagation of a batch.
        &quot;&quot;&quot;</span>
        out = self.conv_net(x)
        reshape_out = out.view(out.size(<span class="hljs-number">0</span>), <span class="hljs-number">960</span> * self.n_channels)
        predict = self.classifier(reshape_out)
        <span class="hljs-keyword">return</span> predict
</code></pre>
<p><strong>&#x521B;&#x5EFA;&#x8BAD;&#x7EC3;&#x51FD;&#x6570;</strong></p>
<p>&#x635F;&#x5931;&#x51FD;&#x6570;&#xFF1A;CrossEntropyLoss</p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span>:</span>
    net = net.to(device)
    print(<span class="hljs-string">&quot;training on &quot;</span>, device)
    loss = torch.nn.CrossEntropyLoss()
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
        train_l_sum, train_acc_sum, n, batch_count, start = <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, time.time()
        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_iter:
            X = X.to(device)
            y = y.to(device)
            y_hat = net(X)
            y = y.squeeze()
            l = loss(y_hat, y)
            optimizer.zero_grad()
            l.backward()
            optimizer.step()
            train_l_sum += l.cpu().item()
            train_acc_sum += (y_hat.argmax(dim=<span class="hljs-number">1</span>) == y).sum().cpu().item()
            n += y.shape[<span class="hljs-number">0</span>]
            batch_count += <span class="hljs-number">1</span>
        test_acc = evaluate_accuracy(test_iter, net)
        print(<span class="hljs-string">&apos;epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec&apos;</span>
              % (epoch + <span class="hljs-number">1</span>, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))
</code></pre>
<p><strong>&#x521B;&#x5EFA;&#x51C6;&#x786E;&#x7387;&#x8BA1;&#x7B97;&#x51FD;&#x6570;</strong></p>
<pre><code class="lang-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate_accuracy</span><span class="hljs-params">(data_iter, net, device=None)</span>:</span>
    <span class="hljs-keyword">if</span> device <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span> <span class="hljs-keyword">and</span> isinstance(net, torch.nn.Module):
        device = list(net.parameters())[<span class="hljs-number">0</span>].device
    acc_sum, n = <span class="hljs-number">0.0</span>, <span class="hljs-number">0</span>
    <span class="hljs-keyword">with</span> torch.no_grad():
        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> data_iter:
            <span class="hljs-keyword">if</span> isinstance(net, torch.nn.Module):
                net.eval() <span class="hljs-comment"># &#x8BC4;&#x4F30;&#x6A21;&#x5F0F;, &#x8FD9;&#x4F1A;&#x5173;&#x95ED;dropout</span>
                acc_sum += (net(X.to(device)).argmax(dim=<span class="hljs-number">1</span>)==y.to(device).squeeze()).float().sum().cpu().item()
                net.train() <span class="hljs-comment"># &#x6539;&#x56DE;&#x8BAD;&#x7EC3;&#x6A21;&#x5F0F;</span>
            <span class="hljs-keyword">else</span>: 
                <span class="hljs-keyword">if</span>(<span class="hljs-string">&apos;is_training&apos;</span> <span class="hljs-keyword">in</span> net.__code__.co_varnames): <span class="hljs-comment"># &#x5982;&#x679C;&#x6709;is_training&#x8FD9;&#x4E2A;&#x53C2;&#x6570;</span>
                    <span class="hljs-comment"># &#x5C06;is_training&#x8BBE;&#x7F6E;&#x6210;False</span>
                    acc_sum += (net(X, is_training=<span class="hljs-keyword">False</span>).argmax(dim=<span class="hljs-number">1</span>) == y).float().sum().item() 
                <span class="hljs-keyword">else</span>:
                    acc_sum += (net(X).argmax(dim=<span class="hljs-number">1</span>) == y).float().sum().item() 
            n += y.shape[<span class="hljs-number">0</span>]
    <span class="hljs-keyword">return</span> acc_sum / n
</code></pre>
<p><strong>&#x5F00;&#x542F;&#x8BAD;&#x7EC3;</strong></p>
<table>
<thead>
<tr>
<th>&#x8D85;&#x53C2;&#x6570;</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>lr</td>
<td>&#x5B66;&#x4E60;&#x7387;</td>
</tr>
<tr>
<td>num_epochs</td>
<td>&#x8BAD;&#x7EC3;&#x6279;&#x6B21;</td>
</tr>
</tbody>
</table>
<p>&#x4F18;&#x5316;&#x5668;&#xFF1A;Adam</p>
<p>&#x4F18;&#x5316;&#x5668;&#xFF1A;SGD</p>
<pre><code class="lang-python">lr, num_epochs = <span class="hljs-number">0.001</span>, <span class="hljs-number">10</span>
optimizer = torch.optim.SGD(net.parameters(), lr=lr)
train(net, train_dataloader, val_dataloader, <span class="hljs-number">256</span>, optimizer, device, num_epochs)
</code></pre>
<p><strong>&#x8BAD;&#x7EC3;&#x7ED3;&#x679C;&#x53EF;&#x89C6;&#x5316;</strong></p>
<p>&#x66F4;&#x65B0;train&#x51FD;&#x6570;</p>
<p>&#x6052;&#x6E90;&#x4E91;</p>
<pre><code class="lang-python">logger.log_value(<span class="hljs-string">&apos;loss&apos;</span>, train_l_sum/batch_count,  epoch*len(train_iter) + batch_count)
logger.log_value(<span class="hljs-string">&apos;train_acc&apos;</span>, <span class="hljs-number">100.</span> *train_acc_sum / n, epoch*len(train_iter) + batch_count)
logger.log_value(<span class="hljs-string">&apos;val_acc&apos;</span>,test_acc,epoch)
</code></pre>
<p><strong>TensorBoard_logger</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> tensorboard_logger <span class="hljs-keyword">import</span> Logger
logger = Logger(logdir=<span class="hljs-string">&quot;./tb_logs&quot;</span>, flush_secs=<span class="hljs-number">10</span>)<span class="hljs-comment">#&#x8BBE;&#x7F6E;&#x8F93;&#x51FA;&#x7684;log&#x6587;&#x4EF6;&#x4F4D;&#x7F6E;</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs)</span>:</span>
    net = net.to(device)
    print(<span class="hljs-string">&quot;training on &quot;</span>, device)
    loss = torch.nn.CrossEntropyLoss()
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(num_epochs):
        train_l_sum, train_acc_sum, n, batch_count, start = <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, time.time()
        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_iter:
            X = X.to(device)
            y = y.to(device)
            y_hat = net(X)
            y = y.squeeze()

            l = loss(y_hat, y)
            optimizer.zero_grad()
            l.backward()
            optimizer.step()
            train_l_sum += l.cpu().item()
            train_acc_sum += (y_hat.argmax(dim=<span class="hljs-number">1</span>) == y).sum().cpu().item()
            n += y.shape[<span class="hljs-number">0</span>]
            batch_count += <span class="hljs-number">1</span>


        test_acc = evaluate_accuracy(test_iter, net)
        logger.log_value(<span class="hljs-string">&apos;loss&apos;</span>, train_l_sum/batch_count,  epoch*len(train_iter) + batch_count)
        logger.log_value(<span class="hljs-string">&apos;train_acc&apos;</span>, <span class="hljs-number">100.</span> *train_acc_sum / n, epoch*len(train_iter) + batch_count)
        logger.log_value(<span class="hljs-string">&apos;val_acc&apos;</span>,test_acc,epoch)
        print(<span class="hljs-string">&apos;epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec&apos;</span>
              % (epoch + <span class="hljs-number">1</span>, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))
</code></pre>
<h4 id="&#x6A21;&#x578B;&#x6846;&#x67B6;"><strong>&#x6A21;&#x578B;&#x6846;&#x67B6;</strong></h4>
<p><strong>&#x6587;&#x4EF6;&#x7EC4;&#x7EC7;&#x67B6;&#x6784;</strong></p>
<pre><code class="lang-text">/public/home/xwli/xwzhang/deeplearningDATA/rice/pytorch_deepsea

&#x251C;&#x2500;&#x2500; checkpoints/
&#x251C;&#x2500;&#x2500; data/
&#x2502;   &#x251C;&#x2500;&#x2500; __init__.py
&#x2502;   &#x251C;&#x2500;&#x2500; dataset.py
&#x2502;  
&#x251C;&#x2500;&#x2500; models/
&#x2502;   &#x251C;&#x2500;&#x2500; __init__.py
&#x2502;   &#x251C;&#x2500;&#x2500; DeepSEA.py
&#x2502;   
&#x2502;  
&#x2514;&#x2500;&#x2500; utils/
&#x2502;   &#x251C;&#x2500;&#x2500; __init__.py
&#x2502;   &#x2514;&#x2500;&#x2500; visualize.py
&#x251C;&#x2500;&#x2500; config.py
&#x251C;&#x2500;&#x2500; main.py
&#x251C;&#x2500;&#x2500; README.md
</code></pre>
<p><strong>&#x6570;&#x636E;&#x52A0;&#x8F7D;&#x6A21;&#x5757;</strong></p>
<pre><code class="lang-python"><span class="hljs-comment">#dataset.py</span>
<span class="hljs-keyword">from</span> torch.utils <span class="hljs-keyword">import</span> data 

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TensorDataset</span><span class="hljs-params">(data.Dataset)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;Dataset wrapping data and target tensors.
    Each sample will be retrieved by indexing both tensors along the first
    dimension.
    Arguments:
        data_tensor (Tensor): contains sample data.
        target_tensor (Tensor): contains sample targets (labels).
    &quot;&quot;&quot;</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, data_tensor, target_tensor)</span>:</span>
        <span class="hljs-keyword">assert</span> data_tensor.size(<span class="hljs-number">0</span>) == target_tensor.size(<span class="hljs-number">0</span>)
        self.data_tensor = data_tensor
        self.target_tensor = target_tensor

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span><span class="hljs-params">(self, index)</span>:</span>
        <span class="hljs-keyword">return</span> self.data_tensor[index], self.target_tensor[index]

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> len(self.data_tensor)
</code></pre>
<p><strong>&#x6A21;&#x578B;&#x5B9A;&#x4E49;&#x6A21;&#x5757;</strong></p>
<pre><code class="lang-python"><span class="hljs-comment"># coding: utf-8</span>


<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> time


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BasicModule</span><span class="hljs-params">(torch.nn.Module)</span>:</span>
    <span class="hljs-string">&apos;&apos;&apos;
    &#x5C01;&#x88C5;&#x4E86;nn.Module&#xFF0C;&#x4E3B;&#x8981;&#x63D0;&#x4F9B;save&#x548C;load&#x4E24;&#x4E2A;&#x65B9;&#x6CD5;
    &apos;&apos;&apos;</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,opt=None)</span>:</span>
        super(BasicModule,self).__init__()
        self.model_name = str(type(self)) <span class="hljs-comment"># &#x6A21;&#x578B;&#x7684;&#x9ED8;&#x8BA4;&#x540D;&#x5B57;</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load</span><span class="hljs-params">(self, path)</span>:</span>
        <span class="hljs-string">&apos;&apos;&apos;
        &#x53EF;&#x52A0;&#x8F7D;&#x6307;&#x5B9A;&#x8DEF;&#x5F84;&#x7684;&#x6A21;&#x578B;
        &apos;&apos;&apos;</span>
        self.load_state_dict(torch.load(path))

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save</span><span class="hljs-params">(self, name=None)</span>:</span>
        <span class="hljs-string">&apos;&apos;&apos;
        &#x4FDD;&#x5B58;&#x6A21;&#x578B;&#xFF0C;&#x9ED8;&#x8BA4;&#x4F7F;&#x7528;&#x201C;&#x6A21;&#x578B;&#x540D;&#x5B57;+&#x65F6;&#x95F4;&#x201D;&#x4F5C;&#x4E3A;&#x6587;&#x4EF6;&#x540D;&#xFF0C;
        &#x5982;AlexNet_0710_23:57:29.pth
        &apos;&apos;&apos;</span>
        <span class="hljs-keyword">if</span> name <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span>:
            prefix = <span class="hljs-string">&apos;checkpoints/&apos;</span> + self.model_name + <span class="hljs-string">&apos;_&apos;</span>
            name = time.strftime(prefix + <span class="hljs-string">&apos;%m%d_%H:%M:%S.pth&apos;</span>)
        torch.save(self.state_dict(), name)
        <span class="hljs-keyword">return</span> name
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment">#__init__.py</span>
<span class="hljs-keyword">from</span> .DeepSEA <span class="hljs-keyword">import</span> DeepSEA
<span class="hljs-comment">#from .new_module import NewModule</span>
</code></pre>
<pre><code class="lang-python"><span class="hljs-comment"># coding: utf-8</span>

<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn
<span class="hljs-keyword">from</span> .BasicModule <span class="hljs-keyword">import</span> BasicModule

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DeepSEA</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, sequence_length=<span class="hljs-number">500</span>, n_genomic_features=<span class="hljs-number">2</span>)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;
        Parameters
        ----------
        sequence_length : int
        n_genomic_features : int
        &quot;&quot;&quot;</span>
        super(DeepSEA, self).__init__()
        conv_kernel_size = <span class="hljs-number">8</span>
        pool_kernel_size = <span class="hljs-number">4</span>

        self.conv_net = nn.Sequential(
            nn.Conv1d(<span class="hljs-number">4</span>, <span class="hljs-number">320</span>, kernel_size=conv_kernel_size),
            nn.ReLU(inplace=<span class="hljs-keyword">True</span>),
            nn.MaxPool1d(
                kernel_size=pool_kernel_size, stride=pool_kernel_size),
            nn.Dropout(p=<span class="hljs-number">0.2</span>),

            nn.Conv1d(<span class="hljs-number">320</span>, <span class="hljs-number">480</span>, kernel_size=conv_kernel_size),
            nn.ReLU(inplace=<span class="hljs-keyword">True</span>),
            nn.MaxPool1d(
                kernel_size=pool_kernel_size, stride=pool_kernel_size),
            nn.Dropout(p=<span class="hljs-number">0.2</span>),

            nn.Conv1d(<span class="hljs-number">480</span>, <span class="hljs-number">960</span>, kernel_size=conv_kernel_size),
            nn.ReLU(inplace=<span class="hljs-keyword">True</span>),
            nn.Dropout(p=<span class="hljs-number">0.5</span>))

        reduce_by = conv_kernel_size - <span class="hljs-number">1</span>

        self.n_channels = int(
            np.floor(
                (np.floor(
                    (sequence_length - reduce_by) / pool_kernel_size)
                 - reduce_by) / pool_kernel_size)
            - reduce_by)
        self.classifier = nn.Sequential(
            nn.Linear(<span class="hljs-number">960</span> * self.n_channels, n_genomic_features),
            nn.ReLU(inplace=<span class="hljs-keyword">True</span>),
            nn.Linear(n_genomic_features, n_genomic_features),
            nn.Sigmoid()
        )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-string">&quot;&quot;&quot;Forward propagation of a batch.
        &quot;&quot;&quot;</span>
        out = self.conv_net(x)
        reshape_out = out.view(out.size(<span class="hljs-number">0</span>), <span class="hljs-number">960</span> * self.n_channels)
        predict = self.classifier(reshape_out)
        <span class="hljs-keyword">return</span> predict
</code></pre>
<p><strong>&#x5DE5;&#x5177;&#x51FD;&#x6570;</strong></p>
<pre><code class="lang-python"><span class="hljs-comment">#coding:utf8</span>
<span class="hljs-comment">#visualize.py</span>
<span class="hljs-keyword">import</span> visdom
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Visualizer</span><span class="hljs-params">(object)</span>:</span>
    <span class="hljs-string">&apos;&apos;&apos;
    &#x5C01;&#x88C5;&#x4E86;visdom&#x7684;&#x57FA;&#x672C;&#x64CD;&#x4F5C;&#xFF0C;&#x4F46;&#x662F;&#x4F60;&#x4ECD;&#x7136;&#x53EF;&#x4EE5;&#x901A;&#x8FC7;`self.vis.function`
    &#x6216;&#x8005;`self.function`&#x8C03;&#x7528;&#x539F;&#x751F;&#x7684;visdom&#x63A5;&#x53E3;
    &#x6BD4;&#x5982; 
    self.text(&apos;hello visdom&apos;)
    self.histogram(t.randn(1000))
    self.line(t.arange(0, 10),t.arange(1, 11))
    &apos;&apos;&apos;</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, env=<span class="hljs-string">&apos;default&apos;</span>, **kwargs)</span>:</span>
        self.vis = visdom.Visdom(env=env, **kwargs)

       <span class="hljs-comment"># &#x753B;&#x7684;&#x7B2C;&#x51E0;&#x4E2A;&#x6570;&#xFF0C;&#x76F8;&#x5F53;&#x4E8E;&#x6A2A;&#x5750;&#x6807;</span>
       <span class="hljs-comment"># &#x6BD4;&#x5982;&#xFF08;&#x2019;loss&apos;,23&#xFF09; &#x5373;loss&#x7684;&#x7B2C;23&#x4E2A;&#x70B9;</span>
        self.index = {}
        self.log_text = <span class="hljs-string">&apos;&apos;</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">reinit</span><span class="hljs-params">(self, env=<span class="hljs-string">&apos;default&apos;</span>, **kwargs)</span>:</span>
        <span class="hljs-string">&apos;&apos;&apos;
        &#x4FEE;&#x6539;visdom&#x7684;&#x914D;&#x7F6E;
        &apos;&apos;&apos;</span>
        self.vis = visdom.Visdom(env=env, **kwargs)
        <span class="hljs-keyword">return</span> self

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_many</span><span class="hljs-params">(self, d)</span>:</span>
        <span class="hljs-string">&apos;&apos;&apos;
        &#x4E00;&#x6B21;plot&#x591A;&#x4E2A;
        @params d: dict (name, value) i.e. (&apos;loss&apos;, 0.11)
        &apos;&apos;&apos;</span>
        <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> d.iteritems():
            self.plot(k, v)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">img_many</span><span class="hljs-params">(self, d)</span>:</span>
        <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> d.iteritems():
            self.img(k, v)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">plot</span><span class="hljs-params">(self, name, y, **kwargs)</span>:</span>
        <span class="hljs-string">&apos;&apos;&apos;
        self.plot(&apos;loss&apos;, 1.00)
        &apos;&apos;&apos;</span>
        x = self.index.get(name, <span class="hljs-number">0</span>)
        self.vis.line(Y=np.array([y]), X=np.array([x]),
                     win=unicode(name),
                     opts=dict(title=name),
                     update=<span class="hljs-keyword">None</span> <span class="hljs-keyword">if</span> x == <span class="hljs-number">0</span> <span class="hljs-keyword">else</span> <span class="hljs-string">&apos;append&apos;</span>,
                     **kwargs
                     )
        self.index[name] = x + <span class="hljs-number">1</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">img</span><span class="hljs-params">(self, name, img_, **kwargs)</span>:</span>
        <span class="hljs-string">&apos;&apos;&apos;
        self.img(&apos;input_img&apos;, t.Tensor(64, 64))
        self.img(&apos;input_imgs&apos;, t.Tensor(3, 64, 64))
        self.img(&apos;input_imgs&apos;, t.Tensor(100, 1, 64, 64))
        self.img(&apos;input_imgs&apos;, t.Tensor(100, 3, 64, 64), nrows=10)
        &apos;&apos;&apos;</span>
        self.vis.images(img_.cpu().numpy(),
                      win=unicode(name),
                      opts=dict(title=name),
                      **kwargs
                      )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">log</span><span class="hljs-params">(self, info, win=<span class="hljs-string">&apos;log_text&apos;</span>)</span>:</span>
        <span class="hljs-string">&apos;&apos;&apos;
        self.log({&apos;loss&apos;:1, &apos;lr&apos;:0.0001})
        &apos;&apos;&apos;</span>

        self.log_text += (<span class="hljs-string">&apos;[{time}] {info} &lt;br&gt;&apos;</span>.format(
                           time=time.strftime(<span class="hljs-string">&apos;%m%d_%H%M%S&apos;</span>),\
                           info=info))
        self.vis.text(self.log_text, win)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getattr__</span><span class="hljs-params">(self, name)</span>:</span>
        <span class="hljs-string">&apos;&apos;&apos;
        self.function &#x7B49;&#x4EF7;&#x4E8E;self.vis.function
        &#x81EA;&#x5B9A;&#x4E49;&#x7684;plot,image,log,plot_many&#x7B49;&#x9664;&#x5916;
        &apos;&apos;&apos;</span>
        <span class="hljs-keyword">return</span> getattr(self.vis, name)
</code></pre>
<p><strong>&#x914D;&#x7F6E;&#x6587;&#x4EF6;</strong></p>
<pre><code class="lang-python"><span class="hljs-comment">#config.py</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DefaultConfig</span><span class="hljs-params">(object)</span>:</span>
    env = <span class="hljs-string">&apos;default&apos;</span> <span class="hljs-comment"># visdom &#x73AF;&#x5883;</span>
    model = <span class="hljs-string">&apos;DeepSEA&apos;</span> <span class="hljs-comment"># &#x4F7F;&#x7528;&#x7684;&#x6A21;&#x578B;&#xFF0C;&#x540D;&#x5B57;&#x5FC5;&#x987B;&#x4E0E;models/__init__.py&#x4E2D;&#x7684;&#x540D;&#x5B57;&#x4E00;&#x81F4;</span>

    train_data_root = <span class="hljs-string">&apos;./data/one_hot_10w_sh.npy&apos;</span> <span class="hljs-comment"># &#x8BAD;&#x7EC3;&#x96C6;&#x5B58;&#x653E;&#x8DEF;&#x5F84;</span>
    test_data_root = <span class="hljs-string">&apos;./data/test1&apos;</span> <span class="hljs-comment"># &#x6D4B;&#x8BD5;&#x96C6;&#x5B58;&#x653E;&#x8DEF;&#x5F84;</span>
    target_data_root = <span class="hljs-string">&apos;./data/target_10w_sh.txt&apos;</span>
    load_model_path = <span class="hljs-string">&apos;checkpoints/model.pth&apos;</span> <span class="hljs-comment"># &#x52A0;&#x8F7D;&#x9884;&#x8BAD;&#x7EC3;&#x7684;&#x6A21;&#x578B;&#x7684;&#x8DEF;&#x5F84;&#xFF0C;&#x4E3A;None&#x4EE3;&#x8868;&#x4E0D;&#x52A0;&#x8F7D;</span>

    batch_size = <span class="hljs-number">256</span> <span class="hljs-comment"># batch size</span>
    use_gpu = <span class="hljs-keyword">True</span> <span class="hljs-comment"># use GPU or not</span>
    num_workers = <span class="hljs-number">2</span> <span class="hljs-comment"># how many workers for loading data</span>
    print_freq = <span class="hljs-number">20</span> <span class="hljs-comment"># print info every N batch</span>
    <span class="hljs-comment">#debug_file = &apos;/tmp/debug&apos; # if os.path.exists(debug_file): enter ipdb</span>
    result_file = <span class="hljs-string">&apos;result.csv&apos;</span>

    num_epochs = <span class="hljs-number">10</span>
    lr = <span class="hljs-number">0.01</span> <span class="hljs-comment"># initial learning rate</span>
    lr_decay = <span class="hljs-number">0.95</span> <span class="hljs-comment"># when val_loss increase, lr = lr*lr_decay</span>
    weight_decay = <span class="hljs-number">1e-4</span> <span class="hljs-comment"># &#x635F;&#x5931;&#x51FD;&#x6570;</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">parse</span><span class="hljs-params">(self, kwargs)</span>:</span>
        <span class="hljs-string">&apos;&apos;&apos;
        &#x6839;&#x636E;&#x5B57;&#x5178;kwargs &#x66F4;&#x65B0; config&#x53C2;&#x6570;
        &apos;&apos;&apos;</span>
        <span class="hljs-comment"># &#x66F4;&#x65B0;&#x914D;&#x7F6E;&#x53C2;&#x6570;</span>
        <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> kwargs.items():
            <span class="hljs-string">&apos;&apos;&apos;
            if not hasattr(self, k):
                # &#x8B66;&#x544A;&#x8FD8;&#x662F;&#x62A5;&#x9519;&#xFF0C;&#x53D6;&#x51B3;&#x4E8E;&#x4F60;&#x4E2A;&#x4EBA;&#x7684;&#x559C;&#x597D;

                warnings.warn(&quot;Warning: opt has not attribut %s&quot; %k)
            &apos;&apos;&apos;</span>
            setattr(self, k, v)

        <span class="hljs-comment"># &#x6253;&#x5370;&#x914D;&#x7F6E;&#x4FE1;&#x606F;  </span>
        print(<span class="hljs-string">&apos;user config:&apos;</span>)
        <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> self.__class__.__dict__.items():
            <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> k.startswith(<span class="hljs-string">&apos;__&apos;</span>):
                print(k, getattr(self, k))

DefaultConfig.parse = parse
opt =DefaultConfig()
</code></pre>
<p><strong>&#x4E3B;&#x51FD;&#x6570;</strong></p>
<pre><code class="lang-python"><span class="hljs-comment">#main.py</span>
<span class="hljs-keyword">from</span> config <span class="hljs-keyword">import</span> opt
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">from</span> torch.utils <span class="hljs-keyword">import</span> data
<span class="hljs-keyword">import</span> models
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> data.dataset <span class="hljs-keyword">import</span> TensorDataset
<span class="hljs-keyword">from</span> torch.utils.data <span class="hljs-keyword">import</span> DataLoader
<span class="hljs-comment">#from torch.autograd import Variable</span>
<span class="hljs-comment">#from torchnet import meter</span>
<span class="hljs-comment">#from utils.visualize import Visualizer</span>
<span class="hljs-comment">#from tqdm import tqdm</span>
<span class="hljs-keyword">import</span> time




<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">evaluate_accuracy</span><span class="hljs-params">(data_iter, net, device=None)</span>:</span>
    <span class="hljs-keyword">if</span> device <span class="hljs-keyword">is</span> <span class="hljs-keyword">None</span> <span class="hljs-keyword">and</span> isinstance(net, torch.nn.Module):
        <span class="hljs-comment"># &#x5982;&#x679C;&#x6CA1;&#x6307;&#x5B9A;device&#x5C31;&#x4F7F;&#x7528;net&#x7684;device</span>
        device = list(net.parameters())[<span class="hljs-number">0</span>].device
    acc_sum, n = <span class="hljs-number">0.0</span>, <span class="hljs-number">0</span>
    <span class="hljs-keyword">with</span> torch.no_grad():
        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> data_iter:
            <span class="hljs-string">&apos;&apos;&apos;
            if isinstance(net, torch.nn.Module):
                net.eval() # &#x8BC4;&#x4F30;&#x6A21;&#x5F0F;, &#x8FD9;&#x4F1A;&#x5173;&#x95ED;dropout
                acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item()
                net.train() # &#x6539;&#x56DE;&#x8BAD;&#x7EC3;&#x6A21;&#x5F0F;
            else: 
                if(&apos;is_training&apos; in net.__code__.co_varnames): # &#x5982;&#x679C;&#x6709;is_training&#x8FD9;&#x4E2A;&#x53C2;&#x6570;
                    # &#x5C06;is_training&#x8BBE;&#x7F6E;&#x6210;False
                    acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() 
                else:
                    acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()
            &apos;&apos;&apos;</span>
            acc_sum += (net(X.to(device)).argmax(dim=<span class="hljs-number">1</span>) == (y.to(device).squeeze())).float().sum().cpu().item()
            n += y.shape[<span class="hljs-number">0</span>]

    <span class="hljs-keyword">return</span> acc_sum / n



<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(**kwargs)</span>:</span>
    opt.parse(kwargs)
    <span class="hljs-comment">#vis = Visualizer(opt.env)</span>

    model = getattr(models, opt.model)()

    <span class="hljs-string">&apos;&apos;&apos;
    if opt.load_model_path:
        model.load(opt.load_model_path)
    if opt.use_gpu: model.cuda()
    &apos;&apos;&apos;</span>
    device = torch.device(<span class="hljs-string">&apos;cuda&apos;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&apos;cpu&apos;</span>)
    print(<span class="hljs-string">&quot;training on &quot;</span>, device)

    data_np = np.load(opt.train_data_root)
    data_tensor = torch.from_numpy(data_np)

    target_pd =pd.read_csv(opt.target_data_root,sep = <span class="hljs-string">&apos;\n&apos;</span>,header = <span class="hljs-keyword">None</span>)
    target_array = np.array(target_pd)
    target_tensor = torch.tensor(target_array)

    data_tensor = data_tensor.reshape(<span class="hljs-number">100000</span>,<span class="hljs-number">4</span>,<span class="hljs-number">500</span>)
    data_tensor = data_tensor.float()
    target_tenor = target_tensor.float()

    train_data = TensorDataset(data_tensor[<span class="hljs-number">0</span>:<span class="hljs-number">99000</span>], target_tensor[<span class="hljs-number">0</span>:<span class="hljs-number">99000</span>])
    val_data = TensorDataset(data_tensor[<span class="hljs-number">99000</span>:<span class="hljs-number">100000</span>], target_tensor[<span class="hljs-number">99000</span>:<span class="hljs-number">100000</span>])
    train_dataloader = data.DataLoader(train_data,batch_size = <span class="hljs-number">200</span>,
                        shuffle=<span class="hljs-keyword">True</span>,num_workers=<span class="hljs-number">0</span>)
    val_dataloader = data.DataLoader(val_data,batch_size = <span class="hljs-number">200</span>,
                        shuffle=<span class="hljs-keyword">False</span>,num_workers=<span class="hljs-number">0</span>)

    loss = torch.nn.CrossEntropyLoss()
    lr = opt.lr
    optimizer = torch.optim.SGD(model.parameters(),
                                lr=lr,
                                weight_decay = opt.weight_decay)




    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(opt.num_epochs):
        model = model.to(device)
        train_l_sum, train_acc_sum, n, batch_count, start = <span class="hljs-number">0.0</span>, <span class="hljs-number">0.0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, time.time()
        <span class="hljs-keyword">for</span> X, y <span class="hljs-keyword">in</span> train_dataloader:
            X = X.to(device)
            y = y.to(device)
            y_hat = model(X)
            y = y.squeeze()
            l = loss(y_hat, y)
            optimizer.zero_grad()
            l.backward()
            optimizer.step()
            train_l_sum += l.cpu().item()
            train_acc_sum += (y_hat.argmax(dim=<span class="hljs-number">1</span>) == y).sum().cpu().item()
            n += y.shape[<span class="hljs-number">0</span>]
            batch_count += <span class="hljs-number">1</span>
        test_acc = evaluate_accuracy(val_dataloader, model)
        print(<span class="hljs-string">&apos;epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec&apos;</span>
              % (epoch + <span class="hljs-number">1</span>, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start))


        <span class="hljs-comment">#model.save()</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">val</span><span class="hljs-params">(model, dataloader)</span>:</span>
    <span class="hljs-string">&apos;&apos;&apos;
    &#x8BA1;&#x7B97;&#x6A21;&#x578B;&#x5728;&#x9A8C;&#x8BC1;&#x96C6;&#x4E0A;&#x7684;&#x51C6;&#x786E;&#x7387;&#x7B49;&#x4FE1;&#x606F;&#xFF0C;&#x7528;&#x4EE5;&#x8F85;&#x52A9;&#x8BAD;&#x7EC3;
    &apos;&apos;&apos;</span>
    <span class="hljs-keyword">pass</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">(**kwargs)</span>:</span>
    <span class="hljs-string">&apos;&apos;&apos;
    &#x6D4B;&#x8BD5;&#xFF08;inference&#xFF09;
    &apos;&apos;&apos;</span>
    <span class="hljs-keyword">pass</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">help</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-string">&apos;&apos;&apos;
    &#x6253;&#x5370;&#x5E2E;&#x52A9;&#x7684;&#x4FE1;&#x606F; 
    &apos;&apos;&apos;</span>
    print(<span class="hljs-string">&apos;help&apos;</span>)

<span class="hljs-keyword">if</span> __name__==<span class="hljs-string">&apos;__main__&apos;</span>:
    <span class="hljs-keyword">import</span> fire
    fire.Fire()
</code></pre>
<h3 id="deephistone"><strong>DeepHistone</strong></h3>
<p><strong>&#x53C2;&#x8003;&#x6587;&#x732E;&#xFF1A;</strong></p>
<p><a href="https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-5489-4" target="_blank">DeepHistone: a deep learning approach to predicting histone modification</a></p>
<p>github&#xFF1A;<a href="https://github.com/QijinYin/DeepHistone" target="_blank">https://github.com/QijinYin/DeepHistone</a>.</p>
<h4 id="&#x6A21;&#x578B;&#x67B6;&#x6784;"><strong>&#x6A21;&#x578B;&#x67B6;&#x6784;</strong></h4>
<p>&#x4E09;&#x4E2A;&#x6A21;&#x5757;&#xFF1A;DNA module &#xFF0C;DNase module&#xFF0C; joint module</p>
<p>DNA and DNase module&#xFF1A; <strong>densely connected</strong> convolutional neural network</p>
<p>joint module&#xFF1A; distinguish histone modification sites of a marker from those of other markers</p>
<p><img src="https://media.springernature.com/full/springer-static/image/art%3A10.1186%2Fs12864-019-5489-4/MediaObjects/12864_2019_5489_Fig1_HTML.png" alt="Fig. 1"></p>
<p><em>&#x56FE;&#x7247;&#x6765;&#x81EA;&#x53C2;&#x8003;&#x6587;&#x732E;</em></p>
<h4 id="densenet">DenseNet</h4>
<p><img src="https://pic2.zhimg.com/v2-c81da515c8fa9796601fde82e4d36f61_r.jpg" alt="preview" style="zoom:67%;"></p>
<p><em>&#x56FE;&#x7247;&#x6765;&#x81EA;&#x53C2;&#x8003;&#x6587;&#x732E;</em></p>
<p>&#x5229;&#x7528;&#x524D;&#x9762;&#x6240;&#x6709;&#x5C42;&#x4E0E;&#x540E;&#x9762;&#x5C42;&#x7684;&#x201C;&#x77ED;&#x8DEF;&#x8FDE;&#x63A5;&#x201D;&#x6765;&#x5B9E;&#x73B0;&#x7279;&#x5F81;&#x91CD;&#x7528;&#xFF0C;&#x5177;&#x6709;&#x66F4;&#x9AD8;&#x7684;&#x6027;&#x80FD;&#xFF0C;&#x66F4;&#x5C11;&#x7684;&#x53C2;&#x6570;&#x91CF;</p>
<p><strong>&#x53C2;&#x8003;&#x6587;&#x732E;</strong>&#xFF1A;</p>
<p>&#x8BBA;&#x6587;&#xFF1A;<a href="https://ieeexplore.ieee.org/document/8099726" target="_blank">Densely Connected Convolutional Networks</a></p>
<p>&#x6A21;&#x578B;&#x8BE6;&#x89E3;&#xFF1A;</p>
<p><a href="https://blog.csdn.net/u014380165/article/details/75142664" target="_blank">https://blog.csdn.net/u014380165/article/details/75142664</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/37189203" target="_blank">https://zhuanlan.zhihu.com/p/37189203</a></p>
<h4 id="&#x6570;&#x636E;&#x96C6;&#x683C;&#x5F0F;&#x6539;&#x826F;">&#x6570;&#x636E;&#x96C6;&#x683C;&#x5F0F;&#x6539;&#x826F;</h4>
<p>labels onehot encoder</p>
<p>&#x53EF;&#x4EE5;&#x5C06;&#x6B64;&#x51FD;&#x6570;&#x52A0;&#x5165;OneHotEncoder</p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">dense_to_one_hot</span><span class="hljs-params">(labels_dense, num_classes)</span>:</span>
    <span class="hljs-string">&quot;&quot;&quot;Convert class labels from scalars to one-hot vectors.&quot;&quot;&quot;</span>
    num_labels = labels_dense.shape[<span class="hljs-number">0</span>]
    index_offset = np.arange(num_labels) * num_classes
    labels_one_hot = np.zeros((num_labels, num_classes))
    labels_one_hot.flat[index_offset+labels_dense.ravel()] = <span class="hljs-number">1</span>
    <span class="hljs-keyword">return</span> labels_one_hot

labels_dense = np.array([<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]) 
num_classes  = <span class="hljs-number">5</span>
dense_to_one_hot(labels_dense,num_classes)
</code></pre>
<p>&#x7EE7;&#x7EED;&#x6539;&#x8FDB;target_mark.py : &#x5C06;&#x7EC4;&#x86CB;&#x767D;&#x4FEE;&#x9970;&#xFF08;m&#xFF09;&#x4E0E;&#x6570;&#x5B57;&#x6807;&#x7B7E;&#x5BF9;&#x5E94;&#xFF0C;&#x540C;&#x65F6;<strong>&#x6539;&#x826F;&#x6BCF;&#x79CD;&#x7EC4;&#x86CB;&#x767D;&#x7684;&#x6807;&#x8BB0;&#x65B9;&#x6CD5;</strong></p>
<p>npz&#x5C01;&#x88C5;&#xFF1A;&#x5C06;npz&#x5C01;&#x88C5;&#x4E5F;&#x52A0;&#x5165;OneHotEncoder&#xFF0C;&#x52A0;&#x5165;&#x5230;make</p>
<pre><code class="lang-python">np.savez(<span class="hljs-string">&apos;C:/Users/12394/PycharmProjects/Spyder/data.npz&apos;</span>,a = a, b = b)
</code></pre>
<p>1.&#x5C06;5&#x79CD;&#x7EC4;&#x86CB;&#x767D;&#x4FEE;&#x9970;&#x9633;&#x6027;&#x6837;&#x672C;&#x96C6;&#x4E2D;&#x5728;&#x4E00;&#x4E2A;&#x6570;&#x636E;&#x96C6;</p>
<p>2.reshape(len(data),1,4,500)</p>
<p>OneHotEncoder3.0.py&#x5B8C;&#x6210;</p>
<pre><code class="lang-shell">bsub -q high -e 12.err -o 12.out &apos;python3 scripts/py_scripts/OneHotEncoder3.0.py -i dataset_ex1_10w.txt -c make -m ex1 -n 10w&apos;
</code></pre>
<table>
<thead>
<tr>
<th>Keys</th>
<th>DNA seq</th>
<th>labels</th>
</tr>
</thead>
<tbody>
<tr>
<td>(nums,)</td>
<td>(nums, 1 , 4 , 500 )</td>
<td>(nums ,1, 5)</td>
</tr>
<tr>
<td>species_histone_chr_start_end</td>
<td>onehot</td>
<td>onehot</td>
</tr>
</tbody>
</table>
<h4 id="dna-only&#xFF08;dna-module&#xFF09;"><strong>DNA-only</strong>&#xFF08;DNA module&#xFF09;</h4>
<pre><code class="lang-txt">/public/home/xwli/xwzhang/deeplearningDATA/rice/DeepHistone/

&#x251C;&#x2500;&#x2500; data/
&#x251C;&#x2500;&#x2500; results/
&#x2502;   &#x2514;&#x2500;&#x2500; model.txt
&#x2502;   &#x2514;&#x2500;&#x2500; label.txt
&#x2502;   &#x2514;&#x2500;&#x2500; pred.txt
&#x251C;&#x2500;&#x2500; model_dna.py 
&#x251C;&#x2500;&#x2500; utils_dna.py
&#x251C;&#x2500;&#x2500; train_dna.py
</code></pre>
<p><strong>model_dna.py</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F
<span class="hljs-keyword">import</span> torch.optim <span class="hljs-keyword">as</span> optim
<span class="hljs-keyword">from</span> torch.autograd <span class="hljs-keyword">import</span> Variable
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics
<span class="hljs-keyword">from</span> torch.optim <span class="hljs-keyword">import</span>  Optimizer
<span class="hljs-keyword">import</span> math 
<span class="hljs-keyword">from</span> torch.nn.parameter <span class="hljs-keyword">import</span> Parameter

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">BasicBlock</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, in_planes, grow_rate,)</span>:</span>
        super(BasicBlock, self).__init__()
        self.block = nn.Sequential(
            nn.BatchNorm2d(in_planes),
            nn.ReLU(),
            nn.Conv2d(in_planes, grow_rate, (<span class="hljs-number">1</span>,<span class="hljs-number">9</span>), <span class="hljs-number">1</span>, (<span class="hljs-number">0</span>,<span class="hljs-number">4</span>)),
            <span class="hljs-comment">#nn.Dropout2d(0.2)</span>
        )
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        out = self.block(x)
        <span class="hljs-keyword">return</span> torch.cat([x, out],<span class="hljs-number">1</span>)

<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DenseBlock</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, nb_layers, in_planes, grow_rate,)</span>:</span>
        super(DenseBlock, self).__init__()
        layers = []
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(nb_layers):
            layers.append(BasicBlock(in_planes + i*grow_rate, grow_rate,))
        self.layer = nn.Sequential(*layers)
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-keyword">return</span> self.layer(x)


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ModuleDense</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(ModuleDense, self).__init__()


        self.conv1 = nn.Sequential(
            nn.Conv2d(<span class="hljs-number">1</span>,<span class="hljs-number">128</span>,(<span class="hljs-number">4</span>,<span class="hljs-number">9</span>),<span class="hljs-number">1</span>,(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>)),
            <span class="hljs-comment">#nn.Dropout2d(0.2),</span>
            )

        self.block1 = DenseBlock(<span class="hljs-number">3</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>)    
        self.trans1 = nn.Sequential(
            nn.BatchNorm2d(<span class="hljs-number">128</span>+<span class="hljs-number">3</span>*<span class="hljs-number">128</span>),
            nn.ReLU(),
            nn.Conv2d(<span class="hljs-number">128</span>+<span class="hljs-number">3</span>*<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<span class="hljs-number">1</span>),
            <span class="hljs-comment">#nn.Dropout2d(0.2),</span>
            nn.MaxPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">4</span>)),
        )
        self.block2 = DenseBlock(<span class="hljs-number">3</span>,<span class="hljs-number">256</span>,<span class="hljs-number">256</span>)
        self.trans2 = nn.Sequential(
            nn.BatchNorm2d(<span class="hljs-number">256</span>+<span class="hljs-number">3</span>*<span class="hljs-number">256</span>),
            nn.ReLU(),
            nn.Conv2d(<span class="hljs-number">256</span>+<span class="hljs-number">3</span>*<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>),<span class="hljs-number">1</span>),
            <span class="hljs-comment">#nn.Dropout2d(0.2),</span>
            nn.MaxPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">4</span>)),
        )
        self.out_size = <span class="hljs-number">500</span> // <span class="hljs-number">4</span> // <span class="hljs-number">4</span>  * <span class="hljs-number">512</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, seq)</span>:</span>
        n, h, w = seq.size()

        seq = seq.view(n,<span class="hljs-number">1</span>,<span class="hljs-number">4</span>,w)

        out = self.conv1(seq)
        out = self.block1(out)
        out = self.trans1(out)
        out = self.block2(out)
        out = self.trans2(out)
        n, c, h, w = out.size()
        out = out.view(n,c*h*w) 
        <span class="hljs-keyword">return</span> out



<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NetDeepHistone</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(NetDeepHistone, self).__init__()
        print(<span class="hljs-string">&apos;DeepHistone(Dense) is used.&apos;</span>)
        self.seq_map = ModuleDense()
        self.seq_len = self.seq_map.out_size
        seq_len = self.seq_len

        self.linear_map = nn.Sequential(
            nn.Dropout(<span class="hljs-number">0.5</span>),
            nn.Linear(int(seq_len),<span class="hljs-number">925</span>),
            nn.BatchNorm1d(<span class="hljs-number">925</span>),
            nn.ReLU(),
            <span class="hljs-comment">#nn.Dropout(0.1),</span>
            nn.Linear(<span class="hljs-number">925</span>,<span class="hljs-number">5</span>),
            nn.Sigmoid(),
        )

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, seq)</span>:</span>
        flat_seq = self.seq_map(seq)    
        out = self.linear_map(flat_seq)
        <span class="hljs-keyword">return</span> out


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">DeepHistone</span><span class="hljs-params">()</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,use_gpu,learning_rate=<span class="hljs-number">0.001</span>)</span>:</span>
        self.forward_fn = NetDeepHistone()
        self.criterion  = nn.BCELoss()
        self.optimizer  = optim.Adam(self.forward_fn.parameters(), lr=learning_rate, weight_decay = <span class="hljs-number">0</span>)
        self.use_gpu    = use_gpu
        <span class="hljs-keyword">if</span> self.use_gpu : self.criterion,self.forward_fn = self.criterion.cuda(), self.forward_fn.cuda()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">updateLR</span><span class="hljs-params">(self, fold)</span>:</span>
        <span class="hljs-keyword">for</span> param_group <span class="hljs-keyword">in</span> self.optimizer.param_groups:
            param_group[<span class="hljs-string">&apos;lr&apos;</span>] *= fold

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_on_batch</span><span class="hljs-params">(self,seq_batch,lab_batch,)</span>:</span> 
        self.forward_fn.train()
        seq_batch  = Variable(torch.Tensor(seq_batch))
        lab_batch  = Variable(torch.Tensor(lab_batch))
        <span class="hljs-keyword">if</span> self.use_gpu: seq_batch, lab_batch = seq_batch.cuda(), lab_batch.cuda()
        output = self.forward_fn(seq_batch)
        loss = self.criterion(output,lab_batch)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        <span class="hljs-keyword">return</span> loss.cpu().data

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">eval_on_batch</span><span class="hljs-params">(self,seq_batch,lab_batch,)</span>:</span>
        self.forward_fn.eval()
        seq_batch  = Variable(torch.Tensor(seq_batch))
        lab_batch  = Variable(torch.Tensor(lab_batch))
        <span class="hljs-keyword">if</span> self.use_gpu: seq_batch,  lab_batch = seq_batch.cuda(), lab_batch.cuda()
        output = self.forward_fn(seq_batch)
        loss = self.criterion(output,lab_batch)
        <span class="hljs-keyword">return</span> loss.cpu().data,output.cpu().data.numpy()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test_on_batch</span><span class="hljs-params">(self, seq_batch)</span>:</span>
        self.forward_fn.eval()
        seq_batch  = Variable(torch.Tensor(seq_batch))
        <span class="hljs-keyword">if</span> self.use_gpu: seq_batch = seq_batch.cuda()
        output = self.forward_fn(seq_batch)
        pred = output.cpu().data.numpy()
        <span class="hljs-keyword">return</span> pred

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save_model</span><span class="hljs-params">(self, path)</span>:</span>
        torch.save(self.forward_fn.state_dict(), path)


    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">load_model</span><span class="hljs-params">(self, path)</span>:</span>
        self.forward_fn.load_state_dict(torch.load(path))
</code></pre>
<p><strong>untils_dna.py</strong></p>
<p>&#x5728;&#x539F;&#x51FD;&#x6570;&#x7684;&#x57FA;&#x7840;&#x4E0A;&#x589E;&#x52A0;&#x4E86;MCC&#xFF0C;Specificity&#x548C;Sensitivity&#x7684;&#x8BA1;&#x7B97;&#x51FD;&#x6570;&#xFF0C;&#x73B0;&#x5728;&#x6709;&#x4E09;&#x4E2A;&#x5177;&#x4F53;&#x8BC4;&#x4F30;&#x6307;&#x6807;</p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> auc,roc_auc_score,precision_recall_curve,matthews_corrcoef,precision_score,recall_score
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
histones=[<span class="hljs-string">&apos;H3K4me3&apos;</span>,<span class="hljs-string">&apos;H3K27ac&apos;</span>,<span class="hljs-string">&apos;H3K4me1&apos;</span>,<span class="hljs-string">&apos;H3K27me3&apos;</span>,<span class="hljs-string">&apos;H3K9me2&apos;</span>]

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">loadRegions</span><span class="hljs-params">(regions_indexs,dna_dict,label_dict,)</span>:</span>
    <span class="hljs-keyword">if</span> dna_dict <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>:
        dna_regions = np.concatenate([dna_dict[meta]  <span class="hljs-keyword">for</span> meta <span class="hljs-keyword">in</span> regions_indexs],axis=<span class="hljs-number">0</span>)
    <span class="hljs-keyword">else</span>: dna_regions =[]

    label_regions = np.concatenate([label_dict[meta] <span class="hljs-keyword">for</span> meta <span class="hljs-keyword">in</span> regions_indexs],axis=<span class="hljs-number">0</span>).astype(int)
    <span class="hljs-keyword">return</span> dna_regions,label_regions

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model_train</span><span class="hljs-params">(regions,model,batchsize,dna_dict,label_dict,)</span>:</span>
    train_loss = []
    regions_len = len(regions)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, regions_len , batchsize):
        regions_batch = [regions[i+j] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(batchsize) <span class="hljs-keyword">if</span> (i+j) &lt; regions_len]
        seq_batch ,lab_batch = loadRegions(regions_batch,dna_dict,label_dict)
        _loss= model.train_on_batch(seq_batch, lab_batch)
        train_loss.append(_loss)
    <span class="hljs-keyword">return</span> np.mean(train_loss) 

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model_eval</span><span class="hljs-params">(regions,model,batchsize,dna_dict,label_dict,)</span>:</span>
    loss = []
    pred =[]
    lab =[]
    regions_len = len(regions)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, regions_len , batchsize):
        regions_batch = [regions[i+j] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(batchsize) <span class="hljs-keyword">if</span> (i+j) &lt; regions_len]
        seq_batch ,lab_batch = loadRegions(regions_batch,dna_dict,label_dict)
        _loss,_pred = model.eval_on_batch(seq_batch, lab_batch)
        loss.append(_loss)
        lab.extend(lab_batch)
        pred.extend(_pred)
        ground_truths = np.eye(np.array(pred).shape[<span class="hljs-number">1</span>])[np.array(pred).argmax(<span class="hljs-number">1</span>)]
    <span class="hljs-keyword">return</span> np.mean(loss), np.array(lab),np.array(pred),ground_truths

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">model_predict</span><span class="hljs-params">(regions,model,batchsize,dna_dict,label_dict,)</span>:</span>
    lab  = []
    pred = []
    regions_len = len(regions)
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>, len(regions), batchsize):
        regions_batch = [regions[i+j] <span class="hljs-keyword">for</span> j <span class="hljs-keyword">in</span> range(batchsize) <span class="hljs-keyword">if</span> (i+j) &lt; regions_len]
        seq_batch ,lab_batch = loadRegions(regions_batch,dna_dict,label_dict)
        _pred = model.test_on_batch(seq_batch)
        lab.extend(lab_batch)
        pred.extend(_pred)        
        ground_truths = np.eye(np.array(pred).shape[<span class="hljs-number">1</span>])[np.array(pred).argmax(<span class="hljs-number">1</span>)]
    <span class="hljs-keyword">return</span> np.array(lab), np.array(pred) ,ground_truths


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">ROC</span><span class="hljs-params">(label,pred)</span>:</span>
    <span class="hljs-keyword">if</span> len(np.unique(np.array(label).reshape(<span class="hljs-number">-1</span>)))  == <span class="hljs-number">1</span>:
        print(<span class="hljs-string">&quot;all the labels are the same !&quot;</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>
    <span class="hljs-keyword">else</span>:
        label = np.array(label).reshape(<span class="hljs-number">-1</span>)
        pred = np.array(pred).reshape(<span class="hljs-number">-1</span>)
        <span class="hljs-keyword">return</span> roc_auc_score(label,pred)
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">auPR</span><span class="hljs-params">(label,pred)</span>:</span>
    <span class="hljs-keyword">if</span> len(np.unique(np.array(label).reshape(<span class="hljs-number">-1</span>)))  == <span class="hljs-number">1</span>:
        print(<span class="hljs-string">&quot;all the labels are the same !&quot;</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>
    <span class="hljs-keyword">else</span>:
        label = np.array(label).reshape(<span class="hljs-number">-1</span>)
        pred = np.array(pred).reshape(<span class="hljs-number">-1</span>)
        precision, recall, thresholds = precision_recall_curve(label,pred)
        <span class="hljs-keyword">return</span> auc(recall,precision)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">MCC</span><span class="hljs-params">(label,truths)</span>:</span>
    <span class="hljs-keyword">if</span> len(np.unique(np.array(label).reshape(<span class="hljs-number">-1</span>)))  == <span class="hljs-number">1</span>:
        print(<span class="hljs-string">&quot;all the labels are the same !&quot;</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>
    <span class="hljs-keyword">else</span>:
        label = np.array(label).reshape(<span class="hljs-number">-1</span>)
        truths = np.array(truths).reshape(<span class="hljs-number">-1</span>)
        <span class="hljs-keyword">return</span> matthews_corrcoef(label, truths)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Spec</span><span class="hljs-params">(label,truths)</span>:</span>
    <span class="hljs-keyword">if</span> len(np.unique(np.array(label).reshape(<span class="hljs-number">-1</span>)))  == <span class="hljs-number">1</span>:
        print(<span class="hljs-string">&quot;all the labels are the same !&quot;</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>
    <span class="hljs-keyword">else</span>:
        label = np.array(label).reshape(<span class="hljs-number">-1</span>)
        truths = np.array(truths).reshape(<span class="hljs-number">-1</span>)
        <span class="hljs-keyword">return</span> precision_score(label, truths)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">Sen</span><span class="hljs-params">(label,truths)</span>:</span>
    <span class="hljs-keyword">if</span> len(np.unique(np.array(label).reshape(<span class="hljs-number">-1</span>)))  == <span class="hljs-number">1</span>:
        print(<span class="hljs-string">&quot;all the labels are the same !&quot;</span>)
        <span class="hljs-keyword">return</span> <span class="hljs-number">0</span>
    <span class="hljs-keyword">else</span>:
        label = np.array(label).reshape(<span class="hljs-number">-1</span>)
        truths = np.array(truths).reshape(<span class="hljs-number">-1</span>)
        <span class="hljs-keyword">return</span> recall_score(label, truths)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">metrics</span><span class="hljs-params">(lab,pred,truths,Type=<span class="hljs-string">&apos;test&apos;</span>,loss=None)</span>:</span>
        <span class="hljs-keyword">if</span> Type == <span class="hljs-string">&apos;Valid&apos;</span>:
            training_color = <span class="hljs-string">&apos;\033[0;34m&apos;</span>
        <span class="hljs-keyword">elif</span> Type == <span class="hljs-string">&apos;Test&apos;</span>:
            training_color = <span class="hljs-string">&apos;\033[0;35m&apos;</span>
        <span class="hljs-keyword">else</span>:
            training_color = <span class="hljs-string">&apos;\033[0;36m&apos;</span>

        auPRC_dict={}
        auROC_dict ={}
        MCC_dict ={}
        Spec_dict ={}
        Sen_dict ={}
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(histones)):
            auPRC_dict[histones[i]] = auPR(lab[:,i],pred[:,i])
            auROC_dict[histones[i]] = ROC(lab[:,i],pred[:,i])
            MCC_dict[histones[i]] = MCC(lab[:,i],truths[:,i])
            Spec_dict[histones[i]] = Spec(lab[:,i],truths[:,i])   
            Sen_dict[histones[i]] = Sen(lab[:,i],truths[:,i])
        print_str = training_color + <span class="hljs-string">&apos;\t%s\t%s\tauROC : %.4f,auPRC : %.4f,MCC : %.4f,Specificity : %.4f,Sensitivity : %.4f\033[0m&apos;</span>
        print(<span class="hljs-string">&apos;-&apos;</span>*<span class="hljs-number">25</span>+Type+<span class="hljs-string">&apos;-&apos;</span>*<span class="hljs-number">25</span>)
        <span class="hljs-keyword">if</span> loss <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-keyword">None</span>: loss_str = <span class="hljs-string">&apos;,Loss : %.4f&apos;</span>%loss
        <span class="hljs-keyword">else</span> :loss_str =<span class="hljs-string">&apos;&apos;</span>
        print(<span class="hljs-string">&apos;\033[0;36m%s\tTotalMean\tauROC : %.4f,auPRC : %.4f,MCC : %.4f,Specificity : %.4f,Sensitivity : %.4f%s\033[0m&apos;</span>%(Type,np.mean(list(auROC_dict.values())),np.mean(list(auPRC_dict.values())),np.mean(list(MCC_dict.values())),np.mean(list(Spec_dict.values())),np.mean(list(Sen_dict.values())),loss_str) )
        <span class="hljs-keyword">for</span> histone <span class="hljs-keyword">in</span> histones:
                print(print_str%(Type,histone.ljust(<span class="hljs-number">10</span>),auROC_dict[histone],auPRC_dict[histone],MCC_dict[histone],Spec_dict[histone],Sen_dict[histone]))
        <span class="hljs-keyword">return</span> auPRC_dict,auROC_dict,MCC_dict,Spec_dict,Sen_dict
</code></pre>
<p><strong>train_dna.py</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">from</span> model_dna <span class="hljs-keyword">import</span> DeepHistone
<span class="hljs-keyword">import</span> copy
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> utils_dna <span class="hljs-keyword">import</span> metrics,model_train,model_eval,model_predict
<span class="hljs-keyword">import</span> torch
<span class="hljs-comment">#setting </span>
batchsize=<span class="hljs-number">20</span>
train_file = <span class="hljs-string">&apos;onehot_rice_train.npz&apos;</span>
test_file = <span class="hljs-string">&apos;onehot_rice_test.npz&apos;</span>
model_save_file = <span class="hljs-string">&apos;results/model.txt&apos;</span>
lab_save_file =<span class="hljs-string">&apos;results/label.txt&apos;</span>
pred_save_file =<span class="hljs-string">&apos;results/pred.txt&apos;</span>

print(<span class="hljs-string">&apos;Begin loading data...&apos;</span>)
<span class="hljs-keyword">with</span> np.load(train_file) <span class="hljs-keyword">as</span> f:
    indexs = f[<span class="hljs-string">&apos;keys&apos;</span>]
    dna_dict = dict(zip(f[<span class="hljs-string">&apos;keys&apos;</span>],f[<span class="hljs-string">&apos;DNAseq&apos;</span>]))
    lab_dict = dict(zip(f[<span class="hljs-string">&apos;keys&apos;</span>],f[<span class="hljs-string">&apos;labels&apos;</span>]))
np.random.shuffle(indexs)
idx_len = len(indexs)
train_index=indexs[:int(idx_len*<span class="hljs-number">4</span>/<span class="hljs-number">5</span>)]
valid_index=indexs[int(idx_len*<span class="hljs-number">4</span>/<span class="hljs-number">5</span>):]






use_gpu = torch.cuda.is_available()
model = DeepHistone(use_gpu)
print(<span class="hljs-string">&apos;Begin training model...&apos;</span>)
best_model = copy.deepcopy(model)
best_valid_auPRC=<span class="hljs-number">0</span>
best_valid_loss = np.float64(<span class="hljs-string">&apos;Inf&apos;</span>)
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">50</span>):
    np.random.shuffle(train_index)
    train_loss= model_train(train_index,model,batchsize,dna_dict,lab_dict,)
    valid_loss,valid_lab,valid_pred,valid_truths= model_eval(valid_index, model,batchsize,dna_dict,lab_dict,)
    valid_auPRC,valid_auROC,valid_MCC,valid_Spec,valid_Sen= metrics(valid_lab,valid_pred,valid_truths,<span class="hljs-string">&apos;Valid&apos;</span>,valid_loss)

    <span class="hljs-keyword">if</span> np.mean(list(valid_auPRC.values())) &gt;best_valid_auPRC:
        best_model = copy.deepcopy(model)

    <span class="hljs-keyword">if</span> valid_loss &lt; best_valid_loss: 
        early_stop_time = <span class="hljs-number">0</span>
        best_valid_loss = valid_loss    
    <span class="hljs-keyword">else</span>:
        model.updateLR(<span class="hljs-number">0.1</span>)
        early_stop_time += <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> early_stop_time &gt;= <span class="hljs-number">5</span>: <span class="hljs-keyword">break</span>


print(<span class="hljs-string">&apos;Begin predicting...&apos;</span>)
<span class="hljs-keyword">with</span> np.load(test_file) <span class="hljs-keyword">as</span> f:
    indexs2 = f[<span class="hljs-string">&apos;keys&apos;</span>]
    dna_dict2 = dict(zip(f[<span class="hljs-string">&apos;keys&apos;</span>],f[<span class="hljs-string">&apos;DNAseq&apos;</span>]))
    lab_dict2 = dict(zip(f[<span class="hljs-string">&apos;keys&apos;</span>],f[<span class="hljs-string">&apos;labels&apos;</span>]))

np.random.shuffle(indexs2)
idx_len2 = len(indexs)


<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">5</span>):
    test_index=indexs2[int((i/<span class="hljs-number">5</span>)*idx_len2):int(((i+<span class="hljs-number">1</span>)/<span class="hljs-number">5</span>)*idx_len2)]
    test_lab,test_pred,test_truths = model_predict(test_index,best_model,batchsize,dna_dict2,lab_dict2,)    
    test_auPR,test_roc,test_MCC,test_Spec,test_Sen= metrics(test_lab,test_pred,test_truths,<span class="hljs-string">&apos;Test&apos;</span>)


print(<span class="hljs-string">&apos;Begin saving...&apos;</span>)
np.savetxt(lab_save_file, test_lab, fmt=<span class="hljs-string">&apos;%d&apos;</span>, delimiter=<span class="hljs-string">&apos;\t&apos;</span>)
np.savetxt(pred_save_file, test_pred, fmt=<span class="hljs-string">&apos;%.4f&apos;</span>, delimiter=<span class="hljs-string">&apos;\t&apos;</span>)
best_model.save_model(model_save_file)
torch.save(model,<span class="hljs-string">&apos;model1.pth&apos;</span>)

print(<span class="hljs-string">&apos;Finished.&apos;</span>)
</code></pre>
<h4 id="&#x6784;&#x5EFA;&#x603B;&#x6570;&#x636E;&#x96C6;">&#x6784;&#x5EFA;&#x603B;&#x6570;&#x636E;&#x96C6;</h4>
<p>20&#x4E2A;&#x54C1;&#x79CD;5&#x79CD;&#x7EC4;&#x86CB;&#x767D;&#x4FEE;&#x9970;&#x6240;&#x6709;&#x9633;&#x6027;</p>
<table>
<thead>
<tr>
<th>histone modifications</th>
<th>nums</th>
</tr>
</thead>
<tbody>
<tr>
<td>H3K4me3&#xFF08;1&#xFF09;</td>
<td>256721</td>
</tr>
<tr>
<td>H3K27ac&#xFF08;2&#xFF09;</td>
<td>227526</td>
</tr>
<tr>
<td>H3K4me1&#xFF08;3&#xFF09;</td>
<td>152355</td>
</tr>
<tr>
<td>H3K27me3&#xFF08;4&#xFF09;</td>
<td>136028</td>
</tr>
<tr>
<td>H3K9me2&#xFF08;5&#xFF09;</td>
<td>118142</td>
</tr>
<tr>
<td>total</td>
<td>890772</td>
</tr>
<tr>
<td>total&#xFF08;after check&#xFF09;</td>
<td>887724</td>
</tr>
</tbody>
</table>
<pre><code class="lang-shell">#!/bin/bash

for n in $(cat histone)
do
    for i in $(cat id.txt)
    do
    python3 target_mark5.0.py -f ${i}.fasta.fai -b data/signal_area/sort_${i}.bed -o target_${i}_${n}.bed -s ${i} -m ${n} &amp;&amp; paste $[i}.fasta target_${i}_${n}.bed &gt; data/dataset/ex1/${n}.csv &amp;&amp; awk &apos;$3 != 0 {print}&apos; data/dataset/ex1/${n}.csv &gt;&gt; dataset_all.csv | awk &apos;{print $3}&apos; | uniq -c  
    done
done
</code></pre>
<pre><code class="lang-shell">bsub -q gpu -o deep_histone_result.out -e histone.err &apos;python3 train_dna.py&apos;
</code></pre>
<h3 id="nttransformer"><strong>Nt_Transformer</strong></h3>
<p><img src="https://www.biorxiv.org/content/biorxiv/early/2021/01/29/2021.01.28.428629/F1.large.jpg?width=800&amp;height=600&amp;carousel=1" alt="img" style="zoom: 50%;"></p>
<p><strong>&#x53C2;&#x8003;&#x6587;&#x732E;&#xFF1A;</strong></p>
<p><a href="https://www.biorxiv.org/content/10.1101/2021.01.28.428629v1" target="_blank">NUCLEIC TRANSFORMER: DEEP LEARNING ON NUCLEIC ACIDS WITH SELF-ATTENTION AND CONVOLUTIONS</a></p>
<p>github:<a href="https://github.com/Shujun-He/Nucleic-Transformer" target="_blank">https://github.com/Shujun-He/Nucleic-Transformer</a></p>
<h4 id="transformer&#xFF1A;attention-is-all-you-need">Transformer&#xFF1A;Attention is All You Need</h4>
<p><strong>&#x53C2;&#x8003;&#x6587;&#x732E;&#xFF1A;</strong></p>
<p><a href="https://arxiv.org/abs/1706.03762" target="_blank">Attention Is All You Need</a></p>
<p><strong>&#x8BBA;&#x6587;&#x89E3;&#x6790;&#xFF1A;</strong></p>
<p><a href="https://blog.csdn.net/weixin_42431920/article/details/110731751" target="_blank">https://blog.csdn.net/weixin_42431920/article/details/110731751</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/48508221" target="_blank">https://zhuanlan.zhihu.com/p/48508221</a></p>
<p><a href="https://www.bilibili.com/video/BV1Di4y1c7Zm?from=search&amp;seid=15155897634554281203" target="_blank">https://www.bilibili.com/video/BV1Di4y1c7Zm?from=search&amp;seid=15155897634554281203</a></p>
<p><a href="D:\cmder_mini\gitbook\xfchen\&#x80CC;&#x666F;&#x77E5;&#x8BC6;\&#x6DF1;&#x5EA6;&#x5B66;&#x4E60;.md" target="_blank"><strong>&#x5B66;&#x4E60;&#x7B14;&#x8BB0;</strong></a></p>
<h4 id="&#x6A21;&#x578B;&#x67B6;&#x6784;"><strong>&#x6A21;&#x578B;&#x67B6;&#x6784;</strong></h4>
<pre><code>/public/home/xwli/xwzhang/deeplearningDATA/rice/Nt_Transformer/

&#x251C;&#x2500;&#x2500; data/
&#x251C;&#x2500;&#x2500; results/
&#x2502;   &#x2514;&#x2500;&#x2500; model.txt
&#x2502;   &#x2514;&#x2500;&#x2500; label.txt
&#x2502;   &#x2514;&#x2500;&#x2500; pred.txt
&#x251C;&#x2500;&#x2500; Dataset.py 
&#x251C;&#x2500;&#x2500; evalute_test.py
&#x251C;&#x2500;&#x2500; Functions.py
&#x251C;&#x2500;&#x2500; Logger.py
&#x251C;&#x2500;&#x2500; LrScheduler.py
&#x251C;&#x2500;&#x2500; Metrics.py
&#x251C;&#x2500;&#x2500; Network.py
&#x251C;&#x2500;&#x2500; train.py
&#x251C;&#x2500;&#x2500; run.sh
&#x251C;&#x2500;&#x2500; test.sh
</code></pre><p><strong>Dataset.py</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> pickle
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> torch


nt_int={
<span class="hljs-string">&quot;A&quot;</span>: <span class="hljs-number">0</span>,
<span class="hljs-string">&quot;T&quot;</span>: <span class="hljs-number">1</span>,
<span class="hljs-string">&quot;G&quot;</span>: <span class="hljs-number">2</span>,
<span class="hljs-string">&quot;C&quot;</span>: <span class="hljs-number">3</span>,}

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">nucleatide2int</span><span class="hljs-params">(nt_sequence,target_length=None)</span>:</span>
    int_sequence=[]
    <span class="hljs-keyword">for</span> nt <span class="hljs-keyword">in</span> nt_sequence:
        nt=nt.upper()
        <span class="hljs-keyword">if</span> nt <span class="hljs-keyword">in</span> nt_int:
            int_sequence.append(nt_int[nt])
    int_sequence=np.asarray(int_sequence,dtype=<span class="hljs-string">&apos;int32&apos;</span>)
    <span class="hljs-keyword">if</span> target_length:
        int_sequence=np.pad(int_sequence,(<span class="hljs-number">0</span>,target_length-len(int_sequence)),constant_values=<span class="hljs-number">-1</span>)
    <span class="hljs-keyword">return</span> int_sequence


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">ViraminerDataset</span><span class="hljs-params">(torch.utils.data.Dataset)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,sequences,labels)</span>:</span>
        self.data=[]
        <span class="hljs-keyword">for</span> seq <span class="hljs-keyword">in</span> sequences:
            self.data.append(nucleatide2int(seq))

        self.data=np.asarray(self.data,dtype=<span class="hljs-string">&apos;int&apos;</span>)
        self.labels=np.asarray(labels,dtype=<span class="hljs-string">&apos;int&apos;</span>)

        print(self.data.shape)
        print(self.labels.shape)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__len__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> len(self.labels)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__getitem__</span><span class="hljs-params">(self,idx)</span>:</span>
        <span class="hljs-keyword">return</span> {<span class="hljs-string">&apos;data&apos;</span>:self.data[idx], <span class="hljs-string">&apos;labels&apos;</span>:self.labels[idx]}
</code></pre>
<p><strong>evalute_test.py</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">from</span> Functions <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">from</span> Dataset <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">from</span> Network <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">from</span> LrScheduler <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">import</span> Metrics
<span class="hljs-keyword">from</span> Logger <span class="hljs-keyword">import</span> CSVLogger
<span class="hljs-keyword">import</span> argparse
<span class="hljs-keyword">try</span>:
    <span class="hljs-comment">#from apex.parallel import DistributedDataParallel as DDP</span>
    <span class="hljs-keyword">from</span> apex.fp16_utils <span class="hljs-keyword">import</span> *
    <span class="hljs-keyword">from</span> apex <span class="hljs-keyword">import</span> amp, optimizers
    <span class="hljs-keyword">from</span> apex.multi_tensor_apply <span class="hljs-keyword">import</span> multi_tensor_applier
<span class="hljs-keyword">except</span> ImportError:
    <span class="hljs-keyword">raise</span> ImportError(<span class="hljs-string">&quot;Please install apex from https://www.github.com/nvidia/apex to run this example.&quot;</span>)
<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_args</span><span class="hljs-params">()</span>:</span>
    parser = argparse.ArgumentParser()
    parser.add_argument(<span class="hljs-string">&apos;--gpu_id&apos;</span>, type=str, default=<span class="hljs-string">&apos;0,1&apos;</span>,  help=<span class="hljs-string">&apos;which gpu to use&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--path&apos;</span>, type=str, default=<span class="hljs-string">&apos;../&apos;</span>, help=<span class="hljs-string">&apos;path of csv file with DNA sequences and labels&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--epochs&apos;</span>, type=int, default=<span class="hljs-number">150</span>, help=<span class="hljs-string">&apos;number of epochs to train&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--batch_size&apos;</span>, type=int, default=<span class="hljs-number">24</span>, help=<span class="hljs-string">&apos;size of each batch during training&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--weight_decay&apos;</span>, type=float, default=<span class="hljs-number">0</span>, help=<span class="hljs-string">&apos;weight dacay used in optimizer&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--ntoken&apos;</span>, type=int, default=<span class="hljs-number">4</span>, help=<span class="hljs-string">&apos;number of tokens to represent DNA nucleotides (should always be 4)&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--nclass&apos;</span>, type=int, default=<span class="hljs-number">2</span>, help=<span class="hljs-string">&apos;number of classes from the linear decoder&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--ninp&apos;</span>, type=int, default=<span class="hljs-number">512</span>, help=<span class="hljs-string">&apos;ninp for transformer encoder&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--nhead&apos;</span>, type=int, default=<span class="hljs-number">8</span>, help=<span class="hljs-string">&apos;nhead for transformer encoder&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--nhid&apos;</span>, type=int, default=<span class="hljs-number">2048</span>, help=<span class="hljs-string">&apos;nhid for transformer encoder&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--nlayers&apos;</span>, type=int, default=<span class="hljs-number">6</span>, help=<span class="hljs-string">&apos;nlayers for transformer encoder&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--save_freq&apos;</span>, type=int, default=<span class="hljs-number">1</span>, help=<span class="hljs-string">&apos;saving checkpoints per save_freq epochs&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--dropout&apos;</span>, type=float, default=<span class="hljs-number">.1</span>, help=<span class="hljs-string">&apos;transformer dropout&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--warmup_steps&apos;</span>, type=int, default=<span class="hljs-number">3200</span>, help=<span class="hljs-string">&apos;training schedule warmup steps&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--lr_scale&apos;</span>, type=float, default=<span class="hljs-number">0.1</span>, help=<span class="hljs-string">&apos;learning rate scale&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--nmute&apos;</span>, type=int, default=<span class="hljs-number">18</span>, help=<span class="hljs-string">&apos;number of mutations during training&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--kmers&apos;</span>, type=int, nargs=<span class="hljs-string">&apos;+&apos;</span>, default=[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>], help=<span class="hljs-string">&apos;k-mers to be aggregated&apos;</span>)
    <span class="hljs-comment">#parser.add_argument(&apos;--kmer_aggregation&apos;, type=bool, default=True, help=&apos;k-mers to be aggregated&apos;)</span>
    parser.add_argument(<span class="hljs-string">&apos;--kmer_aggregation&apos;</span>, dest=<span class="hljs-string">&apos;kmer_aggregation&apos;</span>, action=<span class="hljs-string">&apos;store_true&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--no_kmer_aggregation&apos;</span>, dest=<span class="hljs-string">&apos;kmer_aggregation&apos;</span>, action=<span class="hljs-string">&apos;store_false&apos;</span>)
    parser.set_defaults(kmer_aggregation=<span class="hljs-keyword">True</span>)
    parser.add_argument(<span class="hljs-string">&apos;--nfolds&apos;</span>, type=int, default=<span class="hljs-number">5</span>, help=<span class="hljs-string">&apos;number of cross validation folds&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--fold&apos;</span>, type=int, default=<span class="hljs-number">0</span>, help=<span class="hljs-string">&apos;which fold to train&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--val_freq&apos;</span>, type=int, default=<span class="hljs-number">1</span>, help=<span class="hljs-string">&apos;which fold to train&apos;</span>)
    opts = parser.parse_args()
    <span class="hljs-keyword">return</span> opts


opts=get_args()
<span class="hljs-comment">#gpu selection</span>
os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = opts.gpu_id
device = torch.device(<span class="hljs-string">&apos;cuda&apos;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&apos;cpu&apos;</span>)
<span class="hljs-comment">#lr=0</span>

<span class="hljs-comment">#checkpointing</span>
checkpoints_folder=<span class="hljs-string">&apos;checkpoints_fold{}&apos;</span>.format((opts.fold))
csv_file=<span class="hljs-string">&apos;log_fold{}.csv&apos;</span>.format((opts.fold))
columns=[<span class="hljs-string">&apos;epoch&apos;</span>,<span class="hljs-string">&apos;train_loss&apos;</span>,<span class="hljs-string">&apos;train_acc&apos;</span>,<span class="hljs-string">&apos;recon_acc&apos;</span>,
         <span class="hljs-string">&apos;val_loss&apos;</span>,<span class="hljs-string">&apos;val_auc&apos;</span>,<span class="hljs-string">&apos;val_acc&apos;</span>,<span class="hljs-string">&apos;val_sens&apos;</span>,<span class="hljs-string">&apos;val_spec&apos;</span>]
<span class="hljs-comment">#logger=CSVLogger(columns,csv_file)</span>

<span class="hljs-comment">#build model and logger</span>
MODELS=[]
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">3</span>):
    model=NucleicTransformer(opts.ntoken, opts.nclass, opts.ninp, opts.nhead, opts.nhid,
                           opts.nlayers, opts.kmer_aggregation, kmers=opts.kmers,
                           dropout=opts.dropout).to(device)
    optimizer=torch.optim.Adam(model.parameters(), weight_decay=opts.weight_decay)
    criterion=nn.CrossEntropyLoss(reduction=<span class="hljs-string">&apos;none&apos;</span>)
    lr_schedule=lr_AIAYN(optimizer,opts.ninp,opts.warmup_steps,opts.lr_scale)
    <span class="hljs-comment"># Initialization</span>
    opt_level = <span class="hljs-string">&apos;O1&apos;</span>
    model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)
    model = nn.DataParallel(model)


    pytorch_total_params = sum(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters())
    print(<span class="hljs-string">&apos;Total number of paramters: {}&apos;</span>.format(pytorch_total_params))

    model.load_state_dict(torch.load(<span class="hljs-string">&quot;best_weights/fold0top{}.ckpt&quot;</span>.format(i+<span class="hljs-number">1</span>)))
    model.eval()
    MODELS.append(model)

dict=MODELS[<span class="hljs-number">0</span>].module.state_dict()
<span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> dict:
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>,len(MODELS)):
        dict[key]=dict[key]+MODELS[i].module.state_dict()[key]

    dict[key]=dict[key]/float(len(MODELS))

MODELS[<span class="hljs-number">0</span>].module.load_state_dict(dict)
avg_model=MODELS[<span class="hljs-number">0</span>]

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">geometric_mean</span><span class="hljs-params">(preds)</span>:</span>
    gmean=np.ones(preds.shape[<span class="hljs-number">1</span>:])

    <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> preds:
        gmean=gmean*pred

    gmean=gmean**(<span class="hljs-number">1</span>/len(preds))
    <span class="hljs-keyword">return</span> gmean

df=pd.read_csv(<span class="hljs-string">&apos;../fullset_test.csv&apos;</span>,header=<span class="hljs-keyword">None</span>)

seqs=[]
labels=[]

<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(df)):
    seqs.append(nucleatide2int(df.iloc[i,<span class="hljs-number">1</span>]))
    labels.append(df.iloc[i,<span class="hljs-number">2</span>])
labels=np.asarray(labels).astype(<span class="hljs-string">&quot;int&quot;</span>)
seqs=np.asarray(seqs).astype(<span class="hljs-string">&quot;int&quot;</span>)


batch_size=<span class="hljs-number">128</span>
batches=np.around(len(df)/batch_size+<span class="hljs-number">0.5</span>).astype(<span class="hljs-string">&apos;int&apos;</span>)
preds=[]
softmax = nn.Softmax(dim=<span class="hljs-number">1</span>)
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(range(batches)):
    <span class="hljs-keyword">with</span> torch.no_grad():
        outputs=[]
        <span class="hljs-comment">#for model in MODELS:</span>
        x=torch.Tensor(seqs[i*batch_size:(i+<span class="hljs-number">1</span>)*batch_size]).to(device).long()
        y=softmax(avg_model(x))
        <span class="hljs-comment">#outputs.append(softmax(y).cpu().numpy())</span>
        <span class="hljs-keyword">for</span> vec <span class="hljs-keyword">in</span> y:
            preds.append(vec.cpu().numpy())

<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics
preds=np.asarray(preds)
auc=metrics.roc_auc_score(labels,preds[:,<span class="hljs-number">1</span>])

<span class="hljs-keyword">with</span> open(<span class="hljs-string">&quot;test_results.p&quot;</span>,<span class="hljs-string">&apos;wb+&apos;</span>) <span class="hljs-keyword">as</span> f:
    pickle.dump([labels,preds],f)


print(auc)
<span class="hljs-keyword">with</span> open(<span class="hljs-string">&quot;test_score.txt&quot;</span>,<span class="hljs-string">&apos;w+&apos;</span>) <span class="hljs-keyword">as</span> f:
    f.write(<span class="hljs-string">&quot;test auc score: {}&quot;</span>.format(auc))




<span class="hljs-comment"># for i in range(3,10):</span>
    <span class="hljs-comment"># ngrams=np.arange(2,i)</span>
    <span class="hljs-comment"># print(ngrams)</span>
    <span class="hljs-comment"># train_fold(0,ngrams)</span>
<span class="hljs-comment"># # train_fold(0,[2,3,4])</span>
</code></pre>
<p><strong>Functions.py</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F
<span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-keyword">import</span> Metrics
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> random

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">seed_everything</span><span class="hljs-params">(seed=<span class="hljs-number">42</span>)</span>:</span>
    random.seed(seed)
    os.environ[<span class="hljs-string">&apos;PYTHONHASHSEED&apos;</span>] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = <span class="hljs-keyword">True</span>

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_best_weights_from_fold</span><span class="hljs-params">(fold, record, top=<span class="hljs-number">3</span>)</span>:</span>
    csv_file=<span class="hljs-string">&apos;{}log_fold{}.csv&apos;</span>.format(record, fold)

    history=pd.read_csv(csv_file)
    scores=np.asarray(history.val_auc)
    top_epochs=scores.argsort()[<span class="hljs-number">-3</span>:][::<span class="hljs-number">-1</span>]
    print(scores[top_epochs])
    os.system(<span class="hljs-string">&apos;mkdir best_weights&apos;</span>)

    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(top):
        weights_path=<span class="hljs-string">&apos;{}checkpoints_fold{}/epoch{}.ckpt&apos;</span>.format(record, fold,history.epoch[top_epochs[i]])
        print(weights_path)
        os.system(<span class="hljs-string">&apos;cp {} best_weights/fold{}top{}.ckpt&apos;</span>.format(weights_path,fold,i+<span class="hljs-number">1</span>))
    os.system(<span class="hljs-string">&apos;rm -r {}checkpoints_fold{}&apos;</span>.format(record, fold))

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">smoothcrossentropyloss</span><span class="hljs-params">(pred,gold,n_class=<span class="hljs-number">2</span>,smoothing=<span class="hljs-number">0.05</span>)</span>:</span>
    gold = gold.contiguous().view(<span class="hljs-number">-1</span>)
    one_hot = torch.zeros_like(pred).scatter(<span class="hljs-number">1</span>, gold.view(<span class="hljs-number">-1</span>, <span class="hljs-number">1</span>), <span class="hljs-number">1</span>)
    one_hot = one_hot * (<span class="hljs-number">1</span> - smoothing) + (<span class="hljs-number">1</span> - one_hot) * smoothing / (n_class - <span class="hljs-number">1</span>)
    log_prb = F.log_softmax(pred, dim=<span class="hljs-number">1</span>)
    loss = -(one_hot * log_prb)
    <span class="hljs-comment">#loss=loss.sum(1).mean()</span>
    <span class="hljs-keyword">return</span> loss

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">mutate_dna_sequence</span><span class="hljs-params">(sequence,nmute=<span class="hljs-number">15</span>)</span>:</span>
    mutation=torch.randint(<span class="hljs-number">0</span>,<span class="hljs-number">4</span>,size=(sequence.shape[<span class="hljs-number">0</span>],nmute))
    to_mutate = torch.randperm(sequence.shape[<span class="hljs-number">1</span>])[:nmute]
    sequence[:,to_mutate]=mutation
    <span class="hljs-keyword">return</span> sequence

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_MLM_mask</span><span class="hljs-params">(sequence,nmask=<span class="hljs-number">12</span>)</span>:</span>
    mask=np.zeros(sequence.shape,dtype=<span class="hljs-string">&apos;bool&apos;</span>)
    to_mask=np.random.choice(len(sequence[<span class="hljs-number">0</span>]),size=(nmask),replace=<span class="hljs-keyword">False</span>)
    mask[:,to_mask]=<span class="hljs-keyword">True</span>
    <span class="hljs-keyword">return</span> mask

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_complementary_sequence</span><span class="hljs-params">(sequence)</span>:</span>
    complementary_sequence=sequence.copy()
    complementary_sequence[sequence==<span class="hljs-number">0</span>]=<span class="hljs-number">1</span>
    complementary_sequence[sequence==<span class="hljs-number">1</span>]=<span class="hljs-number">0</span>
    complementary_sequence[sequence==<span class="hljs-number">2</span>]=<span class="hljs-number">3</span>
    complementary_sequence[sequence==<span class="hljs-number">3</span>]=<span class="hljs-number">2</span>
    complementary_sequence=complementary_sequence[:,::<span class="hljs-number">-1</span>]
    <span class="hljs-keyword">return</span> complementary_sequence

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">update_lr</span><span class="hljs-params">(optimizer, lr)</span>:</span>
    <span class="hljs-keyword">for</span> param_group <span class="hljs-keyword">in</span> optimizer.param_groups:
        param_group[<span class="hljs-string">&apos;lr&apos;</span>] = lr

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">save_weights</span><span class="hljs-params">(model,optimizer,epoch,folder)</span>:</span>
    <span class="hljs-keyword">if</span> os.path.isdir(folder)==<span class="hljs-keyword">False</span>:
        os.makedirs(folder,exist_ok=<span class="hljs-keyword">True</span>)
    torch.save(model.state_dict(), folder+<span class="hljs-string">&apos;/epoch{}.ckpt&apos;</span>.format(epoch+<span class="hljs-number">1</span>))



<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">validate</span><span class="hljs-params">(model,device,dataset,batch_size=<span class="hljs-number">64</span>)</span>:</span>
    batches=len(dataset)
    model.train(<span class="hljs-keyword">False</span>)
    total=<span class="hljs-number">0</span>
    predictions=[]
    outputs=[]
    ground_truths=[]
    loss=<span class="hljs-number">0</span>
    criterion=nn.CrossEntropyLoss()
    <span class="hljs-keyword">with</span> torch.no_grad():
        <span class="hljs-keyword">for</span> data <span class="hljs-keyword">in</span> tqdm(dataset):
            X=data[<span class="hljs-string">&apos;data&apos;</span>].to(device)
            Y=data[<span class="hljs-string">&apos;labels&apos;</span>].to(device)

            output= model(X)
            <span class="hljs-keyword">del</span> X
            loss+=criterion(output,Y)
            classification_predictions = torch.argmax(output,dim=<span class="hljs-number">1</span>).squeeze()
            <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> classification_predictions:
                predictions.append(pred.cpu().numpy())
            <span class="hljs-keyword">for</span> vector <span class="hljs-keyword">in</span> output:
                outputs.append(vector.cpu().numpy())
            <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> Y:
                ground_truths.append(t.cpu().numpy())
            <span class="hljs-keyword">del</span> output
    torch.cuda.empty_cache()
    val_loss=(loss/batches).cpu()
    ground_truths=np.asarray(ground_truths)
    predictions=np.asarray(predictions)
    outputs=np.asarray(outputs)
    <span class="hljs-comment">#print(predictions)</span>
    <span class="hljs-comment">#print(ground_truths)</span>
    <span class="hljs-comment">#score=metrics.cohen_kappa_score(ground_truths,predictions,weights=&apos;quadratic&apos;)</span>
    val_acc=Metrics.accuracy(predictions,ground_truths)
    auc=metrics.roc_auc_score(ground_truths,outputs[:,<span class="hljs-number">1</span>])
    val_sens=Metrics.sensitivity(predictions,ground_truths)
    val_spec=Metrics.specificity(predictions,ground_truths)
    print(<span class="hljs-string">&apos;Val accuracy: {}, Val Loss: {}&apos;</span>.format(val_acc,val_loss))
    <span class="hljs-keyword">return</span> val_loss,auc,val_acc,val_sens,val_spec


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict</span><span class="hljs-params">(model,device,dataset,batch_size=<span class="hljs-number">64</span>)</span>:</span>
    batches=int(len(dataset.val_indices)/batch_size)+<span class="hljs-number">1</span>
    model.train(<span class="hljs-keyword">False</span>)
    total=<span class="hljs-number">0</span>
    ground_truths=dataset.labels[dataset.val_indices]
    predictions=[]
    attention_weights=[]
    loss=<span class="hljs-number">0</span>
    criterion=nn.CrossEntropyLoss()
    dataset.switch_mode(training=<span class="hljs-keyword">False</span>)
    dataset.update_batchsize(batch_size)
    <span class="hljs-keyword">with</span> torch.no_grad():
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> tqdm(range(len(dataset))):
            data=dataset[i]
            X=torch.Tensor(data[<span class="hljs-string">&apos;data&apos;</span>]).to(device,).long()
            Y=torch.Tensor(data[<span class="hljs-string">&apos;labels&apos;</span>]).to(device,dtype=torch.int64)
            directions=data[<span class="hljs-string">&apos;directions&apos;</span>]
            directions=directions.reshape(len(directions),<span class="hljs-number">1</span>)*np.ones(X.shape)
            directions=torch.Tensor(directions).to(device).long()
            output,_,_,aw= model(X,directions,<span class="hljs-keyword">None</span>)
            <span class="hljs-keyword">del</span> X
            loss+=criterion(output,Y)
            classification_predictions = torch.argmax(output,dim=<span class="hljs-number">1</span>).squeeze()
            <span class="hljs-keyword">for</span> pred <span class="hljs-keyword">in</span> output:
                predictions.append(pred.cpu().numpy())
            <span class="hljs-keyword">for</span> weight <span class="hljs-keyword">in</span> aw:
                attention_weights.append(weight.cpu().numpy())

            <span class="hljs-keyword">del</span> output
    torch.cuda.empty_cache()
    val_loss=(loss/batches).cpu()
    predictions=np.asarray(predictions)
    attention_weights=np.asarray(attention_weights)
    binary_predictions=predictions.copy()
    binary_predictions[binary_predictions==<span class="hljs-number">2</span>]=<span class="hljs-number">1</span>
    binary_ground_truths=ground_truths.copy()
    binary_ground_truths[binary_ground_truths==<span class="hljs-number">2</span>]=<span class="hljs-number">1</span>
    <span class="hljs-keyword">return</span> predictions,attention_weights,np.asarray(dataset.data[dataset.val_indices])
</code></pre>
<p><strong>Logger.py</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> csv
<span class="hljs-keyword">from</span> os <span class="hljs-keyword">import</span> path


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CSVLogger</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,columns,file)</span>:</span>
        self.columns=columns
        self.file=file
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.check_header():
            self._write_header()


    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">check_header</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">if</span> path.exists(self.file):
            header=<span class="hljs-keyword">True</span>
        <span class="hljs-keyword">else</span>:
            header=<span class="hljs-keyword">False</span>
        <span class="hljs-keyword">return</span> header


    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_write_header</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">with</span> open(self.file,<span class="hljs-string">&quot;a&quot;</span>) <span class="hljs-keyword">as</span> f:
            string=<span class="hljs-string">&quot;&quot;</span>
            <span class="hljs-keyword">for</span> attrib <span class="hljs-keyword">in</span> self.columns:
                string+=<span class="hljs-string">&quot;{},&quot;</span>.format(attrib)
            string=string[:len(string)<span class="hljs-number">-1</span>]
            string+=<span class="hljs-string">&quot;\n&quot;</span>
            f.write(string)
        <span class="hljs-keyword">return</span> self

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">log</span><span class="hljs-params">(self,row)</span>:</span>
        <span class="hljs-keyword">if</span> len(row)!=len(self.columns):
            <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&quot;Mismatch between row vector and number of columns in logger&quot;</span>)
        <span class="hljs-keyword">with</span> open(self.file,<span class="hljs-string">&quot;a&quot;</span>) <span class="hljs-keyword">as</span> f:
            string=<span class="hljs-string">&quot;&quot;</span>
            <span class="hljs-keyword">for</span> attrib <span class="hljs-keyword">in</span> row:
                string+=<span class="hljs-string">&quot;{},&quot;</span>.format(attrib)
            string=string[:len(string)<span class="hljs-number">-1</span>]
            string+=<span class="hljs-string">&quot;\n&quot;</span>
            f.write(string)
        <span class="hljs-keyword">return</span> self
</code></pre>
<p><strong>LrScheduler.py</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> csv
<span class="hljs-keyword">from</span> os <span class="hljs-keyword">import</span> path


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">CSVLogger</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,columns,file)</span>:</span>
        self.columns=columns
        self.file=file
        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.check_header():
            self._write_header()


    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">check_header</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">if</span> path.exists(self.file):
            header=<span class="hljs-keyword">True</span>
        <span class="hljs-keyword">else</span>:
            header=<span class="hljs-keyword">False</span>
        <span class="hljs-keyword">return</span> header


    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">_write_header</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">with</span> open(self.file,<span class="hljs-string">&quot;a&quot;</span>) <span class="hljs-keyword">as</span> f:
            string=<span class="hljs-string">&quot;&quot;</span>
            <span class="hljs-keyword">for</span> attrib <span class="hljs-keyword">in</span> self.columns:
                string+=<span class="hljs-string">&quot;{},&quot;</span>.format(attrib)
            string=string[:len(string)<span class="hljs-number">-1</span>]
            string+=<span class="hljs-string">&quot;\n&quot;</span>
            f.write(string)
        <span class="hljs-keyword">return</span> self

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">log</span><span class="hljs-params">(self,row)</span>:</span>
        <span class="hljs-keyword">if</span> len(row)!=len(self.columns):
            <span class="hljs-keyword">raise</span> Exception(<span class="hljs-string">&quot;Mismatch between row vector and number of columns in logger&quot;</span>)
        <span class="hljs-keyword">with</span> open(self.file,<span class="hljs-string">&quot;a&quot;</span>) <span class="hljs-keyword">as</span> f:
            string=<span class="hljs-string">&quot;&quot;</span>
            <span class="hljs-keyword">for</span> attrib <span class="hljs-keyword">in</span> row:
                string+=<span class="hljs-string">&quot;{},&quot;</span>.format(attrib)
            string=string[:len(string)<span class="hljs-number">-1</span>]
            string+=<span class="hljs-string">&quot;\n&quot;</span>
            f.write(string)
        <span class="hljs-keyword">return</span> self
</code></pre>
<p><strong>Metrics.py</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">accuracy</span><span class="hljs-params">(predictions,ground_truths)</span>:</span>
    <span class="hljs-keyword">return</span> np.sum(predictions==ground_truths)/len(ground_truths)


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">sensitivity</span><span class="hljs-params">(predictions,ground_truths)</span>:</span>
    <span class="hljs-string">&apos;&apos;&apos;
    Here it is assumed:
    0=negative
    1=positive
    &apos;&apos;&apos;</span>
    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>-len(predictions[(predictions==<span class="hljs-number">0</span>)*(ground_truths==<span class="hljs-number">1</span>)])/len(ground_truths[ground_truths==<span class="hljs-number">1</span>])



<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">specificity</span><span class="hljs-params">(predictions,ground_truths)</span>:</span>
    <span class="hljs-string">&apos;&apos;&apos;
    Here it is assumed:
    0=negative
    1=positive
    &apos;&apos;&apos;</span>
    <span class="hljs-keyword">return</span> <span class="hljs-number">1</span>-len(predictions[(predictions==<span class="hljs-number">1</span>)*(ground_truths==<span class="hljs-number">0</span>)])/len(ground_truths[ground_truths==<span class="hljs-number">0</span>])

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">MCC</span><span class="hljs-params">(predictions,ground_truths)</span>:</span>
    <span class="hljs-string">&apos;&apos;&apos;
    Here it is assumed:
    0=negative
    1=positive
    &apos;&apos;&apos;</span>
    N1=len(predictions[(predictions==<span class="hljs-number">0</span>)&amp;(ground_truths==<span class="hljs-number">1</span>)])
    N2=len(predictions[(predictions==<span class="hljs-number">1</span>)&amp;(ground_truths==<span class="hljs-number">0</span>)])
    N3=len(ground_truths[ground_truths==<span class="hljs-number">1</span>])
    N4=len(ground_truths[ground_truths==<span class="hljs-number">0</span>])
    sens=<span class="hljs-number">1</span>-N1/N3
    spec=<span class="hljs-number">1</span>-N2/N4
    denom=np.sqrt((<span class="hljs-number">1</span>+(N2-N1)/N3)*(<span class="hljs-number">1</span>+(N1-N2)/N4))
    <span class="hljs-keyword">return</span> (<span class="hljs-number">1</span>-sens-spec)/denom
</code></pre>
<p><strong>Network.py</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F


<span class="hljs-comment">#mish activation</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Mish</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super().__init__()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-comment">#inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!)</span>
        <span class="hljs-keyword">return</span> x *( torch.tanh(F.softplus(x)))


<span class="hljs-keyword">from</span> torch.nn.parameter <span class="hljs-keyword">import</span> Parameter
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">gem</span><span class="hljs-params">(x, p=<span class="hljs-number">3</span>, eps=<span class="hljs-number">1e-6</span>)</span>:</span>
    <span class="hljs-keyword">return</span> F.avg_pool1d(x.clamp(min=eps).pow(p), (x.size(<span class="hljs-number">-1</span>))).pow(<span class="hljs-number">1.</span>/p)
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">GeM</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, p=<span class="hljs-number">3</span>, eps=<span class="hljs-number">1e-6</span>)</span>:</span>
        super(GeM,self).__init__()
        self.p = Parameter(torch.ones(<span class="hljs-number">1</span>)*p)
        self.eps = eps
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-keyword">return</span> gem(x, p=self.p, eps=self.eps)
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__repr__</span><span class="hljs-params">(self)</span>:</span>
        <span class="hljs-keyword">return</span> self.__class__.__name__ + <span class="hljs-string">&apos;(&apos;</span> + <span class="hljs-string">&apos;p=&apos;</span> + <span class="hljs-string">&apos;{:.4f}&apos;</span>.format(self.p.data.tolist()[<span class="hljs-number">0</span>]) + <span class="hljs-string">&apos;, &apos;</span> + <span class="hljs-string">&apos;eps=&apos;</span> + str(self.eps) + <span class="hljs-string">&apos;)&apos;</span>


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">TransformerEncoderLayer</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-string">r&quot;&quot;&quot;TransformerEncoderLayer is made up of self-attn and feedforward network.
    This standard encoder layer is based on the paper &quot;Attention Is All You Need&quot;.
    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
    in a different way during application.

    Args:
        d_model: the number of expected features in the input (required).
        nhead: the number of heads in the multiheadattention models (required).
        dim_feedforward: the dimension of the feedforward network model (default=2048).
        dropout: the dropout value (default=0.1).
        activation: the activation function of intermediate layer, relu or gelu (default=relu).

    Examples::
        &gt;&gt;&gt; encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
        &gt;&gt;&gt; src = torch.rand(10, 32, 512)
        &gt;&gt;&gt; out = encoder_layer(src)
    &quot;&quot;&quot;</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, d_model, nhead, dim_feedforward=<span class="hljs-number">2048</span>, dropout=<span class="hljs-number">0.1</span>, activation=<span class="hljs-string">&quot;relu&quot;</span>)</span>:</span>
        super(TransformerEncoderLayer, self).__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout)
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        self.activation = Mish()


    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, src , src_mask = None, src_key_padding_mask = None)</span>:</span>
        src2,attention_weights = self.self_attn(src, src, src, attn_mask=src_mask,
                              key_padding_mask=src_key_padding_mask)
        src = src + self.dropout1(src2)
        src = self.norm1(src)
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src = src + self.dropout2(src2)
        src = self.norm2(src)
        <span class="hljs-keyword">return</span> src,attention_weights


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">LinearDecoder</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,num_classes,ninp,dropout,pool=True,)</span>:</span>
        super(LinearDecoder, self).__init__()
        <span class="hljs-comment"># if pool:</span>
            <span class="hljs-comment"># self.pool_layer=GeM()</span>
        <span class="hljs-keyword">if</span> pool:
            self.classifier=nn.Linear(ninp,num_classes)
        <span class="hljs-keyword">else</span>:
            self.classifier=nn.Linear(ninp,num_classes)
        self.pool=pool
        self.pool_layer=GeM()

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self,x)</span>:</span>
        <span class="hljs-keyword">if</span> self.pool:
            <span class="hljs-comment"># max_x,_=torch.max(x,dim=1)</span>
            <span class="hljs-comment"># x=torch.cat([torch.mean(x,dim=1),max_x],dim=-1)</span>
            <span class="hljs-comment">#print(x.shape)</span>
            x=self.pool_layer(x.permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)).permute(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>).squeeze()
            <span class="hljs-comment">#print(x.shape)</span>
        x=self.classifier(x)
        <span class="hljs-keyword">return</span> x


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">PositionalEncoding</span><span class="hljs-params">(nn.Module)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, d_model, dropout=<span class="hljs-number">0.1</span>, max_len=<span class="hljs-number">5000</span>)</span>:</span>
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(<span class="hljs-number">0</span>, max_len, dtype=torch.float).unsqueeze(<span class="hljs-number">1</span>)
        div_term = torch.exp(torch.arange(<span class="hljs-number">0</span>, d_model, <span class="hljs-number">2</span>).float() * (-math.log(<span class="hljs-number">10000.0</span>) / d_model))
        pe[:, <span class="hljs-number">0</span>::<span class="hljs-number">2</span>] = torch.sin(position * div_term)
        pe[:, <span class="hljs-number">1</span>::<span class="hljs-number">2</span>] = torch.cos(position * div_term)
        pe = pe.unsqueeze(<span class="hljs-number">0</span>).transpose(<span class="hljs-number">0</span>, <span class="hljs-number">1</span>)
        self.register_buffer(<span class="hljs-string">&apos;pe&apos;</span>, pe)

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        x = x + self.pe[:x.size(<span class="hljs-number">0</span>), :]
        <span class="hljs-keyword">return</span> self.dropout(x)


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">K_mer_aggregate</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self,kmers,in_dim,out_dim,dropout=<span class="hljs-number">0.1</span>)</span>:</span>
        super(K_mer_aggregate, self).__init__()
        <span class="hljs-comment">#self.dropout=nn.Dropout(dropout)</span>
        self.convs=[]
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> kmers:
            print(i)
            self.convs.append(nn.Conv1d(in_dim,out_dim,i,padding=<span class="hljs-number">0</span>))
        self.convs=nn.ModuleList(self.convs)
        self.norm=nn.LayerNorm(out_dim)
        <span class="hljs-comment">#self.activation=nn.ReLU(inplace=True)</span>
        <span class="hljs-comment">#self.activation=Mish()</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self,x)</span>:</span>
        outputs=[]
        <span class="hljs-keyword">for</span> conv <span class="hljs-keyword">in</span> self.convs:
            outputs.append(conv(x))
        outputs=torch.cat(outputs,dim=<span class="hljs-number">2</span>)
        <span class="hljs-keyword">return</span> self.norm(outputs.permute(<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>))


<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">NucleicTransformer</span><span class="hljs-params">(nn.Module)</span>:</span>

    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self, ntoken, nclass, ninp, nhead, nhid, nlayers, kmer_aggregation, kmers, dropout=<span class="hljs-number">0.5</span>,return_aw=False)</span>:</span>
        super(NucleicTransformer, self).__init__()
        self.model_type = <span class="hljs-string">&apos;Transformer&apos;</span>
        self.src_mask = <span class="hljs-keyword">None</span>
        self.pos_encoder = PositionalEncoding(ninp, dropout)
        self.kmers=kmers
        <span class="hljs-comment">#if self.ngrams!=None:</span>
        self.kmer_aggregation=kmer_aggregation
        <span class="hljs-keyword">if</span> self.kmer_aggregation:
            self.k_mer_aggregate=K_mer_aggregate(kmers,ninp,ninp)
        <span class="hljs-keyword">else</span>:
            print(<span class="hljs-string">&quot;No kmer aggregation is chosen&quot;</span>)
        self.transformer_encoder = []
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(nlayers):
            self.transformer_encoder.append(TransformerEncoderLayer(ninp, nhead, nhid, dropout))
        self.transformer_encoder= nn.ModuleList(self.transformer_encoder)
        self.encoder = nn.Embedding(ntoken, ninp)
        <span class="hljs-comment">#self.directional_encoder = nn.Embedding(3, ninp//8)</span>
        self.ninp = ninp
        self.decoder = LinearDecoder(nclass,ninp,dropout)
        self.return_aw=<span class="hljs-keyword">False</span>


    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, src)</span>:</span>
        src = src.permute(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>)
        <span class="hljs-comment">#dir = dir.permute(1,0)</span>
        src = self.encoder(src) <span class="hljs-comment">#* math.sqrt(self.ninp)</span>
        <span class="hljs-comment">#dir = self.directional_encoder(dir)</span>
        <span class="hljs-comment">#src = torch.cat([src,dir],dim=-1)</span>
        src = self.pos_encoder(src)
        <span class="hljs-comment">#if self.ngrams!=None:</span>
        <span class="hljs-keyword">if</span> self.kmer_aggregation:
            kmer_output = self.k_mer_aggregate(src.permute(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>))
            <span class="hljs-comment">#src = torch.cat([src,kmer_output],dim=0)</span>
            src = kmer_output
        attention_weights=[]
        <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> self.transformer_encoder:
            src,attention_weights_layer=layer(src)
            attention_weights.append(attention_weights_layer)

        encoder_output = src.permute(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2</span>)
        <span class="hljs-comment">#print(encoder_output.shape)</span>
        output = self.decoder(encoder_output)
        <span class="hljs-keyword">if</span> self.return_aw:
            attention_weights=torch.stack(attention_weights).permute(<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)
            <span class="hljs-keyword">return</span> output, attention_weights
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">return</span> output
</code></pre>
<p><strong>train.py</strong></p>
<pre><code class="lang-python"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> torch
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> time
<span class="hljs-keyword">from</span> Functions <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">from</span> Dataset <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">from</span> Network <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">from</span> LrScheduler <span class="hljs-keyword">import</span> *
<span class="hljs-keyword">import</span> Metrics
<span class="hljs-keyword">from</span> Logger <span class="hljs-keyword">import</span> CSVLogger
<span class="hljs-keyword">import</span> argparse
<span class="hljs-keyword">from</span> tensorboardX <span class="hljs-keyword">import</span> SummaryWriter
<span class="hljs-keyword">from</span> torch.cuda.amp <span class="hljs-keyword">import</span> autocast <span class="hljs-keyword">as</span> autocast
<span class="hljs-keyword">from</span> torch.cuda.amp <span class="hljs-keyword">import</span> GradScaler <span class="hljs-keyword">as</span> GradScaler



<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">get_args</span><span class="hljs-params">()</span>:</span>
    parser = argparse.ArgumentParser()
    parser.add_argument(<span class="hljs-string">&apos;--gpu_id&apos;</span>, type=str, default=<span class="hljs-string">&apos;0&apos;</span>,  help=<span class="hljs-string">&apos;which gpu to use&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--path&apos;</span>, type=str, default=<span class="hljs-string">&apos;../&apos;</span>, help=<span class="hljs-string">&apos;path of csv file with DNA sequences and labels&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--epochs&apos;</span>, type=int, default=<span class="hljs-number">150</span>, help=<span class="hljs-string">&apos;number of epochs to train&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--batch_size&apos;</span>, type=int, default=<span class="hljs-number">24</span>, help=<span class="hljs-string">&apos;size of each batch during training&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--weight_decay&apos;</span>, type=float, default=<span class="hljs-number">0</span>, help=<span class="hljs-string">&apos;weight dacay used in optimizer&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--ntoken&apos;</span>, type=int, default=<span class="hljs-number">4</span>, help=<span class="hljs-string">&apos;number of tokens to represent DNA nucleotides (should always be 4)&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--nclass&apos;</span>, type=int, default=<span class="hljs-number">2</span>, help=<span class="hljs-string">&apos;number of classes from the linear decoder&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--ninp&apos;</span>, type=int, default=<span class="hljs-number">512</span>, help=<span class="hljs-string">&apos;ninp for transformer encoder&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--nhead&apos;</span>, type=int, default=<span class="hljs-number">8</span>, help=<span class="hljs-string">&apos;nhead for transformer encoder&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--nhid&apos;</span>, type=int, default=<span class="hljs-number">2048</span>, help=<span class="hljs-string">&apos;nhid for transformer encoder&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--nlayers&apos;</span>, type=int, default=<span class="hljs-number">6</span>, help=<span class="hljs-string">&apos;nlayers for transformer encoder&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--save_freq&apos;</span>, type=int, default=<span class="hljs-number">1</span>, help=<span class="hljs-string">&apos;saving checkpoints per save_freq epochs&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--dropout&apos;</span>, type=float, default=<span class="hljs-number">.1</span>, help=<span class="hljs-string">&apos;transformer dropout&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--warmup_steps&apos;</span>, type=int, default=<span class="hljs-number">3200</span>, help=<span class="hljs-string">&apos;training schedule warmup steps&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--lr_scale&apos;</span>, type=float, default=<span class="hljs-number">0.1</span>, help=<span class="hljs-string">&apos;learning rate scale&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--kmers&apos;</span>, type=int, nargs=<span class="hljs-string">&apos;+&apos;</span>, default=[<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>], help=<span class="hljs-string">&apos;k-mers to be aggregated&apos;</span>)
    <span class="hljs-comment">#parser.add_argument(&apos;--kmer_aggregation&apos;, type=bool, default=True, help=&apos;k-mers to be aggregated&apos;)</span>
    parser.add_argument(<span class="hljs-string">&apos;--kmer_aggregation&apos;</span>, dest=<span class="hljs-string">&apos;kmer_aggregation&apos;</span>, action=<span class="hljs-string">&apos;store_true&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--no_kmer_aggregation&apos;</span>, dest=<span class="hljs-string">&apos;kmer_aggregation&apos;</span>, action=<span class="hljs-string">&apos;store_false&apos;</span>)
    parser.set_defaults(kmer_aggregation=<span class="hljs-keyword">True</span>)
    parser.add_argument(<span class="hljs-string">&apos;--nfolds&apos;</span>, type=int, default=<span class="hljs-number">5</span>, help=<span class="hljs-string">&apos;number of cross validation folds&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--fold&apos;</span>, type=int, default=<span class="hljs-number">0</span>, help=<span class="hljs-string">&apos;which fold to train&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--val_freq&apos;</span>, type=int, default=<span class="hljs-number">1</span>, help=<span class="hljs-string">&apos;which fold to train&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--num_workers&apos;</span>, type=int, default=<span class="hljs-number">1</span>, help=<span class="hljs-string">&apos;num_workers&apos;</span>)
    parser.add_argument(<span class="hljs-string">&apos;--record&apos;</span>,  type=str, default=<span class="hljs-string">&apos; &apos;</span>, help=<span class="hljs-string">&apos;train or test record&apos;</span>)
    opts = parser.parse_args()
    <span class="hljs-keyword">return</span> opts

tb = SummaryWriter(<span class="hljs-string">&apos;/root/my_train/tblogdir/H3K27me3&apos;</span>)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train_fold</span><span class="hljs-params">()</span>:</span>

    opts = get_args()
    seed_everything(<span class="hljs-number">2020</span>)
    <span class="hljs-comment">#gpu selection</span>
    os.environ[<span class="hljs-string">&quot;CUDA_VISIBLE_DEVICES&quot;</span>] = opts.gpu_id
    device = torch.device(<span class="hljs-string">&apos;cuda&apos;</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">&apos;cpu&apos;</span>)
    data = pd.read_csv(<span class="hljs-string">&apos;/root/my_train/dataset_all/H3K27me3_train_24w.txt&apos;</span>, header=<span class="hljs-keyword">None</span>,  sep=<span class="hljs-string">&apos;\t&apos;</span>)

    dataset = ViraminerDataset(data.iloc[:<span class="hljs-number">220000</span>,<span class="hljs-number">0</span>],data.iloc[:<span class="hljs-number">220000</span>,<span class="hljs-number">1</span>])
    dataloader = torch.utils.data.DataLoader(dataset,batch_size=opts.batch_size,shuffle=<span class="hljs-keyword">True</span>,num_workers=opts.num_workers)
    val_dataset = ViraminerDataset(data.iloc[<span class="hljs-number">220000</span>:,<span class="hljs-number">0</span>],data.iloc[<span class="hljs-number">220000</span>:,<span class="hljs-number">1</span>])
    val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=opts.batch_size,shuffle=<span class="hljs-keyword">False</span>)

    <span class="hljs-comment">#exit()</span>
    <span class="hljs-comment">#lr=0</span>

    <span class="hljs-comment">#checkpointing</span>
    checkpoints_folder = <span class="hljs-string">&apos;{}checkpoints_fold{}&apos;</span>.format(opts.record, opts.fold)
    csv_file = <span class="hljs-string">&apos;{}log_fold{}.csv&apos;</span>.format(opts.record, opts.fold)
    columns = [<span class="hljs-string">&apos;epoch&apos;</span>,<span class="hljs-string">&apos;train_loss&apos;</span>,
             <span class="hljs-string">&apos;val_loss&apos;</span>,<span class="hljs-string">&apos;val_auc&apos;</span>,<span class="hljs-string">&apos;val_acc&apos;</span>,<span class="hljs-string">&apos;val_sens&apos;</span>,<span class="hljs-string">&apos;val_spec&apos;</span>]
    logger = CSVLogger(columns,csv_file)

    <span class="hljs-comment">#build model and logger</span>
    model = NucleicTransformer(opts.ntoken, opts.nclass, opts.ninp, opts.nhead, opts.nhid,
                           opts.nlayers, opts.kmer_aggregation, kmers=opts.kmers,
                           dropout=opts.dropout).to(device)
    optimizer = torch.optim.Adam(model.parameters(), weight_decay=opts.weight_decay)
    criterion = nn.CrossEntropyLoss(reduction=<span class="hljs-string">&apos;none&apos;</span>)
    lr_schedule = lr_AIAYN(optimizer,opts.ninp,opts.warmup_steps,opts.lr_scale)

    softmax = nn.Softmax(dim=<span class="hljs-number">1</span>)

    pytorch_total_params = sum(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> model.parameters())
    print(<span class="hljs-string">&apos;Total number of paramters: {}&apos;</span>.format(pytorch_total_params))

    print(<span class="hljs-string">&quot;Starting training for fold {}/{}&quot;</span>.format(opts.fold,opts.nfolds))
    <span class="hljs-comment">#training loop</span>
    scaler = GradScaler()
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(opts.epochs):
        model.train(<span class="hljs-keyword">True</span>)
        t = time.time()
        total_loss = <span class="hljs-number">0</span>
        total_steps = len(dataloader)
        <span class="hljs-keyword">for</span> step, data <span class="hljs-keyword">in</span> enumerate(dataloader):
        <span class="hljs-comment">#for step in range(1):</span>
            lr = lr_schedule.step()
            src = data[<span class="hljs-string">&apos;data&apos;</span>].to(device)
            labels = data[<span class="hljs-string">&apos;labels&apos;</span>].to(device)
            optimizer.zero_grad()

            <span class="hljs-keyword">with</span> autocast():
                output = model(src)
                loss = torch.mean(criterion(output,labels))

            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(),<span class="hljs-number">1</span>)
            scaler.step(optimizer)
            scaler.update()

            total_loss += loss

            <span class="hljs-keyword">print</span> (<span class="hljs-string">&quot;Epoch [{}/{}], Step [{}/{}] Loss: {:.3f} Lr:{:.6f} Time: {:.1f}&quot;</span>
                           .format(epoch+<span class="hljs-number">1</span>, opts.epochs, step+<span class="hljs-number">1</span>, total_steps, total_loss/(step+<span class="hljs-number">1</span>) , lr,time.time()-t),end=<span class="hljs-string">&apos;\r&apos;</span>,flush=<span class="hljs-keyword">True</span>) <span class="hljs-comment">#total_loss/(step+1)</span>
            <span class="hljs-comment">#break</span>
        print(<span class="hljs-string">&apos;&apos;</span>)

        train_loss = total_loss/(step+<span class="hljs-number">1</span>)        

        <span class="hljs-keyword">if</span> (epoch+<span class="hljs-number">1</span>)%opts.val_freq == <span class="hljs-number">0</span>:
            val_loss,auc,val_acc,val_sens,val_spec=validate(model,device,val_dataloader,batch_size=opts.batch_size*<span class="hljs-number">2</span>)
            print(<span class="hljs-string">&quot;Epoch {} train loss: {}&quot;</span>.format(epoch+<span class="hljs-number">1</span>,train_loss))
            tb.add_scalars(<span class="hljs-string">&apos;train/val/loss&apos;</span>, {<span class="hljs-string">&apos;train&apos;</span>:train_loss, <span class="hljs-string">&apos;val&apos;</span>:val_loss}, epoch+<span class="hljs-number">1</span>)
            tb.add_scalar(<span class="hljs-string">&apos;val_acc&apos;</span>, val_acc, epoch+<span class="hljs-number">1</span>)
            tb.add_scalar(<span class="hljs-string">&apos;auc&apos;</span>, auc, epoch+<span class="hljs-number">1</span>)
            tb.add_scalar(<span class="hljs-string">&apos;val_sens&apos;</span>, val_sens, epoch+<span class="hljs-number">1</span>)
            tb.add_scalar(<span class="hljs-string">&apos;val_spec&apos;</span>, val_spec, epoch+<span class="hljs-number">1</span>)

            to_log = [epoch+<span class="hljs-number">1</span>,train_loss,val_loss,auc,val_acc,val_sens,val_spec]
            logger.log(to_log)


        <span class="hljs-keyword">if</span> (epoch+<span class="hljs-number">1</span>)%opts.save_freq == <span class="hljs-number">0</span>:
            save_weights(model,optimizer,epoch,checkpoints_folder)

    tb.close()
    get_best_weights_from_fold(opts.record, opts.fold)

train_fold()
</code></pre>
<p><strong>run.sh</strong></p>
<pre><code class="lang-shell">#!/bin/bash
python train.py --gpu_id 0 --kmer_aggregation  --epochs 12 --nlayers 6 \
 --batch_size 64 --kmers 20 --lr_scale 0.1 --ninp 512  --nhid 2048 --num_workers 8 --nhead 8 --record 0504_train1
</code></pre>
<p><strong>test.sh</strong></p>
<pre><code class="lang-shell">#!/bin/bash
python evaluate_test.py --gpu_id 0,1 --kmer_aggregation --nmute 20 --epochs 100 --nlayers 6 \
--batch_size 128 --kmers 13 --lr_scale 0.1 --ninp 512 --nhid 2048
</code></pre>
<h2 id=""> </h2>

                                
                                </section>
                            
                        </div>
                    </div>
                
            </div>

            
                
                <a href="数据获取与预处理.html" class="navigation navigation-prev " aria-label="Previous page: 数据获取与预处理">
                    <i class="fa fa-angle-left"></i>
                </a>
                
                
                <a href="模型评估.html" class="navigation navigation-next " aria-label="Next page: 模型评估">
                    <i class="fa fa-angle-right"></i>
                </a>
                
            
        
    </div>

    <script>
        var gitbook = gitbook || [];
        gitbook.push(function() {
            gitbook.page.hasChanged({"page":{"title":"构建模型","level":"1.2.3","depth":2,"next":{"title":"模型评估","level":"1.2.4","depth":2,"path":"实验流程/模型评估.md","ref":"实验流程/模型评估.md","articles":[]},"previous":{"title":"数据获取与预处理","level":"1.2.2","depth":2,"path":"实验流程/数据获取与预处理.md","ref":"实验流程/数据获取与预处理.md","articles":[]},"dir":"ltr"},"config":{"plugins":["-search","back-to-top-button","chapter-fold","expandable-chapters-small","code","sectionx","page-treeview","sharing","highlight","splitter"],"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"pluginsConfig":{"page-treeview":{"collapsed":false,"copyright":"Copyright © aleen42","minHeaderCount":"1","minHeaderDeep":"1"},"chapter-fold":{},"splitter":{},"lunr":{"maxIndexSize":1000000,"ignoreSpecialCharacters":false},"code":{"copyButtons":true},"fontsettings":{"theme":"white","family":"sans","size":2},"sectionx":{},"highlight":{},"back-to-top-button":{},"expandable-chapters-small":{},"sharing":{"all":["facebook","google","twitter","weibo","instapaper"],"facebook":true,"google":false,"instapaper":false,"twitter":true,"vk":false,"weibo":false},"theme-default":{"styles":{"website":"styles/website.css","pdf":"styles/pdf.css","epub":"styles/epub.css","mobi":"styles/mobi.css","ebook":"styles/ebook.css","print":"styles/print.css"},"showLevel":false}},"theme":"default","author":"xfchen","pdf":{"pageNumbers":true,"fontSize":12,"fontFamily":"Arial","paperSize":"a4","chapterMark":"pagebreak","pageBreaksBefore":"/","margin":{"right":62,"left":62,"top":56,"bottom":56}},"structure":{"langs":"LANGS.md","readme":"README.md","glossary":"GLOSSARY.md","summary":"SUMMARY.md"},"variables":{},"title":"Experimental Record","language":"zh-hans","gitbook":"*","description":"Prediction of histone modifications in rice genome by deep learning"},"file":{"path":"实验流程/构建模型.md","mtime":"2021-05-05T14:12:08.069Z","type":"markdown"},"gitbook":{"version":"3.2.3","time":"2021-05-06T06:35:32.051Z"},"basePath":"..","book":{"language":""}});
        });
    </script>
</div>

        
    <script src="../gitbook/gitbook.js"></script>
    <script src="../gitbook/theme.js"></script>
    
        
        <script src="../gitbook/gitbook-plugin-back-to-top-button/plugin.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-chapter-fold/chapter-fold.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-expandable-chapters-small/expandable-chapters-small.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-code/plugin.js"></script>
        
    
        
        <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sectionx/sectionx.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-sharing/buttons.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-splitter/splitter.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/lunr.min.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-lunr/search-lunr.js"></script>
        
    
        
        <script src="../gitbook/gitbook-plugin-fontsettings/fontsettings.js"></script>
        
    

    </body>
</html>


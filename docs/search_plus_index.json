{"./":{"url":"./","title":"首页","keywords":"","body":" 深度学习预测水稻基因组组蛋白修饰 主要负责人：陈旭峰 联系方式：邮箱：cxf237478882@webmail.hzau.edu.cn （如有任何问题，请及时联系） 团队成员：陈旭峰，陈儒磊 数据来源： 华中农业大学作物遗传改良国家重点实验室三维基因组团队李兴旺教授课题组： 水稻表观遗传信息：Integrative analysis of reference epigenomes in 20 rice varieties 玉米表观遗传信息：3D genome architecture coordinates trans and cis regulation of differentially expressed ear and tassel genes in maize 油菜表观遗传信息：Asymmetric epigenome maps of subgenomes reveal imbalanced transcription and distinct evolutionary trends in Brassica napus 华中农业大学作物遗传改良国家重点实验室张启发教授课题组： 水稻基因型信息：Breeding signatures of rice improvement revealed by a genomic variation map from a large germplasm collection 概览： 实验流程部分： 本部分记录了整个项目的完整过程，包括实验的技术路线，各部分的代码实现和代码的更新过程，详细记录了整个项目的探索和完善过程（其中某些部分列举多个方法，经过不断尝试最终会选择效果最优的方法） 背景知识部分： 本部分为该项目所涉及的必要基础知识和原理（相当于是文献笔记和重要学习资源的整理）包括生物学和深度学习两部分 "},"实验流程/":{"url":"实验流程/","title":"实验流程","keywords":"","body":"研究内容 ​ 组蛋白修饰的相关研究一直是表观遗传领域的热点，近年来，组蛋白修饰的定量检测已经成为研究染色体包装、转录激活和DNA损伤等生物过程的主要手段，目前定量检测组蛋白修饰的技术仍是以染色质免疫共沉淀和二代测序技术相结合的ChIP-seq为主，各课题组都在以更小的样本量，更高的分辨率，降低成本，缩短时间为目标，对ChIP-seq进行改进，但是高通量技术昂贵而耗时的特点仍然非常突出。 ​ 鉴于此，我们通过训练深度学习模型来拟合基因型序列数据与组蛋白修饰位点数据，以实现准确预测不同组蛋白标记的修饰位点的目的，并且使用不同的算法来解析序列信息中起到关键作用的位点，对序列的保守性特征实现可视化，以提取组蛋白修饰区域的motif，研究组蛋白修饰在基因表达调控中的作用，最终将训练好的模型用于组蛋白修饰定性图谱的绘制。 研究意义 应用意义 通过深度学习的方法来预测组蛋白修饰，从而高效绘制水稻各个品种的全基因组的染色质图谱，为研究在表观遗传层面改良农作物重要农艺性状提供数据基础 提取特征 本项目利用卷积神经网络反向传播的特性，使用一阶泰勒展开的方法，将模型的梯度回传，绘制显著性图，提取组蛋白修饰区域基因序列的保守性特征并且进行可视化，属于对深度学习模型可解释性的探索应用，也对组蛋白修饰在基因组层次的影响进行了进一步的探索。 绘制图谱 本项目利用深度学习模型预测的方式来绘制表观遗传修饰的定性图谱，能够较为准确的预测出基因组的某一区域是否含有某种组蛋白修饰并且进行标记，相较于传统的测定方法，具有高效节约的特点，利用计算机算力来替代高通量技术，为今后表观修饰检测技术与大数据技术相结合提供了优秀的示例。 迁移学习 本项目选用水稻基因组作为数据源，搭建pytorch模型训练框架，因此利用深度学习迁移性的特点，可以调整预训练模型，共享参数，横跨多个相关任务，进行其他物种表观遗传修饰预测模型的训练，高效率地实现多个相关问题的探索。同时还能够研究不同物种在同一种表观遗传修饰上存在的保守性。 多视图学习 本项目参考DeepHistone的数据集构建方法，加入了染色质可及性作为辅助预测的数据，可以有效提升模型的准确率，生物领域应用深度学习模型，可以引入多种类型的数据，类比多组学联合分析，利用各类组学数据在生物学方面的关联性来有效地扩增数据集，提升准确率。 "},"实验流程/实验流程概述.html":{"url":"实验流程/实验流程概述.html","title":"实验流程概述","keywords":"","body":"实验流程概述 1.利用变异信息（vcf）里的变异信息（SNP)来替换水稻参考基因组中原本序列，生成品种基因型； 2.参考deepsea中定义组蛋白修饰标签的方式，将基因组切分成每500bp一个bin，根据narrowPeak文件里的信号区域范围，使用slide window的方式，在每个peak的最高点中心取一定长度的区域，与slide window重合超过10%就标记为有组蛋白修饰（标签为1），否则为无修饰（标签为0），注意每个品种的组蛋白修饰情况不同，最终形成训练集。20个水稻品种序列信息 ：组蛋白修饰标签（五个二分类）； 3.将训练集投入深度学习模型，进行调参，选择最优模型参数； 4.利用神经网络的提取特征的特性将PWM矩阵从卷积层中提取出来，获取高保守性的序列motif，与JASPAR数据库中的TFBS进行对比； 5.利用训练好的模型进行预测，绘制组蛋白修饰定性图谱。 整体架构 /public/home/xwli/xwzhang/deeplearningDATA/rice ├── data/ ├── mapto_MSU7/ ├── MHZS_mapto_MHRS1_ZSRS1/ │ ├── pytorch_deepsea/ ├── DeepHistone/ ├── Nt_Transformer/ └── scripts/ │ ├── py_scripts/ │ └── sh_scripts/ 技术路线 "},"实验流程/数据获取与预处理.html":{"url":"实验流程/数据获取与预处理.html","title":"数据获取与预处理","keywords":"","body":"数据获取与预处理 数据命名规则和存放位置 /public/home/xwli/xwzhang/deeplearningDATA/rice/data/ name location 序列文件 C^^^_C^^^.fasta /sequence 注释文件 C^^^_C^^^.fasta.fai /faidx 信号区域处理后 sort_C^^^.bed /signal_area 单品种标签 target_C^^^.txt /target 单品种数据集 data_C^^^.xlsx /dataset 数据集（data(数据量)（随机）） data_10w_sh.xlsx /dataset onehot编码 one_hot_10w_sh.npy /onehot 数据集对应标签 labels_10w_sh.txt /label DeepSEA dataset_rice1.npz /DeepSEA DeepHistone dataset_rice2.npz /DeepHistone Transformer dataset_rice3.npz /Transformer 数据格式 fasta 在生物信息学中，FASTA格式（又称为Pearson格式）是一种基于文本的、用于表示核苷酸序列或氨基酸序列的格式。在这种格式中碱基对或氨基酸用单个字母来表示，且允许在序列前添加序列名及注释。 FASTA文件以序列表示和序列作为一个基本单元，各行记录信息如下： 第一行是由大于号\">\"开头的任意文字说明，用于序列标记，为了保证后续分析软件能够区分每条序列，单个序列的标识必须具有唯一性。； 从第二行开始为序列本身，只允许使用既定的核苷酸或氨基酸编码符号。通常核苷酸符号大小写均可，而氨基酸常用大写字母。使用时应注意有些程序对大小写有明确要求。文件每行的字母一般不应超过80个字符。 IRGSP-1.0_genome.fasta vcf VCF格式：Variant Call Format，用于记录variants (SNP / InDel)的文件格式 水稻12条染色体的变异信息 SNP汇总： nip_total.vcf 注释部分 # vcf版本 日期 水稻12条染色体的长度 主体部分 CHROM ： 参考序列名称 POS ： variant所在的left-most位置(1-base position)（发生变异的位置的第一个碱基所在的位置） ID ： variant的ID。同时对应着dbSNP数据库中的ID，若没有，则默认使用‘.’ REF ： 参考序列的Allele，（等位碱基，即参考序列该位置的碱基类型及碱基数量） ALT ： variant的Allele，若有多个，则使用逗号分隔，（变异所支持的碱基类型及碱基数量）这里的碱基类型和碱基数量，对于SNP来说是单个碱基类型的编号，而对于Indel来说是指碱基个数的添加或缺失，以及碱基类型的变化 QUAL ： variants的质量。Phred格式的数值，代表着此位点是纯合的概率，此值越大，则概率越低，代表着次位点是variants的可能性越大。（表示变异碱基的可能性） FILTER ： 次位点是否要被过滤掉。如果是PASS，则表示此位点可以考虑为variant。 INFO ： variant的相关信息 FORMAT ： variants的格式，例如GT:AD:DP:GQ:PL SAMPLES ： 各个Sample的值，由BAM文件中的@RG下的SM标签所决定，这些值对应着第9列的各个格式，不同格式的值用冒号分开，每一个sample对应着1列；多个samples则对应着多列，这种情况下列的数多余10列。 nip_total.vcf narrowPeak ChIP-seq产生的数据格式 *narrowPeak文件是BED6+4格式，可以上传到UCSC浏览。输出文件每列信息分别包含： 1；染色体号 2：peak起始位点 3：结束位点 4：peak name 5：int(-10*log10qvalue) 6 ：正负链 7：fold change 8：-log10pvalue 9：-log10qvalue 10：relative summit position to peak start（？） fai fasta是常用的序列存储格式，软件对序列进行快速查找的时候通常需要建立索引文件，例如在GATK、IGV等软件中导入序列的时候都需要建立索引。fasta格式文件的一种索引为fai结尾的文件，可以使用samtools faidx命令创建，具体用法如下： 用法： samtools faidx input.fa 该命令对输入的fasta序列有一定要求：对于每条序列，除了最后一行外， 其他行的长度必须相同 第一列 NAME : 序列的名称，只保留“>”后，第一个空白之前的内容； 第二列 LENGTH: 序列的长度， 单位为bp； 第三列 OFFSET : 第一个碱基的偏移量， 从0开始计数，换行符也统计进行； 第四列 LINEBASES : 除了最后一行外， 其他代表序列的行的碱基数， 单位为bp； 第五列 LINEWIDTH : 行宽， 除了最后一行外， 其他代表序列的行的长度， 包括换行符， 在windows系统中换行符为\\r\\n, 要在序列长度的基础上加2； bed BED (Browser Extensible Data)格式文件就是通过规定行的内容来展示注释信息 BED文件每行至少包括chrom，chromStart，chromEnd三列；另外还可以添加额外的9列，这些列的顺序是固定的。 在自定义BED文件时，前面可以有注释行，以“browser”或“track”开头，可以设置一些参数便于浏览器更好展示BED文件信息。但是，下游的一些分析工具，例如bedToBigBed，是不接受有注释的BED文件的。 # BED文件必须的3列: chrom - 染色体号; 例如，chr1，chrX。。。。。。。 chromStart - feature在染色体上起始位置. 从0开始算，染色体上第一个碱基位置标记为0。 chromEnd - feature在染色体上终止位置。染色体上前100个碱基片段的位置位置标记为：chromStart=0, chromEnd=100。 实际上，第100个碱基不属于当前片段中，当前片段的碱基应该是0-99。所以在BED文件中，起始位置从0开始，终止位置从1开始。 # BED文件可选的9列: name - BED行名，在基因组浏览器左边显示； score - 在基因组浏览器中显示的灰度设定，值介于0-1000； gray score strand - 正负链标记. Either \".\" (=no strand) or \"+\" or \"-\". thickStart - feature起始位置(for example, the start codon in gene displays)。 When there is no thick part, thickStart and thickEnd are usually set to the chromStart position. thickEnd - feature编码终止位置 (for example the stop codon in gene displays). itemRgb - R,G,B (e.g. 255,0,0)值，当itemRgb 设置为 \"On\"，BED的行会显示颜色. blockCount - blocks (exons)数目. blockSizes - blocks (exons)大小列表，逗号分隔，对应于blockCount. blockStarts -blocks (exons)起始位置列表，逗号分隔，对应于blockCount.；这个起始位置是与chromStart的一个相对位置。 sort_C019_peak.bed .npy/.npz npy文件是Numpy专用的二进制格式文件，可以用来保存numpy.array import numpy as np # 将数组以二进制格式保存到磁盘 arr=np.arange(5) np.save('test',arr) # 读取数组 print(np.load('test.npy')) npz文件是Numpy的压缩文件，可以将多个数组压缩保存到同一个文件中。 import numpy as np # 将多个数组保存到磁盘 a = np.arange(5) b = np.arange(6) c = np.arange(7) np.savez('test', a, b, c_array=c) # c_array是数组c的命名 # 读取数组 data = np.load('test.npz') #类似于字典{‘arr_0’:a,’arr_1’:b,’c_array’:c} print('arr_0 : ', data['arr_0']) print('arr_1 : ', data['arr_1']) print('c_array : ', data['c_array']) -------------------------------------------------------------------------------- arr_0 : [0 1 2 3 4] arr_1 : [0 1 2 3 4 5] c_array : [0 1 2 3 4 5 6] 生成数据集 本部分包含数据处理脚本的全部更新过程，想查看最新版本数据处理流程请跳转至生成数据集总流程（bash脚本） 参考基因组下载 水稻日本晴第七版 wget http://rapdb.dna.affrc.go.jp/download/archive/irgsp1/IRGSP-1.0_genome.fasta.gz gunzip IRGSP-1.0_genome.fasta.gz 查看基因组的大小和ID bioawk -c fastx '{print $1 \"\\t\" length($seq)}' IRGSP-1.0_genome.fasta 生成品种基因型 以NIP为示例，后续使用其他品种扩大数据集 python脚本 geno_sub.py 目前使用脚本geno_sub2.0.py, 解决兼并碱基的问题（2021.3.28） 使用bash脚本批量并行提交作业 注：bash脚本在windows上编辑后在Linux运行会报错 scripts/sh_scripts/geno_make.sh: line 4: syntax error near unexpected token `$'do\\r'' 'cripts/sh_scripts/geno_make.sh: line 4: `do 解决方法： https://www.jianshu.com/p/55597646fa84 sed 's/\\r//' input.sh > output.sh #!/bin/bash #生成品种基因型，参考基因组日本晴第七版 for i in $(cat id1) do bsub -q high -e ${i}.err -o ${i}.out \"python3 scripts/py_scripts/geno_sub.py -s data/vcf/sort.total.vcf -n ${i} -r data/ref/IRGSP-1.0_genome.fasta -l 500\" done python脚本使用方法 $ python3 geno_sub.py -h usage: geno_sub.py [-h] -s -n -r [-l] New sample genome sequence based on snp data optional arguments: -h, --help show this help message and exit -s , --snpfile snp file in vcf format -n , --samplename sample name, which mest be in snpfile -r , --reference reference file -l , --seqlen seq length per line in output file input Format Description location .fasta 日本晴第七版参考基因组（IRGSP-1.0_genome.fasta） data/ref .vcf 水稻变异信息(vcf) data/vcf output Format Description Location .fasta 水稻品种自定义基因组（C^^^.fasta) data/sequence .fasta 水稻基因组（无>,paste时使用) data/seq 示例 bsub 'python3 scripts/geno_sub.py -s data/vcf/*.vcf -n C^^^ -r data/ref/*fasta -l 500' #geno_sub.py import re import os import sys import argparse from collections import defaultdict def defaultdict_list(): return defaultdict(list) def snpInfo(snp): ''' 处理位点文件，记录样本发生变异的位点坐标信息 ''' snp_file = open(snp, 'r') snp_info = defaultdict(defaultdict_list) snp_geno = defaultdict(defaultdict_list) for line in snp_file: if line.startswith('##') or re.search('^$',line.strip()): continue elif line.startswith('#CHROM'): line_info = line.strip().split('\\t') sample_list = line_info[9:] else: line_info = line.strip().split('\\t') for i in range(len(sample_list)): if line_info[i+9]!='0|0': snp_info[sample_list[i]][line_info[0]].append(int(line_info[1])) snp_geno[sample_list[i]][line_info[0]].append([line_info[3], line_info[4]]) snp_file.close() return snp_info, snp_geno def genomePar(genome): ''' 解析基因组文件 ''' genome_file = open(genome, 'r') seq = '' seq_name = '' seq_info = {} for line in genome_file: if line.startswith('>') and seq == '': seq_name = re.sub('chr0|chr', '',line.strip()[1:]) elif line.startswith('>') and seq != '': seq_info[seq_name] = seq seq_name = re.sub('chr0|chr', '',line.strip()[1:]) seq = '' else: seq += line.strip() seq_info[seq_name] = seq genome_file.close() return seq_info def seqFormat(seq, lenset=60): ''' 按指定长度替换数据 ''' format_seq = '' for i in range(1, len(seq)+1): if i % lenset == 0: format_seq += seq[i-1]+\"\\n\" else: format_seq += seq[i-1] return format_seq def faOut(snppos, snpgeno, fasta, sample, lenset=60): ''' 根据snppos内储存的信息，对基因组中的碱基进行替换 ''' out_fasta = open(f'{sample}.fasta', 'w') for key in fasta: n = 1 seq = '' for i in fasta[key]: if len(snppos[sample][key])!=0 and n==snppos[sample][key][0]: try: ref_index = snpgeno[sample][key][0].index(i.upper()) #print (ref_index, snpgeno[sample][key][0]) if ref_index == 0: seq += snpgeno[sample][key][0][1] else: seq += snpgeno[sample][key][0][0] except: print (f'chr {key} in pos {n} for sample {sample} is wrong: the ref is {i}, but geno is {snpgeno[sample][key][0]}') seq += i finally: snppos[sample][key].pop(0) snpgeno[sample][key].pop(0) else: seq += i n += 1 chr_id = 'chr{:0>2d}'.format(int(key)) seq = seqFormat(seq, lenset) out_fasta.write(f'>{chr_id}\\n{seq}\\n') out_fasta.close() if __name__ == '__main__': parser = argparse.ArgumentParser(description=\"New sample genome sequence based on snp data\") parser.add_argument(\"-s\", \"--snpfile\", metavar=\"\", required=True, help=\"snp file in vcf format\") parser.add_argument(\"-n\", \"--samplename\", metavar=\"\", required=True, help=\"sample name, which mest be in snpfile\") parser.add_argument(\"-r\", \"--reference\", metavar=\"\", required=True, help=\"reference file\") parser.add_argument(\"-l\", \"--seqlen\", metavar=\"\", default=60, type=int, help=\"seq length per line in output file\") args = parser.parse_args() # 解析变异信息文件 snppos, snpgeno = snpInfo(args.snpfile) # 解析基因组文件 fasta = genomePar(args.reference) # 输出替换的结果 faOut(snppos, snpgeno, fasta, args.samplename, args.seqlen) #删除含有N（n）的行 sed -i '/N/d' C^^^.fasta | sed -i '/n/d' #geno_sub2.0.py import re import os import sys import argparse from collections import defaultdict def defaultdict_list(): return defaultdict(list) def snpInfo(snp): ''' 处理位点文件，记录样本发生变异的位点坐标信息 ''' degeneration = {'AG':'R','GA':'R','CT':'Y','TC':'Y', 'AC':'M','CA':'M','GT':'K','TG':'K', 'GC':'S','CG':'S','AT':'W','TA':'W'} snp_file = open(snp, 'r') snp_info = defaultdict(defaultdict_list) snp_geno = defaultdict(defaultdict_list) for line in snp_file: if line.startswith('##') or re.search('^$',line.strip()): continue elif line.startswith('#CHROM'): line_info = line.strip().split('\\t') sample_list = line_info[9:] else: line_info = line.strip().split('\\t') for i in range(len(sample_list)): if line_info[i+9]!='' and line_info[i+9]!='0|0' and line_info!=\".|.\": sample_allele = list(set(line_info[i+9].split('|'))) ref = line_info[3] alts = [ref] alts.extend(line_info[4].split(',')) if len(ref) == 1: snp_info[sample_list[i]][line_info[0]].append(int(line_info[1])) if len(sample_allele) == 1: snp_geno[sample_list[i]][line_info[0]].append([alts[int(sample_allele[0])]]) else: biallele = ''.join([alts[int(sample_allele[0])],alts[int(sample_allele[1])]]) if len(biallele)!=2: snp_geno[sample_list[i]][line_info[0]].append(alts[int(sample_allele[1])]) else: snp_geno[sample_list[i]][line_info[0]].append(degeneration[biallele]) else: snp_info[sample_list[i]][line_info[0]].append([int(line_info[1]),int(line_info[1])+len(alts[0])-1]) if len(sample_allele) == 1: snp_geno[sample_list[i]][line_info[0]].append([alts[int(sample_allele[0])]]) else: biallele = ''.join([alts[int(sample_allele[0])],alts[int(sample_allele[1])]]) snp_geno[sample_list[i]][line_info[0]].append(alts[int(sample_allele[0])]) snp_file.close() return snp_info, snp_geno def genomePar(genome): ''' 解析基因组文件 ''' genome_file = open(genome, 'r') seq = '' seq_name = '' seq_info = {} for line in genome_file: if line.startswith('>') and seq == '': seq_name = re.sub('chr0|chr', '',line.strip()[1:]) elif line.startswith('>') and seq != '': seq_info[seq_name] = seq seq_name = re.sub('chr0|chr', '',line.strip()[1:]) seq = '' else: seq += line.strip() seq_info[seq_name] = seq genome_file.close() return seq_info def seqFormat(seq, lenset=500): ''' 按指定长度替换数据 ''' format_seq = '' for i in range(1, len(seq)+1): if i % lenset == 0: format_seq += seq[i-1]+\"\\n\" else: format_seq += seq[i-1] return format_seq def faOut(snppos, snpgeno, fasta, sample, lenset=60): ''' 根据snppos内储存的信息，对基因组中的碱基进行替换 ''' fasta_file = open(fasta, 'r') out_fasta = open(f'{sample}.fasta', 'w') for line in fasta_file: if line.startswith('>'): #print (line) seq_name = re.sub('chr0|chr', '',line.strip()[1:]) out_fasta.write(f'>{seq_name}\\n') start = 1 else: for i in range(len(line.strip())): if len(snppos[sample][seq_name]) == 0: out_fasta.write(line[i]) elif i == snppos[sample][seq_name][0]: out_fasta.write(snpgeno[sample][seq_name][0]) snppos[sample][seq_name].pop(0) snpgeno[sample][seq_name].pop(0) elif isinstance(snppos[sample][seq_name][0], list) and i in snppos[sample][seq_name][0]: out_fasta.write(snpgeno[sample][seq_name][0]) i = snppos[sample][seq_name][0] snppos[sample][seq_name].pop(0) snpgeno[sample][seq_name].pop(0) else: out_fasta.write(line[i]) out_fasta.write('\\n') ''' for key in fasta: n = 1 seq = '' for i in fasta[key]: if len(snppos[sample][key])!=0 and n==snppos[sample][key][0]: try: ref_index = snpgeno[sample][key][0].index(i.upper()) #print (ref_index, snpgeno[sample][key][0]) if ref_index == 0: seq += snpgeno[sample][key][0][1] else: seq += snpgeno[sample][key][0][0] except: print (f'chr {key} in pos {n} for sample {sample} is wrong: the ref is {i}, but geno is {snpgeno[sample][key][0]}') seq += i finally: snppos[sample][key].pop(0) snpgeno[sample][key].pop(0) else: seq += i n += 1 chr_id = 'chr{:0>2d}'.format(int(key)) seq = seqFormat(seq, lenset) out_fasta.write(f'>{chr_id}\\n{seq}\\n') out_fasta.close() ''' if __name__ == '__main__': parser = argparse.ArgumentParser(description=\"New sample genome sequence based on snp data\") parser.add_argument(\"-s\", \"--snpfile\", metavar=\"\", required=True, help=\"snp file in vcf format\") parser.add_argument(\"-n\", \"--samplename\", metavar=\"\", required=True, help=\"sample name, which mest be in snpfile\") parser.add_argument(\"-r\", \"--reference\", metavar=\"\", required=True, help=\"reference file\") parser.add_argument(\"-l\", \"--seqlen\", metavar=\"\", default=60, type=int, help=\"seq length per line in output file\") args = parser.parse_args() # 解析变异信息文件 snppos, snpgeno = snpInfo(args.snpfile) # 输出替换的结果 faOut(snppos, snpgeno, args.reference, args.samplename, args.seqlen) 生成组蛋白修饰标签 目前使用bash脚本自动化并行处理标签生成任务(2021.3.28) 分别提取组蛋白修饰信号区域 input ：narrowPeak文件 output：bed文件 组蛋白修饰H3K4me3, H3K27ac, H3K4me1, H3K27me3， H3K9me2 awk '{print $1,$2,$3}' mapto*/*_seedlings/*C^^^_peaks.narrowPeak | sort -t ' ' -k1.4nr >> C^^^.bed `按照染色体顺序排列，并计数 sort -k1,1V -k2,2n -k3,3n C^^^.bed > data/signal_area/sort_C^^^.bed rm -f C^^^.bed cat data/signal_area/sort_C^^^.bed | awk '{print $1}' | uniq -c ls *fasta | while read id;do echo ${id%.*} >> id.txt;done #提取品种id #area_extract.sh #!/bin/bash for n in $(cat histone) do for i in $(cat id.txt) do awk '{print $1,$2,$3}' mapto*/${n}_seedlings/*${i}_peaks.*Peak | sort -t ' ' -k1,4nr | sort -k1,1V -k2,2n -k3,3n > data/signal_area/sort_${i}_${n}.bed done done 生成自定义基因组的fai文件 samtools faidx data/sequence/C^^^.fasta 生成标签 python脚本target_mark.py 使用方法 $ python3 target_mark.py -h usage: 20210302_bin_count.py [-h] -f -b -o [-w] [-l] bin data out optional arguments: -h, --help show this help message and exit -f , --fai reference index file in fai -b , --bed bed file -o , --output output file -w , --window window size -l , --overlap overlap size input Format Description Location .narrowPeak 20个水稻品种的ChIP-seq结果 mapto_MSU7/*seedlings .bed 组蛋白修饰信号区域 sort_C^^^_C^^^.bed data/signal_area .fai fasta文件的索引 C^^^_C^^^.fasta.fai data/faidx output Format Description Location .txt（1.0） 标签[0,1] data/target .txt (2.0) onehot标签1-5 da 示例 bsub 'python3 scripts/target_mark.py -f data/faidx/C019_C019.fasta.fai -b data/signal_area/sort_C^^^.bed -o target_C^^^.bed' bsub -e 11.err -o 11.out 'python3 target_mark3.0.py -f data/sequence/C019.fasta.fai -b data/signal_area/sort_C019_H3K4me3.bed -o target_C019_H3K4me3.bed -s C019 -m H3K4me3' #!/bin/bash for n in $(cat histone) do for i in $(cat id.txt) do python3 scripts/py_scripts/target_mark3.0.py -f data/seq/${i}.fasta.fai -b data/signal_area/sort_${i}_${n}.bed -o target_${i}_${n}.bed && awk '{print $3}' target_${i}_${n}.bed > data/target/target_${i}_${n}.txt && rm -f *bed done done bsub -e 11.err -o 11.out 'python3 target_mark3.0.py -f data/sequence/C019.fasta.fai -b data/signal_area/sort_C019_H3K4me3.bed -o target_C019_H3K4me3.bed -s C019 -m H3K4me3' import re import os import sys import argparse from collections import defaultdict def getRefLen(fai): ''' 读取fai文件，获取基因组长度 ''' fai_file = open(fai, 'r') ref_Length = {} for line in fai_file: chr_id, seq_len, *other = line.strip().split('\\t') ref_Length[re.sub('chr0|chr','',chr_id)] = int(seq_len) return ref_Length def bedParse(bed): ''' 解析bed文件，分染色体返回预设区间信息 ''' region = defaultdict(list) bed_info = open(bed, 'r') for line in bed_info: chr_id, start, end = re.split('\\s', line.strip()) chr_id = re.sub('chr0|chr', '', chr_id) region[chr_id].append((int(start), int(end))) bed_info.close() return region def binCal(region, reflen, out, win=500, overlap=250): ''' 根据预设的region，遍历基因组，统计每窗口win内的交集片段大小，overlap超过一定范围后输出1，否则输出0 ''' out_file = open(out, 'w') bin = 0 for chr in reflen: for i in range(0, reflen[chr]-500, 500): start_tmp = i end_tmp = i+win region_chr = region[chr] for start, end in region_chr: if end_tmp overlap: bin = 1 else: bin = 0 break elif start_tmp>=end: region[chr].remove((start, end)) out_file.write(f'{chr}\\t{i}\\t{bin}\\t{start}\\t{end}\\n') out_file.close() if __name__ == '__main__': parser = argparse.ArgumentParser(description=\"bin data out\") parser.add_argument(\"-f\", \"--fai\", metavar=\"\", required=True, help=\"reference index file in fai\") parser.add_argument(\"-b\", \"--bed\", metavar=\"\", required=True, help=\"bed file\") parser.add_argument(\"-o\", \"--output\", metavar=\"\", required=True, help=\"output file\") parser.add_argument(\"-w\", \"--window\", metavar=\"\", default=500, type=int, help=\"window size\") parser.add_argument(\"-l\", \"--overlap\", metavar=\"\", default=250, type=int, help=\"overlap size\") args = parser.parse_args() # 解析索引文件 reflen = getRefLen(args.fai) # 解析bed文件 region = bedParse(args.bed) # 输出bin结果 binCal(region, reflen, args.output, args.window, args.overlap) #target_mark2.0.py import re import os import sys import argparse from collections import defaultdict def getRefLen(fai): ''' 读取fai文件，获取基因组长度 ''' fai_file = open(fai, 'r') ref_Length = {} for line in fai_file: chr_id, seq_len, *other = line.strip().split('\\t') ref_Length[re.sub('chr0|chr','',chr_id)] = int(seq_len) return ref_Length def bedParse(bed): ''' 解析bed文件，分染色体返回预设区间信息 ''' region = defaultdict(list) bed_info = open(bed, 'r') for line in bed_info: chr_id, start, end = re.split('\\s', line.strip()) chr_id = re.sub('chr0|chr', '', chr_id) region[chr_id].append((int(start), int(end))) bed_info.close() return region def binCal(region, reflen, out, win=500, overlap=250): ''' 根据预设的region，遍历基因组，统计每窗口win内的交集片段大小，overlap超过一定范围后输出1，否则输出0 ''' out_file = open(out, 'w') bin = 0 for chr in reflen: for i in range(0, reflen[chr]-500, 500): start_tmp = i end_tmp = i+win region_chr = region[chr] for start, end in region_chr: if end_tmp overlap: bin = 1 else: bin = 0 break elif start_tmp>=end: region[chr].remove((start, end)) out_file.write(f'{chr}\\t{i}\\t{bin}\\t{start}\\t{end}\\n') out_file.close() if __name__ == '__main__': parser = argparse.ArgumentParser(description=\"bin data out\") parser.add_argument(\"-f\", \"--fai\", metavar=\"\", required=True, help=\"reference index file in fai\") parser.add_argument(\"-b\", \"--bed\", metavar=\"\", required=True, help=\"bed file\") parser.add_argument(\"-o\", \"--output\", metavar=\"\", required=True, help=\"output file\") parser.add_argument(\"-w\", \"--window\", metavar=\"\", default=500, type=int, help=\"window size\") parser.add_argument(\"-l\", \"--overlap\", metavar=\"\", default=250, type=int, help=\"overlap size\") args = parser.parse_args() # 解析索引文件 reflen = getRefLen(args.fai) # 解析bed文件 region = bedParse(args.bed) # 输出bin结果 binCal(region, reflen, args.output, args.window, args.overlap) #target_mark3.0.py import re import os import sys import argparse from collections import defaultdict def getRefLen(fai): ''' 读取fai文件，获取基因组长度 ''' fai_file = open(fai, 'r') ref_Length = {} for line in fai_file: chr_id, seq_len, *other = line.strip().split('\\t') ref_Length[re.sub('chr0|chr','',chr_id)] = int(seq_len) return ref_Length def bedParse(bed): ''' 解析bed文件，分染色体返回预设区间信息 ''' region = defaultdict(list) bed_info = open(bed, 'r') for line in bed_info: chr_id, start, end = re.split('\\s', line.strip()) chr_id = re.sub('chr0|chr', '', chr_id) region[chr_id].append((int(start), int(end))) bed_info.close() return region def binCal(region, reflen, out, win=500, overlap=250,threshold = 0.1): ''' 根据预设的region，遍历基因组，统计每窗口win内的交集片段大小，overlap超过一定范围后输出1，否则输出0 ''' out_file = open(out, 'w') bin = 0 for chr in reflen: for i in range(0, reflen[chr], 500): if chr == '1': print (i) start_tmp = i end_tmp = i+win region_chr = region[chr] for start, end in region_chr: mid = (start + end)/2 start_mid = mid - (overlap/2) end_mid = mid + (overlap/2) if end_tmp end_mid: bin = 1 break elif start_midthreshold * overlap: bin = 1 else: bin = 0 break elif start_tmp>=end_mid: region[chr].remove((start, end)) out_file.write(f'{chr}\\t{i}\\t{bin}\\t{start}\\t{end}\\n') out_file.close() if __name__ == '__main__': parser = argparse.ArgumentParser(description=\"bin data out\") parser.add_argument(\"-f\", \"--fai\", metavar=\"\", required=True, help=\"reference index file in fai\") parser.add_argument(\"-b\", \"--bed\", metavar=\"\", required=True, help=\"bed file\") parser.add_argument(\"-o\", \"--output\", metavar=\"\", required=True, help=\"output file\") parser.add_argument(\"-w\", \"--window\", metavar=\"\", default=500, type=int, help=\"window size\") parser.add_argument(\"-l\", \"--overlap\", metavar=\"\", default=250, type=int, help=\"overlap size\") args = parser.parse_args() # 解析索引文件 reflen = getRefLen(args.fai) # 解析bed文件 region = bedParse(args.bed) # 输出bin结果 binCal(region, reflen, args.output, args.window, args.overlap) #target_mark4.0.py import re import os import sys import argparse from collections import defaultdict def getRefLen(fai): ''' 读取fai文件，获取基因组长度 ''' fai_file = open(fai, 'r') ref_Length = {} for line in fai_file: chr_id, seq_len, *other = line.strip().split('\\t') ref_Length[re.sub('chr0|chr','',chr_id)] = int(seq_len) return ref_Length def bedParse(bed): ''' 解析bed文件，分染色体返回预设区间信息 ''' region = defaultdict(list) bed_info = open(bed, 'r') for line in bed_info: chr_id, start, end = re.split('\\s', line.strip()) chr_id = re.sub('chr0|chr', '', chr_id) region[chr_id].append((int(start), int(end))) bed_info.close() return region def binCal(region, reflen, species, histone, out, win=500, overlap=250,threshold = 0.1): ''' 根据预设的region，遍历基因组，统计每窗口win内的交集片段大小，overlap超过一定范围后输出1，否则输出0 ''' out_file = open(out, 'w') bin = 0 for chr in reflen: for i in range(0, reflen[chr], 500): if chr == '1': print (i) start_tmp = i end_tmp = i+win region_chr = region[chr] name = f'{species}_{histone}_{chr}_{start_tmp}_{end_tmp}' for start, end in region_chr: mid = (start + end)/2 start_mid = mid - (overlap/2) end_mid = mid + (overlap/2) if end_tmp end_mid: bin = 1 break elif start_midthreshold * overlap: bin = 1 else: bin = 0 break elif start_tmp>=end_mid: region[chr].remove((start, end)) out_file.write(f'{name}\\t{bin}\\n') out_file.close() if __name__ == '__main__': parser = argparse.ArgumentParser(description=\"bin data out\") parser.add_argument(\"-f\", \"--fai\", metavar=\"\", required=True, help=\"reference index file in fai\") parser.add_argument(\"-b\", \"--bed\", metavar=\"\", required=True, help=\"bed file\") parser.add_argument(\"-o\", \"--output\", metavar=\"\", required=True, help=\"output file\") parser.add_argument(\"-w\", \"--window\", metavar=\"\", default=500, type=int, help=\"window size\") parser.add_argument(\"-l\", \"--overlap\", metavar=\"\", default=250, type=int, help=\"overlap size\") parser.add_argument(\"-s\", \"--species\", metavar=\"\", help=\"rice species\") parser.add_argument(\"-m\", \"--histone\", metavar=\"\", help=\"histone modifications\") args = parser.parse_args() # 解析索引文件 reflen = getRefLen(args.fai) # 解析bed文件 region = bedParse(args.bed) # 输出bin结果 binCal(region, reflen, args.species, args.histone, args.output, args.window, args.overlap) #target_mark5.0.py import re import os import sys import argparse from collections import defaultdict def getRefLen(fai): ''' 读取fai文件，获取基因组长度 ''' fai_file = open(fai, 'r') ref_Length = {} for line in fai_file: chr_id, seq_len, *other = line.strip().split('\\t') ref_Length[re.sub('chr0|chr','',chr_id)] = int(seq_len) return ref_Length def bedParse(bed): ''' 解析bed文件，分染色体返回预设区间信息 ''' region = defaultdict(list) bed_info = open(bed, 'r') for line in bed_info: chr_id, start, end = re.split('\\s', line.strip()) chr_id = re.sub('chr0|chr', '', chr_id) region[chr_id].append((int(start), int(end))) bed_info.close() return region def binCal(region, reflen, species, histone, tar, out, win=500, overlap=250,threshold = 0.1): ''' 根据预设的region，遍历基因组，统计每窗口win内的交集片段大小，overlap超过一定范围后输出1，否则输出0 ''' out_file = open(out, 'w') bin = 0 for chr in reflen: for i in range(0, reflen[chr], 500): if chr == '1': print (i) start_tmp = i end_tmp = i+win region_chr = region[chr] name = f'{species}_{histone}_{chr}_{start_tmp}_{end_tmp}' for start, end in region_chr: mid = (start + end)/2 start_mid = mid - (overlap/2) end_mid = mid + (overlap/2) if end_tmp end_mid: bin = tar break elif start_midthreshold * overlap: bin = tar else: bin = 0 break elif start_tmp>=end_mid: region[chr].remove((start, end)) out_file.write(f'{name}\\t{bin}\\n') out_file.close() if __name__ == '__main__': parser = argparse.ArgumentParser(description=\"bin data out\") parser.add_argument(\"-f\", \"--fai\", metavar=\"\", required=True, help=\"reference index file in fai\") parser.add_argument(\"-b\", \"--bed\", metavar=\"\", required=True, help=\"bed file\") parser.add_argument(\"-o\", \"--output\", metavar=\"\", required=True, help=\"output file\") parser.add_argument(\"-w\", \"--window\", metavar=\"\", default=500, type=int, help=\"window size\") #parser.add_argument(\"-l\", \"--overlap\", metavar=\"\", default=250, type=int, help=\"overlap size\") parser.add_argument(\"-s\", \"--species\", metavar=\"\", help=\"rice species\") parser.add_argument(\"-m\", \"--histone\", metavar=\"\", help=\"histone modifications\") args = parser.parse_args() # 解析索引文件 reflen = getRefLen(args.fai) # 解析bed文件 region = bedParse(args.bed) # 输出bin结果 tar = 0 if args.histone == 'H3K4me3': tar = 1 overlap = 250 elif args.histone == 'H3K27ac': tar = 2 overlap = 250 elif args.histone == 'H3K4me1': tar = 3 overlap = 750 elif args.histone == 'H3K27me3': tar = 4 overlap = 750 elif args.histone == 'H3K9me2': tar = 5 overlap = 1000 binCal(region, reflen, args.species, args.histone, tar, args.output, args.window, overlap) 提取bin列作为标签 awk '{print $3}' target_C^^^.bed >> data/target/target_C^^^.txt rm -f target_C^^^.bed 删除fasta文件中的染色体行 sed -i '/>/d' C^^^_C^^^. 合成数据集 data target 500bp sequence 1 for 组蛋白修饰 0 for 无修饰 one-hot编码 [nums, 500, 4 ] [1,0] 将数据导入excel文件（data_C^^^.xlxs) 数据分布(C019) 1 478680 0 267802 总计 746483 合成标签均衡，随机分布的数据集 paste data/sequence/C^^^.fasta data/target/target_C^^^.txt > data/dataset/C^^^.csv #!/bin/bash for n in $(cat histone) do mkdir ${n} for i in $(cat id.txt) do paste data/seq/${i}.fasta data/target/target_${i}_${n}.txt > data/dataset/${n}/${i}_${n}.csv && sed -i '/>/d'data/dataset/${n}/${i}_${n}.csv done done 输入数据 以onehot（[4,500]）的形式输入 OneHotEncoder.py 注意将excel中含有N(n)的序列删去，否则会引起维度不统一 import numpy as np import pandas as pd import torch np.set_printoptions(threshold= np.inf) # function to convert a DNA sequence string to a numpy array # converts to lower case, changes any non 'acgt' characters to 'n' import re def string_to_array(my_string): my_string = my_string.lower() my_string = re.sub('[^acgt]', 'z', my_string) my_array = np.array(list(my_string)) return my_array # create a label encoder with 'acgtn' alphabet from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() label_encoder.fit(np.array(['a','c','g','t','z'])) # function to one-hot encode a DNA sequence string # non 'acgt' bases (n) are 0000 # returns a L x 4 numpy array from sklearn.preprocessing import OneHotEncoder def one_hot_encoder(my_array): integer_encoded = label_encoder.transform(my_array) onehot_encoder = OneHotEncoder(sparse=False, dtype=int) integer_encoded = integer_encoded.reshape(len(integer_encoded), 1) onehot_encoded = onehot_encoder.fit_transform(integer_encoded) return onehot_encoded data = pd.read_excel(r'./data_bal.xlsx',header = None) one_hot_matrix = [] for i in range(10000): one_hot_matrix.append(one_hot_encoder(string_to_array(data[0][i]))) np.save('one_hot',one_hot_matrix) one = np.load('./one_hot.npy',allow_pickle=True) one.shape OneHotEncoder2.0.py 1.加入argparse接口，方便命令行使用 2.改为双功能函数，check：对数据集进行清理，删除不符合编码规则的样本；make：对数据集进行onehot编码 #OneHotEncoder2.0.py #coding: utf-8 import numpy as np import pandas as pd import torch import os import sys import argparse from collections import defaultdict # function to convert a DNA sequence string to a numpy array # converts to lower case, changes any non 'acgt' characters to 'n' import re def string_to_array(my_string): my_string = my_string.lower() my_string = re.sub('[^acgt]', 'z', my_string) my_array = np.array(list(my_string)) return my_array # create a label encoder with 'acgtn' alphabet from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() label_encoder.fit(np.array(['a','c','g','t','z'])) # function to one-hot encode a DNA sequence string # non 'acgt' bases (n) are 0000 # returns a L x 4 numpy array from sklearn.preprocessing import OneHotEncoder def one_hot_encoder(my_array): integer_encoded = label_encoder.transform(my_array) onehot_encoder = OneHotEncoder(sparse=False, dtype=int) integer_encoded = integer_encoded.reshape(len(integer_encoded), 1) onehot_encoded = onehot_encoder.fit_transform(integer_encoded) return onehot_encoded global one_hot_matrix global index_to_delete one_hot_matrix = [] index_to_delete = [] def encoder(data): one_hot_matrix = [] for i in range(len(data)): one_hot_matrix.append(one_hot_encoder(string_to_array(data[0][i]))) return one_hot_matrix def delete(one_hot_matrix): for i in range(len(data)): if one_hot_matrix[i].shape != (500,4): index_to_delete.append(i) #one_hot_matrix = [one_hot_matrix[i] for i in range(0, len(one_hot_matrix),1) if i not in index_to_delete] return index_to_delete def labels_marker(histone, nums): labels = data.iloc[:len(data),1] #labels.drop(index = index_to_delete, axis = 0, inplace = True) labels.to_csv('labels_{a}_{b}.txt'.format(a=histone, b=nums),sep = '\\t',index = False) print(labels.shape) def dataset(histone,nums): global index_to_delete data.drop(index = index_to_delete, axis = 0,inplace = True) data.to_csv('dataset_{a}_{b}.txt'.format(a=histone, b=nums),sep = '\\t',index = False) #os.system(\"sed -i '1d' 'dataset_{a}_{b}.txt'.format(a=histone, b=nums)\") def save(histone,nums): global one_hot_matrix np.save('onehot_{a}_{b}'.format(a=histone, b=nums),one_hot_matrix) one = np.load('onehot_{a}_{b}.npy'.format(a=histone, b=nums),allow_pickle=True) print(one.shape) if __name__ == '__main__': parser = argparse.ArgumentParser(description=\"data choice out\") parser.add_argument(\"-i\", \"--dataset\", metavar=\"\", required=True, help=\"dataset\") parser.add_argument(\"-c\", \"--choice\", metavar=\"\", required=True,default='make', help=\"make or check\") parser.add_argument(\"-m\", \"--histone\", metavar=\"\" , help=\"hitone modifications\") parser.add_argument(\"-n\", \"--nums\", metavar=\"\", help=\"number of samples\") args = parser.parse_args() # 读取数据集 data = pd.read_csv('%s'%(args.dataset) ,sep = '\\t',header = None) # 编码 one_hot_matrix = encoder(data) if args.choice == 'check': index_to_delete = delete(one_hot_matrix) dataset(args.histone, args.nums) elif args.choice == 'make': labels_marker(args.histone, args.nums) save(args.histone, args.nums) else: print('There is no choice') OneHotEncoder3.0 1.输出npz文件包含['keys', 'DNAseq', 'labels'] --make save() ,保证三者的shape，并且能够输出检查 2.labels的onehot 编码函数，按照数字进行onehot编码 3.导入target文件，并且将keys和labels分开 4.np.delete(labels,0,2)删除标签onehot第一列（0） # coding: utf-8 import numpy as np import pandas as pd import torch import os import sys import argparse from collections import defaultdict # function to convert a DNA sequence string to a numpy array # converts to lower case, changes any non 'acgt' characters to 'n' import re def string_to_array(my_string): my_string = my_string.lower() my_string = re.sub('[^acgt]', 'z', my_string) my_array = np.array(list(my_string)) return my_array # create a label encoder with 'acgtn' alphabet from sklearn.preprocessing import LabelEncoder label_encoder = LabelEncoder() label_encoder.fit(np.array(['a','c','g','t','z'])) # function to one-hot encode a DNA sequence string # non 'acgt' bases (n) are 0000 # returns a L x 4 numpy array from sklearn.preprocessing import OneHotEncoder def one_hot_encoder(my_array): integer_encoded = label_encoder.transform(my_array) onehot_encoder = OneHotEncoder(sparse=False, dtype=int) integer_encoded = integer_encoded.reshape(len(integer_encoded), 1) onehot_encoded = onehot_encoder.fit_transform(integer_encoded) return onehot_encoded def dense_to_one_hot(labels_dense, num_classes): \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\" num_labels = labels_dense.shape[0] index_offset = np.arange(num_labels) * num_classes labels_one_hot = np.zeros((num_labels, num_classes)) labels_one_hot.flat[index_offset+labels_dense.ravel()] = 1 return labels_one_hot global one_hot_matrix global index_to_delete one_hot_matrix = [] index_to_delete = [] def encoder(data): one_hot_matrix = [] for i in range(len(data)): one_hot_matrix.append(one_hot_encoder(string_to_array(data[0][i]))) return one_hot_matrix def delete(one_hot_matrix): for i in range(len(data)): if one_hot_matrix[i].shape != (500,4): index_to_delete.append(i) #one_hot_matrix = [one_hot_matrix[i] for i in range(0, len(one_hot_matrix),1) if i not in index_to_delete] return index_to_delete def labels_marker(): #labels = data.iloc[:len(data),1] labels_dense = np.array(data[2].tolist()) num_classes = 6 labels = dense_to_one_hot(labels_dense, num_classes) labels = labels.reshape(len(data), 1,6) labels = np.delete(labels,0,2) return labels #labels.drop(index = index_to_delete, axis = 0, inplace = True) #labels.to_csv('labels_{a}_{b}.txt'.format(a=histone, b=nums),sep = '\\t',index = False) #print(labels.shape) def dataset(): global index_to_delete data.drop(index = index_to_delete, axis = 0,inplace = True) data.to_csv('dataset_rice_all.txt',sep = '\\t',index = False) dataset = open('dataset_rice_all.txt').readlines() dataset[1] = '' with open('dataset_rice_all.txt','w') as f: f.writelines(dataset) #os.system(\"sed -i '1d' 'dataset_{a}_{b}.txt'.format(a=histone, b=nums)\") def save(labels): global one_hot_matrix one_hot_matrix = np.array(one_hot_matrix) print(one_hot_matrix.shape) one_hot_matrix = one_hot_matrix.reshape(len(data),1,4,500) keys = np.array(data[1].tolist()) np.savez('./onehot_rice.npz',keys = keys, DNAseq = one_hot_matrix, labels = labels) with np.load('onehot_rice.npz') as f: indexs = f['keys'] dna = f['DNAseq'] labels = f['labels'] print(indexs.shape, dna.shape, labels.shape) #one = np.load('onehot_{a}_{b}.npy'.format(a=histone, b=nums),allow_pickle=True) #print(one.shape) if __name__ == '__main__': parser = argparse.ArgumentParser(description=\"data choice out\") parser.add_argument(\"-i\", \"--dataset\", metavar=\"\", required=True, help=\"dataset\") parser.add_argument(\"-c\", \"--choice\", metavar=\"\", required=True,default='make', help=\"make or check\") args = parser.parse_args() # 读取数据集 data = pd.read_csv('%s'%(args.dataset) ,sep = '\\t',header = None) # 编码 one_hot_matrix = encoder(data) if args.choice == 'check': index_to_delete = delete(one_hot_matrix) dataset() elif args.choice == 'make': labels = labels_marker() save(labels) else: print('There is no choice') 生成数据集总流程（bash脚本） rice/scripts/sh_scripts id_geno C019 C051 C135 C139 C145 C146 C147 C148 C151 W081 W105 W125 W128 W161 W169 W257 W261 W286 W294 W306 id_sp C019 C051 C135 C139 ZS97 Nip MH63 C148 C151 W081 W105 W125 W128 W161 W169 W257 W261 W286 W294 W306 生成品种基因型 #!/bin/bash #生成品种基因型，参考基因组日本晴第七版 for i in $(cat id_geno) do bsub -q high -e ${i}.err -o ${i}.out \"python3 scripts/py_scripts/geno_sub2.0.py -s data/vcf/sort.total.vcf -n ${i} -r data/ref/IRGSP-1.0_genome.fasta -l 500\" done samtools #!/bin/bash #使用samtools对基因型进行注释 for i in $(cat id_geno) do samtools faidx ${i}.fasta done #!/bin/bash #将原始基因型的名字改为品种名 for i in $(cat id_geno) do if ${i}.fasta.fai == C145.fasta.fai do mv ${i}.fasta.fai ZS97.fasta.fai elif ${i}.fasta.fai == C146.fasta.fai do mv ${i}.fasta.fai Nip.fasta.fai elif ${i}.fasta.fai == C147.fasta.fai do mv ${i}.fasta.fai MH63.fasta.fai done 提取peak范围 #!/bin/bash for n in $(cat histone) do for i in $(cat id_sp) do awk '{print $1,$2,$3}' mapto*/${n}_seedlings/*${i}_peaks.*Peak | sort -t ' ' -k1.4nr | sort -k1,1V -k2,2n -k3,3n > data/signal_area/sort_${i}_${n}.bed done done 生成组蛋白修饰标签 #!/bin/bash for n in $(cat histone) do for i in $(cat id_sp) do python3 scripts/py_scripts/target_mark5.0.py -f data/sequence/${i}.fasta.fai -b data/signal_area/sort_${i}_${n}.bed -o target_${i}_${n}_2.bed -s ${i} -m ${n} && paste data/seq/${i}_2.fasta target_${i}_${n}_2.bed | awk '$3 != 0 {print}' >> dataset_rice_raw.csv | awk '{print $3}' | uniq -c done done data/seq文件夹中无\">\" 数据预处理(check) bsub -q high -e check.err -o check.out 'python3 scripts/py_scripts/OneHotEncoder3.0.py -i dataset_rice_raw -c check' awk '{print $1,$3}' sed -i '/N/d' awk '$2 == 2 {print}' ../dataset_all_90w.txt | awk -v OFS=\"\\t\" '{if ($2 != 0) $3 = 1}1' >> H3K27ac.txt bsub -q high 'shuf -n243105 dataset_0.txt >> H3K4me3.txt' 编码onehot bsub -q high -e make.err -o make.out 'python3 OneHotEncoder3.0.py -i dataset_rice_all -c make' 生成Transformer数据集 提取$1和$3，然后与等数量的阴性样本连接 awk '{print $1,$3}' dataset_rice_all.txt > dataser_trans.txt "},"实验流程/构建模型.html":{"url":"实验流程/构建模型.html","title":"构建模型","keywords":"","body":"构建模型 构建基本模型（pytorch） 初步实验在恒源云服务器上进行 导入模块 import numpy as np import pandas as pd import torch from torch.utils import data import time from torch import nn, optim import sys 设置GPU device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') 数据导入及格式转换 data_np = np.load('./one_hot.npy') target_pd =pd.read_csv('./target6.txt',sep = '\\n') #转换成tensor data_tensor = torch.from_numpy(data_np) target_array = np.array(target_pd) target_tensor = torch.tensor(target_array) #转置成(4，500) data_tensor = data_tensor.reshape(10000,4,500) #转换成float（后续模型输入需要） data_tensor = data_tensor.float() target_tenor = target_tensor.float() 构建训练集和验证集（TensorDataset类和DataLoader类） class TensorDataset(data.Dataset): \"\"\"Dataset wrapping data and target tensors. Each sample will be retrieved by indexing both tensors along the first dimension. Arguments: data_tensor (Tensor): contains sample data. target_tensor (Tensor): contains sample targets (labels). \"\"\" def __init__(self, data_tensor, target_tensor): assert data_tensor.size(0) == target_tensor.size(0) self.data_tensor = data_tensor self.target_tensor = target_tensor def __getitem__(self, index): return self.data_tensor[index], self.target_tensor[index] def __len__(self): return len(self.data_tensor) train_data = TensorDataset(data_tensor[0:9000], target_tensor[0:9000]) val_data = TensorDataset(data_tensor[9000:10000], target_tensor[9000:10000]) train_dataloader = data.DataLoader(train_data,batch_size = 200, shuffle=True,num_workers=0) val_dataloader = data.DataLoader(val_data,batch_size = 200, shuffle=False,num_workers=0) DeepSEA Predicting effects of noncoding variants with deep learning–based sequence model 为防止过拟合增大泛化能力，在卷积层后加入BN层（归一化） class DeepSEA(nn.Module): def __init__(self, sequence_length=500, n_genomic_features=2): \"\"\" Parameters ---------- sequence_length : int n_genomic_features : int \"\"\" super(DeepSEA, self).__init__() conv_kernel_size = 8 pool_kernel_size = 4 self.conv_net = nn.Sequential( nn.Conv1d(4, 320, kernel_size=conv_kernel_size), nn.BatchNorm1d(320), nn.ReLU(inplace=True), nn.MaxPool1d( kernel_size=pool_kernel_size, stride=pool_kernel_size), nn.Dropout(p=0.2), nn.Conv1d(320, 480, kernel_size=conv_kernel_size), nn.BatchNorm1d(480), nn.ReLU(inplace=True), nn.MaxPool1d( kernel_size=pool_kernel_size, stride=pool_kernel_size), nn.Dropout(p=0.2), nn.Conv1d(480, 960, kernel_size=conv_kernel_size), nn.BatchNorm1d(960), nn.ReLU(inplace=True), nn.Dropout(p=0.5)) reduce_by = conv_kernel_size - 1 self.n_channels = int( np.floor( (np.floor( (sequence_length - reduce_by) / pool_kernel_size) - reduce_by) / pool_kernel_size) - reduce_by) self.classifier = nn.Sequential( nn.Linear(960 * self.n_channels, n_genomic_features), nn.ReLU(inplace=True), nn.Linear(n_genomic_features, n_genomic_features), nn.Sigmoid()) def forward(self, x): \"\"\"Forward propagation of a batch. \"\"\" out = self.conv_net(x) reshape_out = out.view(out.size(0), 960 * self.n_channels) predict = self.classifier(reshape_out) return predict 创建训练函数 损失函数：CrossEntropyLoss def train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs): net = net.to(device) print(\"training on \", device) loss = torch.nn.CrossEntropyLoss() for epoch in range(num_epochs): train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time() for X, y in train_iter: X = X.to(device) y = y.to(device) y_hat = net(X) y = y.squeeze() l = loss(y_hat, y) optimizer.zero_grad() l.backward() optimizer.step() train_l_sum += l.cpu().item() train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item() n += y.shape[0] batch_count += 1 test_acc = evaluate_accuracy(test_iter, net) print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec' % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start)) 创建准确率计算函数 def evaluate_accuracy(data_iter, net, device=None): if device is None and isinstance(net, torch.nn.Module): device = list(net.parameters())[0].device acc_sum, n = 0.0, 0 with torch.no_grad(): for X, y in data_iter: if isinstance(net, torch.nn.Module): net.eval() # 评估模式, 这会关闭dropout acc_sum += (net(X.to(device)).argmax(dim=1)==y.to(device).squeeze()).float().sum().cpu().item() net.train() # 改回训练模式 else: if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数 # 将is_training设置成False acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() else: acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() n += y.shape[0] return acc_sum / n 开启训练 超参数 lr 学习率 num_epochs 训练批次 优化器：Adam 优化器：SGD lr, num_epochs = 0.001, 10 optimizer = torch.optim.SGD(net.parameters(), lr=lr) train(net, train_dataloader, val_dataloader, 256, optimizer, device, num_epochs) 训练结果可视化 更新train函数 恒源云 logger.log_value('loss', train_l_sum/batch_count, epoch*len(train_iter) + batch_count) logger.log_value('train_acc', 100. *train_acc_sum / n, epoch*len(train_iter) + batch_count) logger.log_value('val_acc',test_acc,epoch) TensorBoard_logger from tensorboard_logger import Logger logger = Logger(logdir=\"./tb_logs\", flush_secs=10)#设置输出的log文件位置 def train(net, train_iter, test_iter, batch_size, optimizer, device, num_epochs): net = net.to(device) print(\"training on \", device) loss = torch.nn.CrossEntropyLoss() for epoch in range(num_epochs): train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time() for X, y in train_iter: X = X.to(device) y = y.to(device) y_hat = net(X) y = y.squeeze() l = loss(y_hat, y) optimizer.zero_grad() l.backward() optimizer.step() train_l_sum += l.cpu().item() train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item() n += y.shape[0] batch_count += 1 test_acc = evaluate_accuracy(test_iter, net) logger.log_value('loss', train_l_sum/batch_count, epoch*len(train_iter) + batch_count) logger.log_value('train_acc', 100. *train_acc_sum / n, epoch*len(train_iter) + batch_count) logger.log_value('val_acc',test_acc,epoch) print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec' % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start)) 模型框架 文件组织架构 /public/home/xwli/xwzhang/deeplearningDATA/rice/pytorch_deepsea ├── checkpoints/ ├── data/ │ ├── __init__.py │ ├── dataset.py │ ├── models/ │ ├── __init__.py │ ├── DeepSEA.py │ │ └── utils/ │ ├── __init__.py │ └── visualize.py ├── config.py ├── main.py ├── README.md 数据加载模块 #dataset.py from torch.utils import data class TensorDataset(data.Dataset): \"\"\"Dataset wrapping data and target tensors. Each sample will be retrieved by indexing both tensors along the first dimension. Arguments: data_tensor (Tensor): contains sample data. target_tensor (Tensor): contains sample targets (labels). \"\"\" def __init__(self, data_tensor, target_tensor): assert data_tensor.size(0) == target_tensor.size(0) self.data_tensor = data_tensor self.target_tensor = target_tensor def __getitem__(self, index): return self.data_tensor[index], self.target_tensor[index] def __len__(self): return len(self.data_tensor) 模型定义模块 # coding: utf-8 import torch import time class BasicModule(torch.nn.Module): ''' 封装了nn.Module，主要提供save和load两个方法 ''' def __init__(self,opt=None): super(BasicModule,self).__init__() self.model_name = str(type(self)) # 模型的默认名字 def load(self, path): ''' 可加载指定路径的模型 ''' self.load_state_dict(torch.load(path)) def save(self, name=None): ''' 保存模型，默认使用“模型名字+时间”作为文件名， 如AlexNet_0710_23:57:29.pth ''' if name is None: prefix = 'checkpoints/' + self.model_name + '_' name = time.strftime(prefix + '%m%d_%H:%M:%S.pth') torch.save(self.state_dict(), name) return name #__init__.py from .DeepSEA import DeepSEA #from .new_module import NewModule # coding: utf-8 import numpy as np from torch import nn from .BasicModule import BasicModule class DeepSEA(nn.Module): def __init__(self, sequence_length=500, n_genomic_features=2): \"\"\" Parameters ---------- sequence_length : int n_genomic_features : int \"\"\" super(DeepSEA, self).__init__() conv_kernel_size = 8 pool_kernel_size = 4 self.conv_net = nn.Sequential( nn.Conv1d(4, 320, kernel_size=conv_kernel_size), nn.ReLU(inplace=True), nn.MaxPool1d( kernel_size=pool_kernel_size, stride=pool_kernel_size), nn.Dropout(p=0.2), nn.Conv1d(320, 480, kernel_size=conv_kernel_size), nn.ReLU(inplace=True), nn.MaxPool1d( kernel_size=pool_kernel_size, stride=pool_kernel_size), nn.Dropout(p=0.2), nn.Conv1d(480, 960, kernel_size=conv_kernel_size), nn.ReLU(inplace=True), nn.Dropout(p=0.5)) reduce_by = conv_kernel_size - 1 self.n_channels = int( np.floor( (np.floor( (sequence_length - reduce_by) / pool_kernel_size) - reduce_by) / pool_kernel_size) - reduce_by) self.classifier = nn.Sequential( nn.Linear(960 * self.n_channels, n_genomic_features), nn.ReLU(inplace=True), nn.Linear(n_genomic_features, n_genomic_features), nn.Sigmoid() ) def forward(self, x): \"\"\"Forward propagation of a batch. \"\"\" out = self.conv_net(x) reshape_out = out.view(out.size(0), 960 * self.n_channels) predict = self.classifier(reshape_out) return predict 工具函数 #coding:utf8 #visualize.py import visdom import time import numpy as np class Visualizer(object): ''' 封装了visdom的基本操作，但是你仍然可以通过`self.vis.function` 或者`self.function`调用原生的visdom接口 比如 self.text('hello visdom') self.histogram(t.randn(1000)) self.line(t.arange(0, 10),t.arange(1, 11)) ''' def __init__(self, env='default', **kwargs): self.vis = visdom.Visdom(env=env, **kwargs) # 画的第几个数，相当于横坐标 # 比如（’loss',23） 即loss的第23个点 self.index = {} self.log_text = '' def reinit(self, env='default', **kwargs): ''' 修改visdom的配置 ''' self.vis = visdom.Visdom(env=env, **kwargs) return self def plot_many(self, d): ''' 一次plot多个 @params d: dict (name, value) i.e. ('loss', 0.11) ''' for k, v in d.iteritems(): self.plot(k, v) def img_many(self, d): for k, v in d.iteritems(): self.img(k, v) def plot(self, name, y, **kwargs): ''' self.plot('loss', 1.00) ''' x = self.index.get(name, 0) self.vis.line(Y=np.array([y]), X=np.array([x]), win=unicode(name), opts=dict(title=name), update=None if x == 0 else 'append', **kwargs ) self.index[name] = x + 1 def img(self, name, img_, **kwargs): ''' self.img('input_img', t.Tensor(64, 64)) self.img('input_imgs', t.Tensor(3, 64, 64)) self.img('input_imgs', t.Tensor(100, 1, 64, 64)) self.img('input_imgs', t.Tensor(100, 3, 64, 64), nrows=10) ''' self.vis.images(img_.cpu().numpy(), win=unicode(name), opts=dict(title=name), **kwargs ) def log(self, info, win='log_text'): ''' self.log({'loss':1, 'lr':0.0001}) ''' self.log_text += ('[{time}] {info} '.format( time=time.strftime('%m%d_%H%M%S'),\\ info=info)) self.vis.text(self.log_text, win) def __getattr__(self, name): ''' self.function 等价于self.vis.function 自定义的plot,image,log,plot_many等除外 ''' return getattr(self.vis, name) 配置文件 #config.py class DefaultConfig(object): env = 'default' # visdom 环境 model = 'DeepSEA' # 使用的模型，名字必须与models/__init__.py中的名字一致 train_data_root = './data/one_hot_10w_sh.npy' # 训练集存放路径 test_data_root = './data/test1' # 测试集存放路径 target_data_root = './data/target_10w_sh.txt' load_model_path = 'checkpoints/model.pth' # 加载预训练的模型的路径，为None代表不加载 batch_size = 256 # batch size use_gpu = True # use GPU or not num_workers = 2 # how many workers for loading data print_freq = 20 # print info every N batch #debug_file = '/tmp/debug' # if os.path.exists(debug_file): enter ipdb result_file = 'result.csv' num_epochs = 10 lr = 0.01 # initial learning rate lr_decay = 0.95 # when val_loss increase, lr = lr*lr_decay weight_decay = 1e-4 # 损失函数 def parse(self, kwargs): ''' 根据字典kwargs 更新 config参数 ''' # 更新配置参数 for k, v in kwargs.items(): ''' if not hasattr(self, k): # 警告还是报错，取决于你个人的喜好 warnings.warn(\"Warning: opt has not attribut %s\" %k) ''' setattr(self, k, v) # 打印配置信息 print('user config:') for k, v in self.__class__.__dict__.items(): if not k.startswith('__'): print(k, getattr(self, k)) DefaultConfig.parse = parse opt =DefaultConfig() 主函数 #main.py from config import opt import os import torch from torch.utils import data import models import numpy as np import pandas as pd from data.dataset import TensorDataset from torch.utils.data import DataLoader #from torch.autograd import Variable #from torchnet import meter #from utils.visualize import Visualizer #from tqdm import tqdm import time def evaluate_accuracy(data_iter, net, device=None): if device is None and isinstance(net, torch.nn.Module): # 如果没指定device就使用net的device device = list(net.parameters())[0].device acc_sum, n = 0.0, 0 with torch.no_grad(): for X, y in data_iter: ''' if isinstance(net, torch.nn.Module): net.eval() # 评估模式, 这会关闭dropout acc_sum += (net(X.to(device)).argmax(dim=1) == y.to(device)).float().sum().cpu().item() net.train() # 改回训练模式 else: if('is_training' in net.__code__.co_varnames): # 如果有is_training这个参数 # 将is_training设置成False acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() else: acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() ''' acc_sum += (net(X.to(device)).argmax(dim=1) == (y.to(device).squeeze())).float().sum().cpu().item() n += y.shape[0] return acc_sum / n def train(**kwargs): opt.parse(kwargs) #vis = Visualizer(opt.env) model = getattr(models, opt.model)() ''' if opt.load_model_path: model.load(opt.load_model_path) if opt.use_gpu: model.cuda() ''' device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') print(\"training on \", device) data_np = np.load(opt.train_data_root) data_tensor = torch.from_numpy(data_np) target_pd =pd.read_csv(opt.target_data_root,sep = '\\n',header = None) target_array = np.array(target_pd) target_tensor = torch.tensor(target_array) data_tensor = data_tensor.reshape(100000,4,500) data_tensor = data_tensor.float() target_tenor = target_tensor.float() train_data = TensorDataset(data_tensor[0:99000], target_tensor[0:99000]) val_data = TensorDataset(data_tensor[99000:100000], target_tensor[99000:100000]) train_dataloader = data.DataLoader(train_data,batch_size = 200, shuffle=True,num_workers=0) val_dataloader = data.DataLoader(val_data,batch_size = 200, shuffle=False,num_workers=0) loss = torch.nn.CrossEntropyLoss() lr = opt.lr optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay = opt.weight_decay) for epoch in range(opt.num_epochs): model = model.to(device) train_l_sum, train_acc_sum, n, batch_count, start = 0.0, 0.0, 0, 0, time.time() for X, y in train_dataloader: X = X.to(device) y = y.to(device) y_hat = model(X) y = y.squeeze() l = loss(y_hat, y) optimizer.zero_grad() l.backward() optimizer.step() train_l_sum += l.cpu().item() train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item() n += y.shape[0] batch_count += 1 test_acc = evaluate_accuracy(val_dataloader, model) print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec' % (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start)) #model.save() def val(model, dataloader): ''' 计算模型在验证集上的准确率等信息，用以辅助训练 ''' pass def test(**kwargs): ''' 测试（inference） ''' pass def help(): ''' 打印帮助的信息 ''' print('help') if __name__=='__main__': import fire fire.Fire() DeepHistone 参考文献： DeepHistone: a deep learning approach to predicting histone modification github：https://github.com/QijinYin/DeepHistone. 模型架构 三个模块：DNA module ，DNase module， joint module DNA and DNase module： densely connected convolutional neural network joint module： distinguish histone modification sites of a marker from those of other markers 图片来自参考文献 DenseNet 图片来自参考文献 利用前面所有层与后面层的“短路连接”来实现特征重用，具有更高的性能，更少的参数量 参考文献： 论文：Densely Connected Convolutional Networks 模型详解： https://blog.csdn.net/u014380165/article/details/75142664 https://zhuanlan.zhihu.com/p/37189203 数据集格式改良 labels onehot encoder 可以将此函数加入OneHotEncoder import numpy as np def dense_to_one_hot(labels_dense, num_classes): \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\" num_labels = labels_dense.shape[0] index_offset = np.arange(num_labels) * num_classes labels_one_hot = np.zeros((num_labels, num_classes)) labels_one_hot.flat[index_offset+labels_dense.ravel()] = 1 return labels_one_hot labels_dense = np.array([0,1,2,3,4]) num_classes = 5 dense_to_one_hot(labels_dense,num_classes) 继续改进target_mark.py : 将组蛋白修饰（m）与数字标签对应，同时改良每种组蛋白的标记方法 npz封装：将npz封装也加入OneHotEncoder，加入到make np.savez('C:/Users/12394/PycharmProjects/Spyder/data.npz',a = a, b = b) 1.将5种组蛋白修饰阳性样本集中在一个数据集 2.reshape(len(data),1,4,500) OneHotEncoder3.0.py完成 bsub -q high -e 12.err -o 12.out 'python3 scripts/py_scripts/OneHotEncoder3.0.py -i dataset_ex1_10w.txt -c make -m ex1 -n 10w' Keys DNA seq labels (nums,) (nums, 1 , 4 , 500 ) (nums ,1, 5) species_histone_chr_start_end onehot onehot DNA-only（DNA module） /public/home/xwli/xwzhang/deeplearningDATA/rice/DeepHistone/ ├── data/ ├── results/ │ └── model.txt │ └── label.txt │ └── pred.txt ├── model_dna.py ├── utils_dna.py ├── train_dna.py model_dna.py import numpy as np import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torch.autograd import Variable from sklearn import metrics from torch.optim import Optimizer import math from torch.nn.parameter import Parameter class BasicBlock(nn.Module): def __init__(self, in_planes, grow_rate,): super(BasicBlock, self).__init__() self.block = nn.Sequential( nn.BatchNorm2d(in_planes), nn.ReLU(), nn.Conv2d(in_planes, grow_rate, (1,9), 1, (0,4)), #nn.Dropout2d(0.2) ) def forward(self, x): out = self.block(x) return torch.cat([x, out],1) class DenseBlock(nn.Module): def __init__(self, nb_layers, in_planes, grow_rate,): super(DenseBlock, self).__init__() layers = [] for i in range(nb_layers): layers.append(BasicBlock(in_planes + i*grow_rate, grow_rate,)) self.layer = nn.Sequential(*layers) def forward(self, x): return self.layer(x) class ModuleDense(nn.Module): def __init__(self): super(ModuleDense, self).__init__() self.conv1 = nn.Sequential( nn.Conv2d(1,128,(4,9),1,(0,4)), #nn.Dropout2d(0.2), ) self.block1 = DenseBlock(3, 128, 128) self.trans1 = nn.Sequential( nn.BatchNorm2d(128+3*128), nn.ReLU(), nn.Conv2d(128+3*128, 256, (1,1),1), #nn.Dropout2d(0.2), nn.MaxPool2d((1,4)), ) self.block2 = DenseBlock(3,256,256) self.trans2 = nn.Sequential( nn.BatchNorm2d(256+3*256), nn.ReLU(), nn.Conv2d(256+3*256, 512, (1,1),1), #nn.Dropout2d(0.2), nn.MaxPool2d((1,4)), ) self.out_size = 500 // 4 // 4 * 512 def forward(self, seq): n, h, w = seq.size() seq = seq.view(n,1,4,w) out = self.conv1(seq) out = self.block1(out) out = self.trans1(out) out = self.block2(out) out = self.trans2(out) n, c, h, w = out.size() out = out.view(n,c*h*w) return out class NetDeepHistone(nn.Module): def __init__(self): super(NetDeepHistone, self).__init__() print('DeepHistone(Dense) is used.') self.seq_map = ModuleDense() self.seq_len = self.seq_map.out_size seq_len = self.seq_len self.linear_map = nn.Sequential( nn.Dropout(0.5), nn.Linear(int(seq_len),925), nn.BatchNorm1d(925), nn.ReLU(), #nn.Dropout(0.1), nn.Linear(925,5), nn.Sigmoid(), ) def forward(self, seq): flat_seq = self.seq_map(seq) out = self.linear_map(flat_seq) return out class DeepHistone(): def __init__(self,use_gpu,learning_rate=0.001): self.forward_fn = NetDeepHistone() self.criterion = nn.BCELoss() self.optimizer = optim.Adam(self.forward_fn.parameters(), lr=learning_rate, weight_decay = 0) self.use_gpu = use_gpu if self.use_gpu : self.criterion,self.forward_fn = self.criterion.cuda(), self.forward_fn.cuda() def updateLR(self, fold): for param_group in self.optimizer.param_groups: param_group['lr'] *= fold def train_on_batch(self,seq_batch,lab_batch,): self.forward_fn.train() seq_batch = Variable(torch.Tensor(seq_batch)) lab_batch = Variable(torch.Tensor(lab_batch)) if self.use_gpu: seq_batch, lab_batch = seq_batch.cuda(), lab_batch.cuda() output = self.forward_fn(seq_batch) loss = self.criterion(output,lab_batch) self.optimizer.zero_grad() loss.backward() self.optimizer.step() return loss.cpu().data def eval_on_batch(self,seq_batch,lab_batch,): self.forward_fn.eval() seq_batch = Variable(torch.Tensor(seq_batch)) lab_batch = Variable(torch.Tensor(lab_batch)) if self.use_gpu: seq_batch, lab_batch = seq_batch.cuda(), lab_batch.cuda() output = self.forward_fn(seq_batch) loss = self.criterion(output,lab_batch) return loss.cpu().data,output.cpu().data.numpy() def test_on_batch(self, seq_batch): self.forward_fn.eval() seq_batch = Variable(torch.Tensor(seq_batch)) if self.use_gpu: seq_batch = seq_batch.cuda() output = self.forward_fn(seq_batch) pred = output.cpu().data.numpy() return pred def save_model(self, path): torch.save(self.forward_fn.state_dict(), path) def load_model(self, path): self.forward_fn.load_state_dict(torch.load(path)) untils_dna.py 在原函数的基础上增加了MCC，Specificity和Sensitivity的计算函数，现在有三个具体评估指标 from sklearn.metrics import auc,roc_auc_score,precision_recall_curve,matthews_corrcoef,precision_score,recall_score import numpy as np histones=['H3K4me3','H3K27ac','H3K4me1','H3K27me3','H3K9me2'] def loadRegions(regions_indexs,dna_dict,label_dict,): if dna_dict is not None: dna_regions = np.concatenate([dna_dict[meta] for meta in regions_indexs],axis=0) else: dna_regions =[] label_regions = np.concatenate([label_dict[meta] for meta in regions_indexs],axis=0).astype(int) return dna_regions,label_regions def model_train(regions,model,batchsize,dna_dict,label_dict,): train_loss = [] regions_len = len(regions) for i in range(0, regions_len , batchsize): regions_batch = [regions[i+j] for j in range(batchsize) if (i+j) train_dna.py from model_dna import DeepHistone import copy import numpy as np from utils_dna import metrics,model_train,model_eval,model_predict import torch #setting batchsize=20 train_file = 'onehot_rice_train.npz' test_file = 'onehot_rice_test.npz' model_save_file = 'results/model.txt' lab_save_file ='results/label.txt' pred_save_file ='results/pred.txt' print('Begin loading data...') with np.load(train_file) as f: indexs = f['keys'] dna_dict = dict(zip(f['keys'],f['DNAseq'])) lab_dict = dict(zip(f['keys'],f['labels'])) np.random.shuffle(indexs) idx_len = len(indexs) train_index=indexs[:int(idx_len*4/5)] valid_index=indexs[int(idx_len*4/5):] use_gpu = torch.cuda.is_available() model = DeepHistone(use_gpu) print('Begin training model...') best_model = copy.deepcopy(model) best_valid_auPRC=0 best_valid_loss = np.float64('Inf') for epoch in range(50): np.random.shuffle(train_index) train_loss= model_train(train_index,model,batchsize,dna_dict,lab_dict,) valid_loss,valid_lab,valid_pred,valid_truths= model_eval(valid_index, model,batchsize,dna_dict,lab_dict,) valid_auPRC,valid_auROC,valid_MCC,valid_Spec,valid_Sen= metrics(valid_lab,valid_pred,valid_truths,'Valid',valid_loss) if np.mean(list(valid_auPRC.values())) >best_valid_auPRC: best_model = copy.deepcopy(model) if valid_loss = 5: break print('Begin predicting...') with np.load(test_file) as f: indexs2 = f['keys'] dna_dict2 = dict(zip(f['keys'],f['DNAseq'])) lab_dict2 = dict(zip(f['keys'],f['labels'])) np.random.shuffle(indexs2) idx_len2 = len(indexs) for i in range(5): test_index=indexs2[int((i/5)*idx_len2):int(((i+1)/5)*idx_len2)] test_lab,test_pred,test_truths = model_predict(test_index,best_model,batchsize,dna_dict2,lab_dict2,) test_auPR,test_roc,test_MCC,test_Spec,test_Sen= metrics(test_lab,test_pred,test_truths,'Test') print('Begin saving...') np.savetxt(lab_save_file, test_lab, fmt='%d', delimiter='\\t') np.savetxt(pred_save_file, test_pred, fmt='%.4f', delimiter='\\t') best_model.save_model(model_save_file) torch.save(model,'model1.pth') print('Finished.') 构建总数据集 20个品种5种组蛋白修饰所有阳性 histone modifications nums H3K4me3（1） 256721 H3K27ac（2） 227526 H3K4me1（3） 152355 H3K27me3（4） 136028 H3K9me2（5） 118142 total 890772 total（after check） 887724 #!/bin/bash for n in $(cat histone) do for i in $(cat id.txt) do python3 target_mark5.0.py -f ${i}.fasta.fai -b data/signal_area/sort_${i}.bed -o target_${i}_${n}.bed -s ${i} -m ${n} && paste $[i}.fasta target_${i}_${n}.bed > data/dataset/ex1/${n}.csv && awk '$3 != 0 {print}' data/dataset/ex1/${n}.csv >> dataset_all.csv | awk '{print $3}' | uniq -c done done bsub -q gpu -o deep_histone_result.out -e histone.err 'python3 train_dna.py' Nt_Transformer 参考文献： NUCLEIC TRANSFORMER: DEEP LEARNING ON NUCLEIC ACIDS WITH SELF-ATTENTION AND CONVOLUTIONS github:https://github.com/Shujun-He/Nucleic-Transformer Transformer：Attention is All You Need 参考文献： Attention Is All You Need 论文解析： https://blog.csdn.net/weixin_42431920/article/details/110731751 https://zhuanlan.zhihu.com/p/48508221 https://www.bilibili.com/video/BV1Di4y1c7Zm?from=search&seid=15155897634554281203 学习笔记 模型架构 /public/home/xwli/xwzhang/deeplearningDATA/rice/Nt_Transformer/ ├── data/ ├── results/ │ └── model.txt │ └── label.txt │ └── pred.txt ├── Dataset.py ├── evalute_test.py ├── Functions.py ├── Logger.py ├── LrScheduler.py ├── Metrics.py ├── Network.py ├── train.py ├── run.sh ├── test.sh Dataset.py import pickle import os import numpy as np import pandas as pd from tqdm import tqdm import torch nt_int={ \"A\": 0, \"T\": 1, \"G\": 2, \"C\": 3,} def nucleatide2int(nt_sequence,target_length=None): int_sequence=[] for nt in nt_sequence: nt=nt.upper() if nt in nt_int: int_sequence.append(nt_int[nt]) int_sequence=np.asarray(int_sequence,dtype='int32') if target_length: int_sequence=np.pad(int_sequence,(0,target_length-len(int_sequence)),constant_values=-1) return int_sequence class ViraminerDataset(torch.utils.data.Dataset): def __init__(self,sequences,labels): self.data=[] for seq in sequences: self.data.append(nucleatide2int(seq)) self.data=np.asarray(self.data,dtype='int') self.labels=np.asarray(labels,dtype='int') print(self.data.shape) print(self.labels.shape) def __len__(self): return len(self.labels) def __getitem__(self,idx): return {'data':self.data[idx], 'labels':self.labels[idx]} evalute_test.py import os import torch import torch.nn as nn import time from Functions import * from Dataset import * from Network import * from LrScheduler import * import Metrics from Logger import CSVLogger import argparse try: #from apex.parallel import DistributedDataParallel as DDP from apex.fp16_utils import * from apex import amp, optimizers from apex.multi_tensor_apply import multi_tensor_applier except ImportError: raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to run this example.\") from tqdm import tqdm def get_args(): parser = argparse.ArgumentParser() parser.add_argument('--gpu_id', type=str, default='0,1', help='which gpu to use') parser.add_argument('--path', type=str, default='../', help='path of csv file with DNA sequences and labels') parser.add_argument('--epochs', type=int, default=150, help='number of epochs to train') parser.add_argument('--batch_size', type=int, default=24, help='size of each batch during training') parser.add_argument('--weight_decay', type=float, default=0, help='weight dacay used in optimizer') parser.add_argument('--ntoken', type=int, default=4, help='number of tokens to represent DNA nucleotides (should always be 4)') parser.add_argument('--nclass', type=int, default=2, help='number of classes from the linear decoder') parser.add_argument('--ninp', type=int, default=512, help='ninp for transformer encoder') parser.add_argument('--nhead', type=int, default=8, help='nhead for transformer encoder') parser.add_argument('--nhid', type=int, default=2048, help='nhid for transformer encoder') parser.add_argument('--nlayers', type=int, default=6, help='nlayers for transformer encoder') parser.add_argument('--save_freq', type=int, default=1, help='saving checkpoints per save_freq epochs') parser.add_argument('--dropout', type=float, default=.1, help='transformer dropout') parser.add_argument('--warmup_steps', type=int, default=3200, help='training schedule warmup steps') parser.add_argument('--lr_scale', type=float, default=0.1, help='learning rate scale') parser.add_argument('--nmute', type=int, default=18, help='number of mutations during training') parser.add_argument('--kmers', type=int, nargs='+', default=[2,3,4,5,6], help='k-mers to be aggregated') #parser.add_argument('--kmer_aggregation', type=bool, default=True, help='k-mers to be aggregated') parser.add_argument('--kmer_aggregation', dest='kmer_aggregation', action='store_true') parser.add_argument('--no_kmer_aggregation', dest='kmer_aggregation', action='store_false') parser.set_defaults(kmer_aggregation=True) parser.add_argument('--nfolds', type=int, default=5, help='number of cross validation folds') parser.add_argument('--fold', type=int, default=0, help='which fold to train') parser.add_argument('--val_freq', type=int, default=1, help='which fold to train') opts = parser.parse_args() return opts opts=get_args() #gpu selection os.environ[\"CUDA_VISIBLE_DEVICES\"] = opts.gpu_id device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #lr=0 #checkpointing checkpoints_folder='checkpoints_fold{}'.format((opts.fold)) csv_file='log_fold{}.csv'.format((opts.fold)) columns=['epoch','train_loss','train_acc','recon_acc', 'val_loss','val_auc','val_acc','val_sens','val_spec'] #logger=CSVLogger(columns,csv_file) #build model and logger MODELS=[] for i in range(3): model=NucleicTransformer(opts.ntoken, opts.nclass, opts.ninp, opts.nhead, opts.nhid, opts.nlayers, opts.kmer_aggregation, kmers=opts.kmers, dropout=opts.dropout).to(device) optimizer=torch.optim.Adam(model.parameters(), weight_decay=opts.weight_decay) criterion=nn.CrossEntropyLoss(reduction='none') lr_schedule=lr_AIAYN(optimizer,opts.ninp,opts.warmup_steps,opts.lr_scale) # Initialization opt_level = 'O1' model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level) model = nn.DataParallel(model) pytorch_total_params = sum(p.numel() for p in model.parameters()) print('Total number of paramters: {}'.format(pytorch_total_params)) model.load_state_dict(torch.load(\"best_weights/fold0top{}.ckpt\".format(i+1))) model.eval() MODELS.append(model) dict=MODELS[0].module.state_dict() for key in dict: for i in range(1,len(MODELS)): dict[key]=dict[key]+MODELS[i].module.state_dict()[key] dict[key]=dict[key]/float(len(MODELS)) MODELS[0].module.load_state_dict(dict) avg_model=MODELS[0] def geometric_mean(preds): gmean=np.ones(preds.shape[1:]) for pred in preds: gmean=gmean*pred gmean=gmean**(1/len(preds)) return gmean df=pd.read_csv('../fullset_test.csv',header=None) seqs=[] labels=[] for i in range(len(df)): seqs.append(nucleatide2int(df.iloc[i,1])) labels.append(df.iloc[i,2]) labels=np.asarray(labels).astype(\"int\") seqs=np.asarray(seqs).astype(\"int\") batch_size=128 batches=np.around(len(df)/batch_size+0.5).astype('int') preds=[] softmax = nn.Softmax(dim=1) for i in tqdm(range(batches)): with torch.no_grad(): outputs=[] #for model in MODELS: x=torch.Tensor(seqs[i*batch_size:(i+1)*batch_size]).to(device).long() y=softmax(avg_model(x)) #outputs.append(softmax(y).cpu().numpy()) for vec in y: preds.append(vec.cpu().numpy()) from sklearn import metrics preds=np.asarray(preds) auc=metrics.roc_auc_score(labels,preds[:,1]) with open(\"test_results.p\",'wb+') as f: pickle.dump([labels,preds],f) print(auc) with open(\"test_score.txt\",'w+') as f: f.write(\"test auc score: {}\".format(auc)) # for i in range(3,10): # ngrams=np.arange(2,i) # print(ngrams) # train_fold(0,ngrams) # # train_fold(0,[2,3,4]) Functions.py import torch import os from sklearn import metrics import numpy as np import torch.nn as nn import torch.nn.functional as F from tqdm import tqdm import Metrics import numpy as np import os import pandas as pd import random def seed_everything(seed=42): random.seed(seed) os.environ['PYTHONHASHSEED'] = str(seed) np.random.seed(seed) torch.manual_seed(seed) torch.cuda.manual_seed(seed) torch.backends.cudnn.deterministic = True def get_best_weights_from_fold(fold, record, top=3): csv_file='{}log_fold{}.csv'.format(record, fold) history=pd.read_csv(csv_file) scores=np.asarray(history.val_auc) top_epochs=scores.argsort()[-3:][::-1] print(scores[top_epochs]) os.system('mkdir best_weights') for i in range(top): weights_path='{}checkpoints_fold{}/epoch{}.ckpt'.format(record, fold,history.epoch[top_epochs[i]]) print(weights_path) os.system('cp {} best_weights/fold{}top{}.ckpt'.format(weights_path,fold,i+1)) os.system('rm -r {}checkpoints_fold{}'.format(record, fold)) def smoothcrossentropyloss(pred,gold,n_class=2,smoothing=0.05): gold = gold.contiguous().view(-1) one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1), 1) one_hot = one_hot * (1 - smoothing) + (1 - one_hot) * smoothing / (n_class - 1) log_prb = F.log_softmax(pred, dim=1) loss = -(one_hot * log_prb) #loss=loss.sum(1).mean() return loss def mutate_dna_sequence(sequence,nmute=15): mutation=torch.randint(0,4,size=(sequence.shape[0],nmute)) to_mutate = torch.randperm(sequence.shape[1])[:nmute] sequence[:,to_mutate]=mutation return sequence def get_MLM_mask(sequence,nmask=12): mask=np.zeros(sequence.shape,dtype='bool') to_mask=np.random.choice(len(sequence[0]),size=(nmask),replace=False) mask[:,to_mask]=True return mask def get_complementary_sequence(sequence): complementary_sequence=sequence.copy() complementary_sequence[sequence==0]=1 complementary_sequence[sequence==1]=0 complementary_sequence[sequence==2]=3 complementary_sequence[sequence==3]=2 complementary_sequence=complementary_sequence[:,::-1] return complementary_sequence def update_lr(optimizer, lr): for param_group in optimizer.param_groups: param_group['lr'] = lr def save_weights(model,optimizer,epoch,folder): if os.path.isdir(folder)==False: os.makedirs(folder,exist_ok=True) torch.save(model.state_dict(), folder+'/epoch{}.ckpt'.format(epoch+1)) def validate(model,device,dataset,batch_size=64): batches=len(dataset) model.train(False) total=0 predictions=[] outputs=[] ground_truths=[] loss=0 criterion=nn.CrossEntropyLoss() with torch.no_grad(): for data in tqdm(dataset): X=data['data'].to(device) Y=data['labels'].to(device) output= model(X) del X loss+=criterion(output,Y) classification_predictions = torch.argmax(output,dim=1).squeeze() for pred in classification_predictions: predictions.append(pred.cpu().numpy()) for vector in output: outputs.append(vector.cpu().numpy()) for t in Y: ground_truths.append(t.cpu().numpy()) del output torch.cuda.empty_cache() val_loss=(loss/batches).cpu() ground_truths=np.asarray(ground_truths) predictions=np.asarray(predictions) outputs=np.asarray(outputs) #print(predictions) #print(ground_truths) #score=metrics.cohen_kappa_score(ground_truths,predictions,weights='quadratic') val_acc=Metrics.accuracy(predictions,ground_truths) auc=metrics.roc_auc_score(ground_truths,outputs[:,1]) val_sens=Metrics.sensitivity(predictions,ground_truths) val_spec=Metrics.specificity(predictions,ground_truths) print('Val accuracy: {}, Val Loss: {}'.format(val_acc,val_loss)) return val_loss,auc,val_acc,val_sens,val_spec def predict(model,device,dataset,batch_size=64): batches=int(len(dataset.val_indices)/batch_size)+1 model.train(False) total=0 ground_truths=dataset.labels[dataset.val_indices] predictions=[] attention_weights=[] loss=0 criterion=nn.CrossEntropyLoss() dataset.switch_mode(training=False) dataset.update_batchsize(batch_size) with torch.no_grad(): for i in tqdm(range(len(dataset))): data=dataset[i] X=torch.Tensor(data['data']).to(device,).long() Y=torch.Tensor(data['labels']).to(device,dtype=torch.int64) directions=data['directions'] directions=directions.reshape(len(directions),1)*np.ones(X.shape) directions=torch.Tensor(directions).to(device).long() output,_,_,aw= model(X,directions,None) del X loss+=criterion(output,Y) classification_predictions = torch.argmax(output,dim=1).squeeze() for pred in output: predictions.append(pred.cpu().numpy()) for weight in aw: attention_weights.append(weight.cpu().numpy()) del output torch.cuda.empty_cache() val_loss=(loss/batches).cpu() predictions=np.asarray(predictions) attention_weights=np.asarray(attention_weights) binary_predictions=predictions.copy() binary_predictions[binary_predictions==2]=1 binary_ground_truths=ground_truths.copy() binary_ground_truths[binary_ground_truths==2]=1 return predictions,attention_weights,np.asarray(dataset.data[dataset.val_indices]) Logger.py import csv from os import path class CSVLogger: def __init__(self,columns,file): self.columns=columns self.file=file if not self.check_header(): self._write_header() def check_header(self): if path.exists(self.file): header=True else: header=False return header def _write_header(self): with open(self.file,\"a\") as f: string=\"\" for attrib in self.columns: string+=\"{},\".format(attrib) string=string[:len(string)-1] string+=\"\\n\" f.write(string) return self def log(self,row): if len(row)!=len(self.columns): raise Exception(\"Mismatch between row vector and number of columns in logger\") with open(self.file,\"a\") as f: string=\"\" for attrib in row: string+=\"{},\".format(attrib) string=string[:len(string)-1] string+=\"\\n\" f.write(string) return self LrScheduler.py import csv from os import path class CSVLogger: def __init__(self,columns,file): self.columns=columns self.file=file if not self.check_header(): self._write_header() def check_header(self): if path.exists(self.file): header=True else: header=False return header def _write_header(self): with open(self.file,\"a\") as f: string=\"\" for attrib in self.columns: string+=\"{},\".format(attrib) string=string[:len(string)-1] string+=\"\\n\" f.write(string) return self def log(self,row): if len(row)!=len(self.columns): raise Exception(\"Mismatch between row vector and number of columns in logger\") with open(self.file,\"a\") as f: string=\"\" for attrib in row: string+=\"{},\".format(attrib) string=string[:len(string)-1] string+=\"\\n\" f.write(string) return self Metrics.py import numpy as np def accuracy(predictions,ground_truths): return np.sum(predictions==ground_truths)/len(ground_truths) def sensitivity(predictions,ground_truths): ''' Here it is assumed: 0=negative 1=positive ''' return 1-len(predictions[(predictions==0)*(ground_truths==1)])/len(ground_truths[ground_truths==1]) def specificity(predictions,ground_truths): ''' Here it is assumed: 0=negative 1=positive ''' return 1-len(predictions[(predictions==1)*(ground_truths==0)])/len(ground_truths[ground_truths==0]) def MCC(predictions,ground_truths): ''' Here it is assumed: 0=negative 1=positive ''' N1=len(predictions[(predictions==0)&(ground_truths==1)]) N2=len(predictions[(predictions==1)&(ground_truths==0)]) N3=len(ground_truths[ground_truths==1]) N4=len(ground_truths[ground_truths==0]) sens=1-N1/N3 spec=1-N2/N4 denom=np.sqrt((1+(N2-N1)/N3)*(1+(N1-N2)/N4)) return (1-sens-spec)/denom Network.py import math import torch import torch.nn as nn import torch.nn.functional as F #mish activation class Mish(nn.Module): def __init__(self): super().__init__() def forward(self, x): #inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!) return x *( torch.tanh(F.softplus(x))) from torch.nn.parameter import Parameter def gem(x, p=3, eps=1e-6): return F.avg_pool1d(x.clamp(min=eps).pow(p), (x.size(-1))).pow(1./p) class GeM(nn.Module): def __init__(self, p=3, eps=1e-6): super(GeM,self).__init__() self.p = Parameter(torch.ones(1)*p) self.eps = eps def forward(self, x): return gem(x, p=self.p, eps=self.eps) def __repr__(self): return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')' class TransformerEncoderLayer(nn.Module): r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network. This standard encoder layer is based on the paper \"Attention Is All You Need\". Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 6000-6010. Users may modify or implement in a different way during application. Args: d_model: the number of expected features in the input (required). nhead: the number of heads in the multiheadattention models (required). dim_feedforward: the dimension of the feedforward network model (default=2048). dropout: the dropout value (default=0.1). activation: the activation function of intermediate layer, relu or gelu (default=relu). Examples:: >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8) >>> src = torch.rand(10, 32, 512) >>> out = encoder_layer(src) \"\"\" def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\"relu\"): super(TransformerEncoderLayer, self).__init__() self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout) self.linear1 = nn.Linear(d_model, dim_feedforward) self.dropout = nn.Dropout(dropout) self.linear2 = nn.Linear(dim_feedforward, d_model) self.norm1 = nn.LayerNorm(d_model) self.norm2 = nn.LayerNorm(d_model) self.dropout1 = nn.Dropout(dropout) self.dropout2 = nn.Dropout(dropout) self.activation = Mish() def forward(self, src , src_mask = None, src_key_padding_mask = None): src2,attention_weights = self.self_attn(src, src, src, attn_mask=src_mask, key_padding_mask=src_key_padding_mask) src = src + self.dropout1(src2) src = self.norm1(src) src2 = self.linear2(self.dropout(self.activation(self.linear1(src)))) src = src + self.dropout2(src2) src = self.norm2(src) return src,attention_weights class LinearDecoder(nn.Module): def __init__(self,num_classes,ninp,dropout,pool=True,): super(LinearDecoder, self).__init__() # if pool: # self.pool_layer=GeM() if pool: self.classifier=nn.Linear(ninp,num_classes) else: self.classifier=nn.Linear(ninp,num_classes) self.pool=pool self.pool_layer=GeM() def forward(self,x): if self.pool: # max_x,_=torch.max(x,dim=1) # x=torch.cat([torch.mean(x,dim=1),max_x],dim=-1) #print(x.shape) x=self.pool_layer(x.permute(0,2,1)).permute(0,2,1).squeeze() #print(x.shape) x=self.classifier(x) return x class PositionalEncoding(nn.Module): def __init__(self, d_model, dropout=0.1, max_len=5000): super(PositionalEncoding, self).__init__() self.dropout = nn.Dropout(p=dropout) pe = torch.zeros(max_len, d_model) position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) pe[:, 0::2] = torch.sin(position * div_term) pe[:, 1::2] = torch.cos(position * div_term) pe = pe.unsqueeze(0).transpose(0, 1) self.register_buffer('pe', pe) def forward(self, x): x = x + self.pe[:x.size(0), :] return self.dropout(x) class K_mer_aggregate(nn.Module): def __init__(self,kmers,in_dim,out_dim,dropout=0.1): super(K_mer_aggregate, self).__init__() #self.dropout=nn.Dropout(dropout) self.convs=[] for i in kmers: print(i) self.convs.append(nn.Conv1d(in_dim,out_dim,i,padding=0)) self.convs=nn.ModuleList(self.convs) self.norm=nn.LayerNorm(out_dim) #self.activation=nn.ReLU(inplace=True) #self.activation=Mish() def forward(self,x): outputs=[] for conv in self.convs: outputs.append(conv(x)) outputs=torch.cat(outputs,dim=2) return self.norm(outputs.permute(2,0,1)) class NucleicTransformer(nn.Module): def __init__(self, ntoken, nclass, ninp, nhead, nhid, nlayers, kmer_aggregation, kmers, dropout=0.5,return_aw=False): super(NucleicTransformer, self).__init__() self.model_type = 'Transformer' self.src_mask = None self.pos_encoder = PositionalEncoding(ninp, dropout) self.kmers=kmers #if self.ngrams!=None: self.kmer_aggregation=kmer_aggregation if self.kmer_aggregation: self.k_mer_aggregate=K_mer_aggregate(kmers,ninp,ninp) else: print(\"No kmer aggregation is chosen\") self.transformer_encoder = [] for i in range(nlayers): self.transformer_encoder.append(TransformerEncoderLayer(ninp, nhead, nhid, dropout)) self.transformer_encoder= nn.ModuleList(self.transformer_encoder) self.encoder = nn.Embedding(ntoken, ninp) #self.directional_encoder = nn.Embedding(3, ninp//8) self.ninp = ninp self.decoder = LinearDecoder(nclass,ninp,dropout) self.return_aw=False def forward(self, src): src = src.permute(1,0) #dir = dir.permute(1,0) src = self.encoder(src) #* math.sqrt(self.ninp) #dir = self.directional_encoder(dir) #src = torch.cat([src,dir],dim=-1) src = self.pos_encoder(src) #if self.ngrams!=None: if self.kmer_aggregation: kmer_output = self.k_mer_aggregate(src.permute(1,2,0)) #src = torch.cat([src,kmer_output],dim=0) src = kmer_output attention_weights=[] for layer in self.transformer_encoder: src,attention_weights_layer=layer(src) attention_weights.append(attention_weights_layer) encoder_output = src.permute(1,0,2) #print(encoder_output.shape) output = self.decoder(encoder_output) if self.return_aw: attention_weights=torch.stack(attention_weights).permute(1,0,2,3) return output, attention_weights else: return output train.py import os import torch import torch.nn as nn import time from Functions import * from Dataset import * from Network import * from LrScheduler import * import Metrics from Logger import CSVLogger import argparse from tensorboardX import SummaryWriter from torch.cuda.amp import autocast as autocast from torch.cuda.amp import GradScaler as GradScaler def get_args(): parser = argparse.ArgumentParser() parser.add_argument('--gpu_id', type=str, default='0', help='which gpu to use') parser.add_argument('--path', type=str, default='../', help='path of csv file with DNA sequences and labels') parser.add_argument('--epochs', type=int, default=150, help='number of epochs to train') parser.add_argument('--batch_size', type=int, default=24, help='size of each batch during training') parser.add_argument('--weight_decay', type=float, default=0, help='weight dacay used in optimizer') parser.add_argument('--ntoken', type=int, default=4, help='number of tokens to represent DNA nucleotides (should always be 4)') parser.add_argument('--nclass', type=int, default=2, help='number of classes from the linear decoder') parser.add_argument('--ninp', type=int, default=512, help='ninp for transformer encoder') parser.add_argument('--nhead', type=int, default=8, help='nhead for transformer encoder') parser.add_argument('--nhid', type=int, default=2048, help='nhid for transformer encoder') parser.add_argument('--nlayers', type=int, default=6, help='nlayers for transformer encoder') parser.add_argument('--save_freq', type=int, default=1, help='saving checkpoints per save_freq epochs') parser.add_argument('--dropout', type=float, default=.1, help='transformer dropout') parser.add_argument('--warmup_steps', type=int, default=3200, help='training schedule warmup steps') parser.add_argument('--lr_scale', type=float, default=0.1, help='learning rate scale') parser.add_argument('--kmers', type=int, nargs='+', default=[2,3,4,5,6], help='k-mers to be aggregated') #parser.add_argument('--kmer_aggregation', type=bool, default=True, help='k-mers to be aggregated') parser.add_argument('--kmer_aggregation', dest='kmer_aggregation', action='store_true') parser.add_argument('--no_kmer_aggregation', dest='kmer_aggregation', action='store_false') parser.set_defaults(kmer_aggregation=True) parser.add_argument('--nfolds', type=int, default=5, help='number of cross validation folds') parser.add_argument('--fold', type=int, default=0, help='which fold to train') parser.add_argument('--val_freq', type=int, default=1, help='which fold to train') parser.add_argument('--num_workers', type=int, default=1, help='num_workers') parser.add_argument('--record', type=str, default=' ', help='train or test record') opts = parser.parse_args() return opts tb = SummaryWriter('/root/my_train/tblogdir/H3K27me3') def train_fold(): opts = get_args() seed_everything(2020) #gpu selection os.environ[\"CUDA_VISIBLE_DEVICES\"] = opts.gpu_id device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') data = pd.read_csv('/root/my_train/dataset_all/H3K27me3_train_24w.txt', header=None, sep='\\t') dataset = ViraminerDataset(data.iloc[:220000,0],data.iloc[:220000,1]) dataloader = torch.utils.data.DataLoader(dataset,batch_size=opts.batch_size,shuffle=True,num_workers=opts.num_workers) val_dataset = ViraminerDataset(data.iloc[220000:,0],data.iloc[220000:,1]) val_dataloader = torch.utils.data.DataLoader(val_dataset,batch_size=opts.batch_size,shuffle=False) #exit() #lr=0 #checkpointing checkpoints_folder = '{}checkpoints_fold{}'.format(opts.record, opts.fold) csv_file = '{}log_fold{}.csv'.format(opts.record, opts.fold) columns = ['epoch','train_loss', 'val_loss','val_auc','val_acc','val_sens','val_spec'] logger = CSVLogger(columns,csv_file) #build model and logger model = NucleicTransformer(opts.ntoken, opts.nclass, opts.ninp, opts.nhead, opts.nhid, opts.nlayers, opts.kmer_aggregation, kmers=opts.kmers, dropout=opts.dropout).to(device) optimizer = torch.optim.Adam(model.parameters(), weight_decay=opts.weight_decay) criterion = nn.CrossEntropyLoss(reduction='none') lr_schedule = lr_AIAYN(optimizer,opts.ninp,opts.warmup_steps,opts.lr_scale) softmax = nn.Softmax(dim=1) pytorch_total_params = sum(p.numel() for p in model.parameters()) print('Total number of paramters: {}'.format(pytorch_total_params)) print(\"Starting training for fold {}/{}\".format(opts.fold,opts.nfolds)) #training loop scaler = GradScaler() for epoch in range(opts.epochs): model.train(True) t = time.time() total_loss = 0 total_steps = len(dataloader) for step, data in enumerate(dataloader): #for step in range(1): lr = lr_schedule.step() src = data['data'].to(device) labels = data['labels'].to(device) optimizer.zero_grad() with autocast(): output = model(src) loss = torch.mean(criterion(output,labels)) scaler.scale(loss).backward() scaler.unscale_(optimizer) torch.nn.utils.clip_grad_norm_(model.parameters(),1) scaler.step(optimizer) scaler.update() total_loss += loss print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.3f} Lr:{:.6f} Time: {:.1f}\" .format(epoch+1, opts.epochs, step+1, total_steps, total_loss/(step+1) , lr,time.time()-t),end='\\r',flush=True) #total_loss/(step+1) #break print('') train_loss = total_loss/(step+1) if (epoch+1)%opts.val_freq == 0: val_loss,auc,val_acc,val_sens,val_spec=validate(model,device,val_dataloader,batch_size=opts.batch_size*2) print(\"Epoch {} train loss: {}\".format(epoch+1,train_loss)) tb.add_scalars('train/val/loss', {'train':train_loss, 'val':val_loss}, epoch+1) tb.add_scalar('val_acc', val_acc, epoch+1) tb.add_scalar('auc', auc, epoch+1) tb.add_scalar('val_sens', val_sens, epoch+1) tb.add_scalar('val_spec', val_spec, epoch+1) to_log = [epoch+1,train_loss,val_loss,auc,val_acc,val_sens,val_spec] logger.log(to_log) if (epoch+1)%opts.save_freq == 0: save_weights(model,optimizer,epoch,checkpoints_folder) tb.close() get_best_weights_from_fold(opts.record, opts.fold) train_fold() run.sh #!/bin/bash python train.py --gpu_id 0 --kmer_aggregation --epochs 12 --nlayers 6 \\ --batch_size 64 --kmers 20 --lr_scale 0.1 --ninp 512 --nhid 2048 --num_workers 8 --nhead 8 --record 0504_train1 test.sh #!/bin/bash python evaluate_test.py --gpu_id 0,1 --kmer_aggregation --nmute 20 --epochs 100 --nlayers 6 \\ --batch_size 128 --kmers 13 --lr_scale 0.1 --ninp 512 --nhid 2048 "},"实验流程/模型评估.html":{"url":"实验流程/模型评估.html","title":"模型评估","keywords":"","body":"模型评估 评估指标 参考资料： Python sklearn机器学习各种评价指标——Sklearn.metrics简介及应用示例 Metrics（混淆矩阵） 给定一个二元分类模型和它的阈值，就能从所有样本的（阳性／阴性）真实值和预测值计算出一个 (X=FPR, Y=TPR) 座标点 针对一个二分类问题，将实例分成正类(postive)或者负类(negative)。但是实际中分类时，会出现四种情况. (1)若一个实例是正类并且被预测为正类，即为真正类(True Postive TP) (2)若一个实例是正类，但是被预测成为负类，即为假负类(False Negative FN) (3)若一个实例是负类，但是被预测成为正类，即为假正类(False Postive FP) (4)若一个实例是负类，但是被预测成为负类，即为真负类(True Negative TN) TP:正确的肯定数目 FN:漏报，没有找到正确匹配的数目 FP:误报，没有的匹配不正确 TN:正确拒绝的非匹配数目 列联表如下，1代表正类，0代表负类： 由上表可得出横，纵轴的计算公式： (1)真正类率(True Postive Rate)TPR: TP/(TP+FN),代表分类器预测的正类中实际正实例占所有正实例的比例。Sensitivity (2)负正类率(False Postive Rate)FPR: FP/(FP+TN)，代表分类器预测的正类中实际负实例占所有负实例的比例。1-Specificity (3)真负类率(True Negative Rate)TNR: TN/(FP+TN),代表分类器预测的负类中实际负实例占所有负实例的比例，TNR=1-FPR。Specificity AUC AUC（Area Under Curve）被定义为ROC曲线)下与坐标轴围成的面积，取值范围[0,1],分别随机从正负样本集中抽取一个正样本，一个负样本，正样本的预测值大于负样本的概率。 Roc (Receiver operating characteristic) 曲线是一种二元分类模型分类效果的分析工具，每个点反应这对同一信号刺激的感受性 纵轴TPR（true positive rate）真正例率: 灵敏度（Sensitivity）在所有实际为阳性的样本中，被正确地判断为阳性之比率 TPR = TP/P = TP/(TP+FN) 纵轴FPR（false positive rate）: 在所有实际为阴性的样本中，被错误地判定为阳性之比率 FPR = FP/N = FP/(FP+TN) auPRC PRC(Precision Recall Curve,准确召回率曲线)，相关性评价： 数据库里有500条记录，其中50个是相关的（正样本），你通过一个检索，返回了75个你认为相关，其中只有45个是真正相关的；那么在这个检索对应下的： recall=45/50=0.9【横坐标】 precision=45/75=0.6【纵坐标】（这两个数比值都是在0到1置内的） 结论： 在negative instances的数量远远大于positive instances的data set里， PRC更能有效衡量检测器的好坏。 MCC 马修斯相关系数（Matthews correlation coefficient） 马修斯相关系数是在使用机器学习作为二进制（2类）的质量的度量的分类，通过布赖恩W.马修斯在1975年由生物化学引入 它考虑到真和假阳性和假阴性，并且通常是被视为一种平衡的措施，即使这些类别的规模大小不同也可以使用。 MC实质上是观察到的类别和预测的二元分类之间的相关系数; 它返回介于-1和+1之间的值。系数+1表示完美预测，0表示不比随机预测好，-1表示预测和观察之间的完全不一致。统计数据也称为phi系数。MCC与2×2 列联表的卡方统计量相关 其中n是观察总数。虽然没有完美的方法用一个数字来描述真假阳性和阴性的混淆矩阵，但马修斯相关系数通常被认为是最好的这种测量之一。 当两个类别具有非常不同的大小时，其它度量（例如正确预测的比例（也称为准确性））无用。例如，将每个对象分配给较大的集合可以实现高比例的正确预测，但通常不是有用的分类。可以使用以下公式直接从混淆矩阵计算MCC ： 在这个公式中，TP是真阳性数量，TN的真阴性数量，FP的假阳性数量和FN的假阴性数量。如果分母中的四个和中的任何一个为零，则分母可以任意设置为1; 这导致Matthews相关系数为零，这可以显示为正确的限制值。 马修斯给出的原始公式是： 这等于上面给出的公式。 作为相关系数， 马修斯相关系数是问题及其对偶的回归系数的几何平均数。 Matthews相关系数的分量回归系数是Markedness（Δp）和Youden的J统计量（Informedness或Δp’）。 标记和知情对应于不同的信息流方向，并推广了Youden的J统计量， {\\ displaystyle \\ delta}p统计和（作为它们的几何平均值）马修斯相关系数超过两个类。 一些科学家声称，马修斯相关系数是在混淆矩阵环境中建立二元分类器预测质量的最具信息性的单一分数 过拟合问题 学习曲线 偏差/方差 高偏差—欠拟合 高方差—过拟合 Train set error /Dev set error 参考资料： http://www.ai-start.com/dl2017/ 吴恩达深度学习 https://blog.csdn.net/huangfei711/article/details/79436698 获取和使用更多的数据集 对于解决过拟合的办法就是给与足够多的数据集，让模型在更可能多的数据上进行“观察”和拟合，从而不断修正自己。然而事实上，收集无限多的数据集几乎是不 可能的，因此一个常用的办法就是调整已有的数据，添加大量的“噪音”，或者对图像进行锐化、旋转、明暗度调整等优化。 采用合适的模型 目前来说，针对不同的情况和分类要求，对使用的模型也是千差万别。过于复杂的模型会带来过拟合问题。对于模型的设计，目前公认的一个深度学习规律“deeper is better”。国内外各种大牛通过实验和竞赛发现，对于CNN来说，层数越 多效果越好，但是也更容易产生过拟合，并且计算所耗费的时间也越长。因此对于模型的设计需要合理参考各种模型的取舍。 ​ 使用 Dropout Dropout 是一个非常有用和常用的方法。Dropout 指的是在训练过程中每次按一定的几率关闭或忽略某些层的节点。使得模型在使用同样的数据进行训练时相当于从不同的模型中随机选择一个进行训练至于 Dropout 起作用的原因，可以简单理解成在训练过程中会产生不同的训练模型，不同的训练模型也会产生不同的的计算结果， 随着训练的不断进行，计算结果会在一个范围内波动，但是均值却不会有很大变化，因此可以把最终的训练结果看作是不同模型的平均输出。 正则化 正则化又称为权重衰减，具体做法是将权值的大小加入到损失函数中，在实际使用中分为 L1 正则与 L2 正则。关于正则化能够防止过拟合的原因 正则化 使神经网络在保持原有深度的同时，每一层的隐藏单元产生的影响更小，防止过拟合 https://blog.csdn.net/guyuealian/article/details/88426648 L2正则化： ||w||是欧几里得范数，平方等于所有w的平方和 神经网络中的L2正则化：平方范数被定义为所有元素的平方和 弗罗贝尼乌斯范数：矩阵范数， L2范数正则化，也被称为权重衰减，因为在进行backprop时，增加权重项会使原本的梯度下降更多 直观上理解就是如果正则化设置得足够大，权重矩阵被设置为接近于0的值，直观理解就是把多隐藏单元的权重设为0，于是基本上消除了这些隐藏单元的许多影响。如果是这种情况，这个被大大简化了的神经网络会变成一个很小的网络，小到如同一个逻辑回归单元，可是深度却很大，它会使这个网络从过度拟合的状态更接近左图的高偏差状态。 Dropout正则化 （随机失活） dropout会遍历网络的每一层，并设置消除神经网络中节点的概率。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用backprop方法进行训练。 不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，dropout将产生收缩权重的平方范数的效果，和之前讲的L2正则化类似；实施dropout的结果实它会压缩权重，并完成一些预防过拟合的外层正则化；对不同权重的衰减是不同的，它取决于激活函数倍增的大小。 inverted dropout反向随机失活 可以改变Dropout层的参数Dropout（keep—prob） Early Stopping Early Stopping 是参数微调中的一种，即在每个循环结束一次以后（这里的循环可能是 full data batch,也可能是 mini batch size），计算模型的准确率（accuracy）。当准确率不再增加时就停止训练。这是一种非常简单和自然的办法，准确率不再增加时就停止训练，防止模型对已有的数据继续训练。但是问题在于，准确率在每个循环之后的计算是变化的，没有任何人和任何模型能保证准确率不会变化，可能某次循环结束后，准确率很高，但是下一轮结束后准确率又降得很低 ​ 人为地设定一个范围。当连续10次准确率在此范围内波动时就停止循环。 可变化的学习率 可变化的学习率也是根据模型计算出的准确率进行调整。一个简单的方法是在人为设定的准确率范围内，达到10次范围内的波动后，依次将学习率减半，直到最终的学习率降为原始的 1/1024 时停止模型的训练。 使用 Batch_Normalization 还有一个数据处理的方法 Batch_Normalization，即数据在经过卷积层之后，真正进入激活函数之前需要对其进行一次 Batch_Normalization，分批对输入的数据求取均值和方差之后重新对数据进行归一化计算。这样做的好处就是对数据进行一定程度的预处理，使得无论是训练集还是测试集都在一定范围内进行分布和波动，对数据点中包含的误差进行掩盖化处理，从而增大模型的泛化能力。 超参数调试 贝叶斯优化（BayesianOptimization） https://www.cnblogs.com/yangruiGB2312/p/9374377.html optuna https://tigeraus.gitee.io/doc-optuna-chinese-build/ ROC曲线绘制 ROC曲线和AUC https://blog.csdn.net/pipisorry/article/details/51788927 torch.save(net.state_dict(), 'params.pkl') test_data = TensorDataset(data_tensor[150000:151000], target_tensor[150000:151000]) test_dataloader = data.DataLoader(test_data,batch_size = 1000, shuffle=True,num_workers=0) y_label = [] y_pre = [] def predict(data_iter, net, device=None): net.load_state_dict(torch.load('params.pkl')) if device is None and isinstance(net, torch.nn.Module): device = list(net.parameters())[0].device acc_sum, n = 0.0, 0 with torch.no_grad(): for X, y in data_iter: if isinstance(net, torch.nn.Module): net.eval() # 评估模式, 这会关闭dropout acc_sum += (net(X.to(device)).argmax(dim=1) == (y.to(device).squeeze())).float().sum().cpu().item() y_label = y.cpu().numpy().tolist() y_pre = net(X.to(device))[:,1].cpu().numpy().tolist() fpr, tpr, thersholds = roc_curve(y_label, y_pre, pos_label=1) for i, value in enumerate(thersholds): print(\"%f %f %f\" % (fpr[i], tpr[i], value)) roc_auc = auc(fpr, tpr) plt.plot(fpr, tpr, 'k--', label='ROC (area = {0:.2f})'.format(roc_auc), lw=2) plt.xlim([-0.05, 1.05]) # 设置x、y轴的上下限，以免和边缘重合，更好的观察图像的整体 plt.ylim([-0.05, 1.05]) plt.xlabel('False Positive Rate') plt.ylabel('True Positive Rate') # 可以使用中文，但需要导入一些库即字体 plt.title('ROC Curve') plt.legend(loc=\"lower right\") plt.show() predict(test_dataloader, net, device=None) 改良标签方法 计算每种组蛋白修饰peak的中位数，平均数 histone modifications mid average H3K4me3（1） 880 950.52 H3K27ac（2） 799 854.329 H3K4me1（3） 2694 3446.12 H3K27me3（4） 1586 3149.9 H3K9me2（5） 2158 4094.97 histone modifications function H3K4me3（1） 取peak中央250bp作为标准，win在其中的比例占到10%则标记为阳性 H3K27ac（2） 250 H3K4me1（3） 750 H3K27me3（4） 750 H3K9me2（5） 1000 将H3K4me3和H3K27ac按照同一种标签计算 将H3K4me1和H3K9me2按照同一种标签计算 "},"实验流程/特征提取.html":{"url":"实验流程/特征提取.html","title":"特征提取","keywords":"","body":"特征提取 目前主要使用的方法是方法七 方法合集 参考文献： Deep Learning for Genomics: A Concise Overview 3.1Model Interpretation 方法一： Zeiler and Fergus (2014) gave insights into the function of intermediate features by mapping hidden layers back to input through deconvolution Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pages 818–833. Springer, 2014. 方法二： Simonyan et al. (2013) linearly approximate the network by first-order Taylor expansion and obtained Saliency Maps from a ConvNet by projecting back from the dense layers of the network. People also searched for an understanding of genes by deep networks Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutionalnetworks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034, 2013. 方法三： Denas and Taylor (2013) managed to pass the model knowledge back into the input space through the inverse of the activa- tion function, so that biologically-meaningful patterns can be highlighted. Olgert Denas and James Taylor. Deep modeling of gene expression regulation in an ery-thropoiesis model. In Representation Learning, ICML Workshop, 2013. 方法四：saliency map（显著性图) 一阶泰勒展开和梯度下降的推导 https://mathpretty.com/10683.html Lanchantin et al. (2016b, Dashboard) adopted Saliency Maps to measure nucleotide importance. Their work provided a series of visualization techniques to detect motifs, or sequence patterns from deep learning models, and went further to discuss about the features extracted by CNNs and RNNs Jack Lanchantin, Ritambhara Singh, Beilun Wang, and Yanjun Qi. Deep gdashboard: Visualizing and understanding genomic sequences using deep neural networks. CoRR, abs/1608.03644, 2016b. URL http://arxiv.org/abs/1608.03644. which which parts of the sequence are most influential for the classification? sequence：X0 length：|X0| class : c score: Sc(X0) 衡量碱基重要性的指标，根据碱基变化对Sc的影响来表示重要性 w是神经网络反向传播的一步 This derivative is simply one step of backpropagation in the DNN model, and is therefore easy to compute. We do a pointwise multiplication of the saliency map with the one-hot encoded sequence to get the derivative values for the actual nucleotide characters of the sequence (A,T,C, or G) so we can see the influence of the character at each position on the output score. Finally, we take the element-wise magnitude of the resulting derivative vector to visualize how important each character is regardless of derivative direction. Saliency.py import torch import matplotlib.pyplot as plt import numpy as np import pandas as pd import re from sklearn.preprocessing import LabelEncoder,OneHotEncoder import torchvision from model_dna import NetDeepHistone model= NetDeepHistone() model.load_state_dict(torch.load('D:/桌面/model.txt',map_location=torch.device('cpu'))) #停止梯度更新 for param in model.parameters(): param.requires_grad = False #读入X，y X = pd.read_csv('D:/桌面/data_motif.txt',header = None) Y = pd.read_csv('D:/桌面/target_motif.txt',header = None) #转成onehot # function to convert a DNA sequence string to a numpy array # converts to lower case, changes any non 'acgt' characters to 'n' def string_to_array(my_string): my_string = my_string.lower() my_string = re.sub('[^acgt]', 'z', my_string) my_array = np.array(list(my_string)) return my_array label_encoder = LabelEncoder() label_encoder.fit(np.array(['a','c','g','t','z'])) def one_hot_encoder(my_array): integer_encoded = label_encoder.transform(my_array) onehot_encoder = OneHotEncoder(sparse=False, dtype=int) integer_encoded = integer_encoded.reshape(len(integer_encoded), 1) onehot_encoded = onehot_encoder.fit_transform(integer_encoded) return onehot_encoded one_hot_matrix = [] for i in range(2): one_hot_matrix.append(one_hot_encoder(string_to_array(X[0][i]))) print(one_hot_matrix) X_tensor = torch.Tensor(one_hot_matrix) X_tensor = X_tensor.reshape(len(X),4,500) print(X_tensor.shape) #开启测试模式 model.eval() #确定梯度 X_tensor.requires_grad_() y_tensor = torch.LongTensor([1,1]) print(y_tensor.shape) saliency = None logits = model.forward(X_tensor) print(logits) logits = logits.gather(1, y_tensor.view(-1, 1)).squeeze() logits.backward(torch.FloatTensor([1., 1.])) saliency = abs(X_tensor.grad.data) print(saliency[0]) PWM = pd.DataFrame(saliency[0].numpy()) print(PWM) PWM.to_csv('C:/Users/DELL/Documents/PWM1.txt',sep='\\t',index=False) ggseqlogo > library(ggseqlogo) > matrix row.names(matrix) matrix motif ggseqlogo(motif) 方法五：Mutation Map Alipanahi et al. (2015) visualized the sequence specificities deter- mined by DeepBind through mutation maps that indicate the effect of variations on bound sequences. Babak Alipanahi, Andrew Delong, Matthew T. Weirauch, and Brendan J. Frey. Predicting the sequence specificities of dna- and rna-binding proteins by deep learning. Nat Biotech, 33(8):831–838, Aug 2015. ISSN 1087-0156. URL http://dx.doi.org/10.1038/nbt.3300. Computational Biology Mutation map 1.通过给定序列碱基的高度来表示在DeepBind分析中的重要性 2.heat map （4 * n）（n是序列长度）表示每个可能出现的突变对结合能力的影响 绘制方法 通过计算原始序列（参考基因组）的binding score p(s) 然后依次让每个位点都发生突变生成新的序列，计算p(s)^ ΔSij = （p(s)^ - p(s)) * max(0,p(s),p(s)^) 方法六：Deepmotif https://github.com/bakirillov/deepmotif4pytorch Method for motif generation via class optimization. We find the input matrix which corresponds to the highest locally optimum TFBS probability via backpropagation, and generate a PWM from the matrix. 方法七： 1.将预训练好的模型调整为预测模式，停止梯度更新 2.将预测集投入到模型当中进行计算，然后输出Conv1(第一层卷积层)对应每个Kernel的位置权重矩阵（PWMs） 3.然后将每个样本的矩阵叠加，设置阈值进行判断将保守性矩阵输出 4.将PWMs矩阵的形式转换为meme的形式 5.使用TomTom工具进入JASPAR数据库进行相似性比对 参考文献： DeepHistone: a deep learning approach to predicting histone modifications TSPTFBS: a Docker image for trans-species prediction of transcription factor binding sites in plants github：https://github.com/liulifenyf/TSPTFBS get_PWM.py import numpy as np import pandas as pd import re import torch from model_dna import NetDeepHistone def NUMPY2STRING(input_array): # convert numpy to string for 2 dimension numpy array. output_str = \"\" for i in range(input_array.shape[0]): for j in range(input_array.shape[1]): output_str = output_str + str(input_array[i, j]) + \"\\t\" output_str += \"\\n\" return output_str def matrix2meme(pwm_txt_name, pwm_meme_name, pwm_len): # convert PWM to meme format used in tomtom. write_ofl = open(pwm_meme_name, \"w\") ##### write_ofl.write(\"MEME version 5.0.4\\n\\n\") write_ofl.write(\"ALPHABET= ACGT\\n\\n\") write_ofl.write(\"strands: + -\\n\\n\") write_ofl.write(\"Background letter frequencies\\n\") write_ofl.write(\"A 0.25 C 0.25 G 0.25 T 0.25\\n\\n\") read_ofl = open(pwm_txt_name) ##### oflst = read_ofl.readlines() read_ofl.close() count = 0 for line in oflst: line = line.strip() if re.search(\">\", line): write_ofl.write(\"\\n\") write_ofl.write(\"MOTIF\" + \"\\t\" + \"filter\" + str(count + 1) + \"\\n\") write_ofl.write(\"letter-probability matrix: alength= 4 w= \" + str(pwm_len) + \"\\n\") ####### count += 1 else: write_ofl.write(line + \"\\n\") write_ofl.close() # 用于存储pwm矩阵的文件 input_path = \"onehot_test.npy\" # DNA one-hot encoding with .npy foramt. model_path = \"D:/桌面/model.txt\" # trained model path X = np.load(input_path) X = torch.Tensor(X).reshape(len(X),4,500) pwm_meme_name = \"pwm.meme\" pwm_txt_name = \"pwm.txt\" model = NetDeepHistone() model.load_state_dict(torch.load(model_path,map_location=torch.device('cpu'))) # check out the name of first conv_layer parm = {} for name,parameters in model.seq_map.named_parameters(): print(name,':',parameters.size()) parm[name]=parameters.detach().numpy() '''According to your model, return the output of first conv_layer. please change the \"outputs\" para according to your model construction. And the bias and weights of first conv_layer.''' WEIGHTS, BIAS = parm['conv1.0.weight'],parm['conv1.0.bias'] WEIGHTS = WEIGHTS.squeeze() INSTANCE_LENGTH = WEIGHTS.shape[2] # return the length of filter print(WEIGHTS.shape) conv_out = model.seq_map.get_conv1(X) conv_out = conv_out.squeeze() print(conv_out.shape) ''' layer_output = Model(inputs=model.input, outputs=model.layers[1].output) conv_out = layer_output.predict(X) conv_out = conv_out.squeeze()# 根据模型卷积核数目调整最后一位 print(conv_out.shape) ''' # 用于提取pwm矩阵 ''' WEIGHTS(128,4,9)(Kernels_num,(width,length)) BIAS(128) THRESHOLD INSTANCE_FILTERED_NUMBER INSTANCE_FILTERED INSTANCE_LENGTH ''' motif_ofl = open(pwm_txt_name, \"w\") for i in range(WEIGHTS.shape[0]): one_filter_weight = WEIGHTS[i,: ,: ] THRESHOLD = (np.sum(np.max(one_filter_weight, 1))+ BIAS[i]) * 0.9 #阈值可以调整0.5-1.0 model_c = conv_out[:,i,:] - BIAS[i] position_m = np.where(model_c >= THRESHOLD) print(position_m[1].shape[0]) #(samples,position) INSTANCE_FILTERED_NUMBER = position_m[1].shape[0] print(INSTANCE_FILTERED_NUMBER) INSTANCE_FILTERED = np.zeros( [INSTANCE_FILTERED_NUMBER, 4, INSTANCE_LENGTH]) for j in range(INSTANCE_FILTERED_NUMBER): if position_m[1][j] MOTIF\" + str(i + 1) + \"\\n\" motif_ofl.write(outline) outline = NUMPY2STRING(pwm_matrix.T) motif_ofl.write(outline) motif_ofl.flush() motif_ofl.close() print(\"PWM-txt Done\") matrix2meme(pwm_txt_name, pwm_meme_name, INSTANCE_LENGTH) print(\"PWM-meme Done\") 结果展示 方法七 "},"背景知识/":{"url":"背景知识/","title":"背景知识","keywords":"","body":" 背景知识 本项目为交叉学科，主要应用了人工智能深度学习的方法来解决表观遗传学问题 利用类模式分析方法来解决生物问题，依靠深度学习的先进算法和计算能力处理海量的生物学数据，并且对深度学习的可解释性在基因组学中的应用进行进一步的探索，对深度学习中的经典概念：迁移学习、多任务学习和多视图学习都进行了实践。 "},"背景知识/生物学/":{"url":"背景知识/生物学/","title":"生物学","keywords":"","body":"生物学 "},"背景知识/生物学/数据来源.html":{"url":"背景知识/生物学/数据来源.html","title":"数据来源","keywords":"","body":"数据来源 Integrative analysis of reference epigenomesin 20 rice varieties 文章全面系统地描绘了20个水稻品种的表观基因组图谱，注释了覆盖水稻基因组82%的功能性DNA元件 应用技术：enhanced chromatin immunoprecipitation (eChIP) 和植物常用的ChIP-seq方法相比，eChIP-Seq的总体效率提升至少几十倍以上，可用于鉴定水稻、拟南芥、玉米和甘蓝型油菜等植物少量样品的组蛋白翻译后修饰和转录因子全基因组结合位点。 该研究产生了多达500多套组学数据，覆盖20个有代表性的水稻品种及其多个组织，包括 58个基因表达数据(RNA-Seq) 32个全基因组DNA甲基化图谱(BS-Seq) 354个各种组蛋白修饰数据(eChIP-Seq) 58个全基因组开放染色质区域图谱(FAIRE-Seq) ​ 基于这些数据的整合性分析，作者从水稻表观基因组图谱定义了15种染色质状态。另外，通过ChIP-reChIP实验，鉴定了一种新的H3K9me2/H3K4me1二价染色质状态。该染色质状态普遍存在于基因内含子上，相关基因的表达量较低而且富集Copia转座子，但其潜在的生物学功能有待深入研究。 ​ 研究者定义了活跃启动子的染色质特征和区域：即开放染色质区域与H3K4me3修饰区域，大约为转录起始位点上游500 bp到下游1 kb，为启动子的核心功能区域。结合水稻高分辨三维基因组图谱分析，研究者发现水稻中存在大量的具有增强子活性的启动子（enhancer-like promoters），揭示了启动子不仅调控相邻基因的表达，还可以作为增强子，通过染色质远程相互作用，调控远端与其互作基因的表达。 ​ 通过对20个水稻品种间的表观基因组图谱比较分析，发现异染色质区域容易发生表观修饰变异。同时，该研究注释了籼稻和粳稻之间重要的表观差异区域，这些结果为研究水稻品种分化和环境适应性的提供了独特的视角和重要的数据。 ​ 该研究全面概述了水稻的表观基因组图谱，启动子染色质特征，enhancer-like promoters和不同组织中的表观基因组动态变化模式以及遗传变异对这些水稻表观基因组图谱的影响。该研究这些数据为全面解析水稻基因组提供重要的资源。 Minghui 63 (MH63), Zhensha97 (ZS97), Nipponbare (Nip) and 17 other varieties Mapping epigenomic marks in 20 rice varieties H3K4me3, H3K27ac, H3K4me1, H3K27me3， H3K9me2 young leaf, mature leaf, root, and panicle genome-wide DNA methylation,open chromatin regions, RNA polymerase II (RNAPII) binding sites, and the transcriptome for these tissues and varieties. Breeding signatures of rice improvement revealed by a genomic variation map from a large germplasm collection 从一个大型种质资源的基因组变异图谱揭示水稻改良的育种特征 ​ 这项研究分析了来自73个国家1,479个水稻品种，包括一些地方品种和现代栽培种的低覆盖测序数据。研究人员在籼稻亚种中鉴别出了两大主要的亚群indica I (IndI)和indica II (IndII)，其对应独立育种工作生成的两个杂种优势类群。研究人员检测到了跨越7.8%水稻基因组，在IndI和IndII之间受到不同选择的200个区域，将它们称之为育种印迹。这些区域包括GWAS研究揭示的、与重要农艺性状相关的许多已知功能基因及基因位点。粮食产量与品种中育种印迹的数量呈正比，表明了一个品系中的育种印迹数量或许可用于预测水稻的农艺潜力，选出的基因位点或可为科学家们提供一些水稻改良的靶点。 Neighbor Joining tree NJ法要求输入的数据必须是待聚类数据(taxa)之间的距离信息,matching distance of 188,637 evenly distributed and randomly selected SNPs. NJ法是一种bottom-up的聚类，故首先要计算出进化距离最近的两个物种，将其聚为一类，再计算出距离该新类最近的一个物种再次聚为一个类，如此迭代，遍历所有输入的物种，构建系统发育树。 https://www.jianshu.com/p/6fc635972c11 XP-CLR (cross-population likelihood method) "},"背景知识/生物学/表观遗传.html":{"url":"背景知识/生物学/表观遗传.html","title":"表观遗传基础","keywords":"","body":"表观遗传学 DNA甲基化 基本概念 广义：DNA序列上特定的碱基在DNA甲基转移酶（DNA methyltransferase，DNMT）的催化作用下，以S—腺苷甲硫氨酸（S—adenosyl methionine，SAM）作为甲基供体，通过共价键结合的方式获得一个甲基基团的化学修饰过程。（这种甲基化修饰可以发生在胞嘧啶的C5，腺嘌呤的N6和鸟嘌呤的N7位置上） 狭义（研究中特指）：主要是指发生在CpG二核苷酸（CG）中胞嘧啶上第5位碳原子的甲基化过程，其产物称为5—甲基胞嘧啶（5—mC），是植物、动物等真核生物DNA甲基化的主要形式，也是发现的哺乳动物DNA甲基化的唯一形式 DNA甲基化的分布 在植物和其他生物中，可以在三种不同的序列中发现DNA甲基化： CpG（CG） CHG CHH 其中H对应于A，T或C。 CpG islands CpG是胞嘧啶（C，Cytosine），磷酸（p，phosphoric acid），鸟嘌呤（G，Guanine ）的缩写，也可以去掉磷酸直接叫CG。在哺乳动物中，在基因组中富含GC和CpG的序列区段，叫CpG岛（CpG islands）。 CpG岛的定义 通常定义为以下区域： 长度大于200bp G + C含量大于50％， 观察到的CpG与预期CpG的比率大于0.6，有时也使用其他定义。 CpG岛的分布 除重复序列外，人类基因组中约有25,000个CpG岛，其中75％的岛长小于850bp。 大约50％的CpG岛位于基因启动子区域，而另外25％的岛位于基因内，通常充当替代启动子。 在人中，大约60-70％的基因在其启动子区域中具有CpG岛。大多数CpG岛在结构上未甲基化，并富集一些染色质修饰，例如H3K4甲基化。在体细胞组织中，只有10％的CpG岛被甲基化，其中大部分位于基因间和基因内区域。 CpG islands与转录 在人基因组中，90%以上的CpG位点是被甲基化的，但是CpG岛甲基化程度通常很低，这种情况下，不影响蛋白结合到DNA的启动子区域来启动转录，进而使基因表达。 如果这个DNA的CpG岛被甲基化后，蛋白质不能结合到DNA上，导致转录沉默，基因不表达 下图为常用的基因组甲基化表示形式，每个“棒棒糖”代表一个甲基化位点，位点集中区域就可能是CpG岛，如果在转录因子结合的地方恰好被甲基化（黑色棒棒糖），那么这个基因就废了，不会继续表达了。 DNA methylation in rice 与动物的区别： 动物基因组的甲基化C绝大多数发生在CG 植物基因组可以发生在CG，CHG和CHH上（Non-CG的甲基化主要发生在TEs上，CG主要发生在TEs和genebody上） The DNA methylation landscape： 水稻基因组的胞嘧啶甲基化水平是拟南芥的四倍以上，可能与基因组结构有关 水稻基因组一半以上的编码蛋白质的基因都发生了甲基化（包括genebody和启动子区域），甲基化更易发生在TSS的下游，与转录抑制有关，与拟南芥不同，拟南芥的CG甲基化只发生在genebody上 甲基化DNA在染色体上的分布类似于异染色质在水稻染色体的分布（全基因组范围内的胞嘧啶甲基化mapping，使用methyl CpG immunoprecipitation squencing（甲基 CpG免疫沉淀）） 参与环境反应的基因往往受到甲基化的调控 干旱响应基因显示出显著性地富集DNA甲基化 两系育种系统中利用光敏核雄性不育（PGMS）或温敏核雄性不育系（TGMS）Peiai 64S表现出出显著的全基因组水平上的超甲基化，表明在可变的环境条件下存在的潜在的非孟德尔的基因表达 对发育中的水稻种子DNA甲基化的整体分析表明，非TE基因具有不同的甲基化状态，在水稻种子发育过程中存在复杂的受控的甲基化模式 BS-seq（bisulfite sequencing）：全基因组范围内的 DNA methylome的绘制 24.3% 的胞嘧啶被甲基化（栽培的，幼嫩的花序），是拟南芥的四倍 不同种类胞嘧啶甲基化的占比：CG：44.5%，CHG：20.1%， CHH：4.0%，与水稻叶片的测定结果相同 CG甲基化在编码蛋白的genebody部分的水平非常高，CG和non-CG甲基化在TEs，重复序列，着丝粒周围区域（pericentromeric regions）的出现水平都很高 适度表达的基因更容易被甲基化，表达量过高和过低的基因都不可能被甲基化 水稻杂种和再生植株表现出与其亲本和未再生植株明显不同但可以遗传的甲基化模式，水稻的DNA甲基化在组织培养和再生期间可以被很容易的获得和丢失，这对于建立特异性基因表达模式至关重要 甲基化的功能 基因调控：启动子区域的甲基化会使基因表达沉默 发育调控：细胞分裂时可遗传，建立细胞与组织分化差异 DNA复制起始及错误修正定位 改变DNA区域构象变化，影响了蛋白质与DNA的相互作用，抑制了转录因子与启动区DNA的结合效率 高度甲基化： X染色体失活：持续失活导致女性的一条 X 染色体， 染色体印迹：指基因表达活性只局限于来自双亲之一的基因版本 抑制转座子（TEs）：在CpG密集区域，DNA甲基化是一种强力的转录阻遏物。尽管DNA甲基化不具有微调基因调控所需的灵活性，但其稳定性非常适合确保转座因子的永久沉默。 这种甲基化在几乎所有组织中都会使部分基因永久保持沉默。 甲基化可以被环境影响 调控多倍体的形成 调控基因组稳定性和基因的演化 启动子区域甲基化 在几乎所有被分析的生物中，启动子区域的甲基化与基因表达呈负相关。转录活性基因的CpG密集启动子从未被甲基化，但是，转录沉默基因并不一定带有甲基化的启动子。 在小鼠和人类中，大约60％至70％的基因在其启动子区域中都有一个CpG岛，并且在分化和未分化的细胞类型中，大多数这些CpG岛都保持未甲基化状态，而与基因的转录活性无关。 值得注意的是，尽管CpG岛的DNA甲基化与转录抑制作用明确相关，但对CG缺乏的启动子中DNA甲基化的功能仍不清楚。 DNA甲基化可能以两种方式影响基因的转录 首先，DNA本身的甲基化可能在物理上阻碍转录因子与基因的结合 第二，甲基化的DNA可能被称为甲基CpG结合域（methyl-CpG-binding domain，MBD）的蛋白结合。MBD蛋白将其他蛋白募集到位点，例如组蛋白脱乙酰基酶和其他可以修饰组蛋白的染色质重塑因子，从而形成致密的，无活性的异染色质。 基因内甲基化 在几乎所有存在DNA甲基化的物种中，DNA甲基化在高度转录的基因内特别丰富，也就是说在基因内的DNA甲基化与基因表达正相关。 基因内甲基化的功能尚不清楚。大量证据表明： 它可以调节剪接并抑制基因内转录单位（密码子，启动子或转座因子）的活性。 基因内甲基化似乎与H3K36甲基化紧密相关。在酵母和哺乳动物中，H3K36甲基化在高度转录的基因体内高度富集。 在哺乳动物中，DNMT3a和DNMT3b PWWP结构域与H3K36me3结合，并且这两种酶被募集到活跃转录的基因体内。 胚胎发育过程中的甲基化 在胚胎发育过程中，DNA甲基化先被大量擦除，然后在哺乳动物的各代之间重建。 在配子发生和早期胚胎发育过程中，几乎擦除了来自亲本的所有甲基化，每次都发生去甲基和再甲基化。 早期胚胎发育的去甲基化发生在植入前：最初在合子中，然后在桑椹胚和囊胚。 在胚胎植入阶段发生了甲基化波，保护了CpG岛免受甲基化。这导致整体抑制，并使管家基因在所有细胞中表达。 在植入后阶段，甲基化模式是特定于阶段和组织的，其变化将定义每种单独的细胞类型，可长期稳定持续。 尽管DNA甲基化本身对于转录沉默而言并不是必需的，但是，它代表了一种“锁定”状态，可以使转录失活。 尤其在基因组印迹和X染色体失活的情况下，DNA甲基化对于维持单等位基因沉默显得至关重要。在这些情况下，表达的等位基因和沉默的等位基因的甲基化状态不同，DNA甲基化的丧失导致Xist在体细胞中的印迹和再表达的丧失。 由于存在基因组印迹现象，因此母本和父本基因组具有差异性标记，并且每次通过生殖系时都必须正确地重新编程。因此，在配子发生过程中，原始生殖细胞必须根据传代母体的性别擦除并重新建立其原始的双亲DNA甲基化模式。受精后，将父本和母本基因组再次去甲基化并重新甲基化（与印迹基因相关的差异甲基化区域除外）。重编程可能是新形成胚胎的全能性和擦除获得的表观遗传变化所必需的。 BS-seq——检测DNA甲基化 亚硫酸氢盐测序（Bisulfite sequencing，BS-seq ，methseq），先使用亚硫酸氢盐处理DNA，然后上机测序来确定甲基化模式。 如果听到 WGBS（Whole-genome bisulfite sequencing） ，其实也是BS-seq。 原理 用亚硫酸氢盐处理DNA可将胞嘧啶残基（C）转化为尿嘧啶（U），但5-甲基胞嘧啶残基（5mC）对其有抗性，并不会发生转变。 因此，用亚硫酸氢盐处理过的DNA仅保留甲基化的胞嘧啶。 蓝色的核苷酸是被亚硫酸氢盐转化为尿嘧啶（U）的未甲基化的胞嘧啶（C） 红色的核苷酸是对转化具有抗性的5-甲基胞嘧啶（5mC） 总的来说，样本用 Bisulfite 处理，将基因组中未发生甲基化的 C 碱基转换成 U，进行PCR扩增后变成T，与原本具有甲基化修饰的 C 碱基区分开来，再结合高通量测序技术，与参考序列比对。 未甲基化的 C -> T 甲基化的 C -> C 与常规转录组相比存在的问题： 未甲基化的 C 会转变为 T，这种转变会让基因组内的C少，ATG多，但是在生物中是不存在这种情况的 在比对时，还使用的是一般的参考基因组，所有这些转变的 reads 不能匹配到参考基因组相应的位点 BS-seq 在 Bisulfite 处理和 PCR 后会产生四条不同的链，转录组是两条正负链 组蛋白修饰 组蛋白结构 组蛋白（histone）是真核生物体细胞染色质与原核细胞中的碱性蛋白质，和DNA共同组成核小体结构。它们是染色质的主要蛋白质组分，作为DNA缠绕的线轴，并在基因调控中发挥作用，但是原核细胞组蛋白对基因调控的作用非常微弱。没有组蛋白，染色体中未缠绕的DNA将非常长（人类DNA中的长宽比超过1000万比1）。 组蛋白在染色质中被DNA紧密缠绕，如同念珠一般 核小体 = 组蛋白 + DNA(147bp) 组蛋白八聚体 = 2个H2B + 2个H2A + 2个H3 + 2个H4 每种组蛋白结构都会伸出一个“尾巴”(tail)，是蛋白质的N端，组蛋白修饰就是在tail上进行 组蛋白修饰的描述规则 以共价方式进行的蛋白质翻译后修饰（PTM），包括：甲基化（M)，磷酸化（P），乙酰化（A） 描述规则：组蛋白结构 + 氨基酸名称 + 氨基酸位置 + 修饰类型 H3K4me3：代表H3组蛋白的第4位赖氨酸的三甲基化 H3K14ac：代表H3组蛋白的第14位赖氨酸的乙酰化 组蛋白修饰类型 组蛋白甲基化 甲基化取决于其位置和状态，与抑制或激活有关。 组蛋白甲基化的位点是赖氨酸(K)和精氨酸(R)。 赖氨酸可以分别被一、二、三甲基化，精氨酸只能被一、二甲基化。 研究表明，组蛋白精氨酸甲基化是一种相对动态的标记，精氨酸甲基化与基因激活相关。 相反，赖氨酸甲基化似乎是基因表达调控中一种较为稳定的标记。 例如， H3K4 的甲基化与基因激活相关 H3K9，H3K27单甲基化与基因激活有关，三甲基化与基因沉默相关 H3K9，H3K27甲基化会介导异染色质的形成 组蛋白乙酰化 组蛋白甲基化和乙酰化主要发生在它们的N-末端尾部并且可以影响基因的转录。 组蛋白乙酰化主要与基因激活有关，组蛋白乙酰化主要发生在H3、H4的N端比较保守的赖氨酸位置上，是由组蛋白乙酰转移酶和组蛋白去乙酰化酶协调进行。 特定基因区域的组蛋白乙酰化和去乙酰化是以一种非随机的、位置特异的方式进行。 乙酰化可能通过对组蛋白电荷以及相互作用蛋白的影响，来调节基因转录。 组蛋白修饰也可以发生共同作用 组蛋白修饰在水稻基因表达中的调控机制 组蛋白乙酰化 histone lysine acetylation： dynamic reversible switch for interconversion between permissive and repressive transcriptional states of chromatin domains. dynamic and reversible changes of histone H3K4 methylation and H3 acetylation in responses to environmental changes in rice. （submergence-inducible genes ） 核小体乙酰化动态平衡：histone acetyltransferases and histone deacetylases (HDAC) 组蛋白乙酰转移酶和组蛋白去乙酰化酶 Histone Modification in rice 翻译后在组蛋白N末端发生的修饰，比如，乙酰化，甲基化，磷酸化和泛素化，在表观遗传调控中有重要的作用 翻译后的修饰影响力更高级的染色质结构，调控多种生物进程，包括基因表达，DNA复制，DNA损伤的应答，细胞凋亡 Histone acetylation Histone lysine acetylation —— transcriptional activation 转录激活 histone acetyltransferases（HATs）（8） classes CBP, TAFII250, GNAT (Gcn5- related N-acetyltransferases), MYST (forMOZ, Ybf2, Sas2, and Tip60) OsglHAT1：grain-weight quantitative trait locus (QTL)that encodes a newtype of GNAT-like HAT，OsglHAT1的过表达增加了粒重和产量通过增加细胞数和谷物填充，组蛋白H4全基因组范围内的乙酰化水平 histone deacetylases（HDACs）（18） 调控植物生长发育，参与生物和非生物的胁迫应答 families RPD3/HDA1, SIR2 (SILENT INFORMATION REGULATOR 2), HD2 OsHDA703:该基因表达的下调减少了水稻花梗的伸长和受精， OsHDA704，OsHDA710和OsHFT702的下调导致营养生长发育过程中的不正常现象 OsHDA702/OsHDAC1 的错误调控导致生长率的改变 OsNAC6的表观调控导致茎和根组织生长缺陷的产生 OsHDT701，HD2 family，通过降低水稻中的模式识别受体（PRR）的H4组蛋白乙酰化的水平来负调节先天免疫的效应 OsSRT701/OsSRT1：SIR2 family 敲除该基因，增加了TEs和与细胞死亡，胁迫和代谢有关的基因的组蛋白H3K9的乙酰化水平 干旱胁迫显著诱导水稻HAT基因（OsHAC703, OsHAG703, OsHAF701, and OsHAM701）的表达 OsHDT701的过表达会增强盐和渗透压胁迫的耐受性 Genome-wide profiling of histone acetylation 64.9%的non-TE genes H3K9ac修饰，在TSS的下游富集，与基因的激活有关 H3K27ac主要在TSS下游累积，与H3K9ac高度一致 Histone methylation Histone lysine methylation methylation of histone H3K4 and K36 —— transcriptional activation methylation of histone H3K9 and K27—— transcriptional repression Histone lysine methyltransferases (HKMTs) SET domain（for Su(var)3-9, E(z), Trithorax） 水稻基因组至少编码了37个包含SET结构域的蛋白 SDG714 (SET DOMAIN GROUP PROTEIN 714)：第一个被鉴定出来的水稻HKMT，特异性催化H3K9甲基化，SDG714的表达下调导致表皮毛（trichome）缺陷，并且Tos17反转座子的H3K9甲基化和DNA甲基化减少，这促进了Tos17的转录和转座，证明了SDG714在调控植物发育和维持基因组稳定性方面有重要作用；SDG714介导的H3K9甲基化抑制了Tos17和Ty1-copia元件的转录和转座 SDG724 ，SDG725：催化H3K36的甲基化，在水稻开花基因（MADS50 and RICE FLOWERING LOCUS T1 (RFT1)）发生H3K36me2/3修饰能够促进开花 Polycomb group (PcG) proteins：催化H3K27的甲基化 Trithorax group (TrxG) proteins：催化H3K4的甲基化 PcG和TrxG蛋白的功能在植物和动物当中都是高度保守的 SDG723/OsTrx1：rice TrxG 蛋白，开花的激活子 POLYCOMB REPRESSIVE COMPLEX 2 (PRC2)： 四个核心蛋白：ENHANCER of ZESTE [E(z)], SUPPRESSOR of ZESTE 12 [Su(z)12], EXTRA SEX COMBS (ESC), and p55 catalytic component ：the SET-domain containing protein E(z) 介导H3K27me3 水稻基因组编码 两个同源的E(z):(OsiEZ1/SDG718/OsSET1 and OsCLF/SDG711) 两个Su(z)12-like proteins [EMBRYONIC FLOWER 2a (OsEMF2a) and OsEMF2b], 两个同源的ESC：[RTILIZATION-INDEPENDENT ENDOSPERM 1 (OsFIE1) and OsFIE2] 水稻的PRC2目前还有很多功能是未知的 OsEMF2b：隐性突变显示出多效性缺陷，开花时间的改变，异常的开花性质和完全不育性，表明其在花的发育调控中起重要作用 heading date OsFIE1：唯一的印记基因 甲基化稳态的动态调控 demethylases（去甲基化酶） LYSINE-SPECIFIC DEMETHYLASE 1 (LSD1) （3） Jumonji C (JmjC) domain-containing proteins（20） 移除赖氨酸上的甲基化标记 JMJ706能够在体外（in vitro）去除H3K9me2/me3，在体内去除H3K27me2/me3，是水稻开花发育调控基因表达所必须的 JMJ706，JMJ705也能够去除H3K27me2/me3,并且诱导水稻防卫相关基因（defense-related genes）的激活 JMJ703能够特异性地对H3K4me1/me2/me3进行去甲基化， genome-wide maps 一半以上的编码蛋白质基因（非TEs）都有H3K4me2和/或H3K4me3修饰，与H3K9ac高度一致， H3K4me2的水平与基因表达降低的程度呈现正相关， H3K4me3标记的基因能够激活转录，这两种修饰都倾向于发生在5‘末端（TSS的上游） H3K36me3位于genebody与激活基因表达有关，对于等位基因的特异性表达有重要作用 H3K27me3是一个抑制性标记，发生在大约40%的全部gengbody水稻非TEs基因上，这些位置具有低表达水平和高度的组织特异性 H3K9是异染色质标记，并且是抑制重复序列所必需的修饰，着丝粒包含串联重复的卫星序列，有多重表观修饰调控 H3K4me2, H3K4me3, H3K36me3, and H3K4K9ac，euchromatic histone modification，与水稻着丝粒基因区段有关，水稻着丝粒存在常染色质亚结构域 参考资料： https://blog.csdn.net/u011262253/article/details/109957163 https://www.jianshu.com/p/4719b4cfa7d9 ChIP-seq——检测组蛋白修饰 概述 ChIP 指染色质免疫共沉淀技术（Chromatin Immunoprecipitation，ChIP）， seq 指的是二代测序方法 作用：识别蛋白质与DNA互相作用情况 原理：染色质免疫共沉淀 + 二代测序 应用：常用于转录因子结合位点和组蛋白修饰位点的研究 原理 甲醛交联： 使用甲醛将目标蛋白与染色质交联固定起来 DNA分离断裂： 从细胞裂解液分离基因组DNA，通过超声打断DNA为一定长度的小片段 染色质免疫共沉淀： 添加与目标蛋白质特异的抗体，该抗体与目标蛋白形成免疫沉淀免疫结合复合体 纯化DNA： 去交联，纯化DNA即得到染色质免疫沉淀的DNA样本，准备测序 二代测序： 建立DNA文库，用测序仪进行二代测序 结果注释 将测序得到的 DNA 片段（sequenced fragments）匹配到参考基因组 如果在基因组的某个位置蛋白质结合的概率越大，那么该位置检测到的 DNA 片段堆叠的就会越高。 使用空白对照（control） 为什么需要对照组？ 一般检测出的峰值会有背景噪音，也就是会夹杂一些没有用抗体捕获的DNA片段也被测序了。 开放的染色质区域比封闭的区域更容易断裂 序列标签在基因组中分布不均 允许我们在比对的控件中与相同区域进行比较 消除 ENCODE 提供了 Black list的影响 所以会准备空白对照，排除假阳性，对照组有有两种： input DNA：不用任何抗体捕获的DNA； mock IP DNA：用不含有抗体的DNA 利用对照组去除实验组中的背景噪音，就会让我们检测到的峰更明显。 将覆盖到参考基因组的DNA片段堆叠用柱状图画出来，就会看到峰。 ChIP-seq是可以检测转录因子的结合，也可以检测组蛋白修饰的。而且二者有着截然不同的峰形： 转录因子结合的特征峰： 组蛋白修饰结合的特征峰： 当然我们也可以使用，UCSC基因组浏览器显示。 影响因素 1.免疫共沉淀的影响 高效特异性抗体 起始物料量 ChIP DNA产量取决于多种因素 细胞类型 标记或蛋白质丰富程度（组蛋白比TF具有更高的结合覆盖率） 抗体质量 对于组蛋白，使用来自T细胞的20ug染色质DNA作为起始材料，总共会得到15-50ng DNA。 对于TF，通常从2500万个细胞（200ug染色质）中得到5-25ng。 -Subhash Tripathi，ResearchGate 染色质片段 片段大小：影响ChIP-seq中的信噪比 因细胞类型而异 偏向启动子区域的片段会在ChIP AND对照（输入）样品中的启动子上引起ChIP-seq富集 2.测序的影响 Reads 长度 较长的 Reads 和双末端 Reads 可提高匹配率 仅对于等位基因特异性染色质事件，转座因子研究而言是必需的 避免分批次 测序深度（最小5-10M；对于转录因子，标准为20-40M；对于组蛋白修饰宽谱图则更高） 序列输入对照的深度等于或大于IP样本 3.测序深度的影响 H3K4me3 H3K27me3 从每个子样本中H3K4me3，H3K36me3和H3K27me3回收的全部数据中获得的显著富集区域的百分比 对另外100万条 Reads 进行测序时，捕获的富集区域增加的百分比 eChIP(enhanced chromatin immunoprecipitation) 改进主要体现在第三步DNA分离裂解 eChIP采用直接将裂解的原浆用超声波裂解的方法 regular ChIP匀浆需要先通过筛网过来然后才能对分离的细胞核进行超声波裂解 "},"背景知识/深度学习/":{"url":"背景知识/深度学习/","title":"深度学习","keywords":"","body":"深度学习 "},"背景知识/深度学习/Pytorch.html":{"url":"背景知识/深度学习/Pytorch.html","title":"Pytorch","keywords":"","body":" Pytorch PyTorch是一个基于Torch的Python开源机器学习库，用于自然语言处理等应用程序。它主要由Facebookd的人工智能小组开发，不仅能够 实现强大的GPU加速，同时还支持动态神经网络，这一点是现在很多主流框架如TensorFlow都不支持的。 PyTorch提供了两个高级功能： 1.具有强大的GPU加速的张量计算（如Numpy） 2.包含自动求导系统的深度神经网络 除了Facebook之外，Twitter、GMU和Salesforce等机构都采用了PyTorch。 基本概念 参考资料： Pytorch官方文档（英文最新版） Pytorch官方教程（中文版） Pytorch1.7中文文档&教程 应用实战 动手学深度学习（Pytorch版） Deeplizard《Pytorch神经网络高效入门教程》中文字幕版 "},"背景知识/深度学习/CNN.html":{"url":"背景知识/深度学习/CNN.html","title":"CNN","keywords":"","body":"CNN 卷积神经网络（Convolutional Neural Networks, CNN）是一类包含卷积计算且具有深度结构的前馈神经网络（Feedforward Neural Networks），由若干个卷积层和池化层组成，是深度学习（deep learning）的代表算法之一，尤其在图像处理方面的表现十分出色。 1962年，Hubel和Wiesel 通过对猫脑视觉皮层的研究，首次提出了一种新的概念“感受野”，这对后来人工神经网络的发展有重要的启示作用。感受野（Receptive Field）是卷积神经网络每一层输出的特征图（feature map）上的像素点在输入图片上映射的区域大小。再通俗点的解释是，特征图上的一个点对应输入图上的区域。 1980年，Fukushima基于生物神经学的感受野理论提出了神经认知机和权重共享的卷积神经层，这被视为卷积神经网络的雏形。 1989年，LeCun结合反向传播算法与权值共享的卷积神经层发明了卷积神经网络，并首次将卷积神经网络成功应用到美国邮局的手写字符识别系统中。 1998年，LeCun提出了卷积神经网络的经典网络模型LeNet-5，并再次提高手写字符识别的正确率。 基本结构 CNN的基本结构由输入层、卷积层（convolutional layer）、ReLU层（Rectified Linear Units layer）、池化层（pooling layer，也称为取样层）、全连接层及输出层构成。卷积层和池化层一般会取若干个，采用卷积层和池化层交替设置，即一个卷积层连接一个池化层，池化层后再连接一个卷积层，依此类推。由于卷积层中输出特征图的每个神经元与其输入进行局部连接，并通过对应的连接权值与局部输入进行加权求和再加上偏置值，得到该神经元输入值，该过程等同于卷积过程，CNN也由此而得名。 卷积层（Convolution Layer） 卷积层是构建卷积神经网络的核心层，它产生了网络中大部分的计算量。卷积神经网路中每层卷积层由若干卷积单元组成，每个卷积单元的参数都是通过反向传播算法优化得到的。卷积运算的目的是提取输入的不同特征，第一层卷积层可能只能提取一些低级的特征如边缘、线条和角等层级，更多层的网络能从低级特征中迭代提取更复杂的特征。 局部感知（local Connectivity） 普通神经网络把输入层和隐藏层进行“全连接(Full Connected)“的设计。从计算的角度来讲，相对较小的图像从整幅图像中计算特征是可行的。但是，如果是更大的图像（如 96x96 的图像），要通过这种全连接网络的这种方法来学习整幅图像上的特征，从计算角度而言，将变得非常耗时。你需要设计 10 的 4 次方（=10000）个输入单元，假设你要学习 100 个特征，那么就有 10 的 6 次方个参数需要去学习。与 28x28 的小块图像相比较， 96x96 的图像使用前向输送或者后向传导的计算方式，计算过程也会慢 10 的 2 次方（=100）倍。 所以如果用全连接神经网络处理大尺寸图像具有三个明显的缺点： （1）首先将图像展开为向量会丢失空间信息； （2）其次参数过多效率低下，训练困难； （3）同时大量的参数也很快会导致网络过拟合。 卷积层解决这类问题的一种简单方法是对隐含单元和输入单元间的连接加以限制：每个隐含单元仅仅只能连接输入单元的一部分。例如，每个隐含单元仅仅连接输入图像的一小片相邻区域。（对于不同于图像输入的输入形式，也会有一些特别的连接到单隐含层的输入信号“连接区域”选择方式。如音频作为一种信号输入方式，一个隐含单元所需要连接的输入单元的子集，可能仅仅是一段音频输入所对应的某个时间段上的信号。) 每个隐含单元连接的输入区域大小叫r神经元的感受野(receptive field)。 由于卷积层的神经元也是三维的，所以也具有深度。卷积层的参数包含一系列过滤器（filter）（卷积核/Kernel），每个过滤器训练一个深度，有几个过滤器输出单元就具有多少深度。 具体如下图所示，样例输入单元大小是32×32×3, 输出单元的深度是5, 对于输出单元不同深度的同一位置，与输入图片连接的区域是相同的，但是参数（过滤器）不同。 虽然每个输出单元只是连接输入的一部分，但是值的计算方法是没有变的，都是权重和输入的点积，然后加上偏置，这点与普通神经网络是一样的 参数（权值）共享（Parameter Sharing） 应用参数共享可以大量减少参数数量，参数共享基于一个假设：如果图像中的一点（x1, y1）包含的特征很重要，那么它应该和图像中的另一点（x2, y2）一样重要。换种说法，我们把同一深度的平面叫做深度切片(depth slice)（(e.g. a volume of size [55x55x96] has 96 depth slices, each of size [55x55])），那么同一个切片应该共享同一组权重和偏置。我们仍然可以使用梯度下降的方法来学习这些权值，只需要对原始算法做一些小的改动， 这里共享权值的梯度是所有共享参数的梯度的总和。 我们不禁会问为什么要权重共享呢？一方面，重复单元能够对特征进行识别，而不考虑它在可视域中的位置。另一方面，权值共享使得我们能更有效的进行特征抽取，因为它极大的减少了需要学习的自由变量的个数。通过控制模型的规模，卷积网络对视觉问题可以具有很好的泛化能力。 神经元的空间排列（Spatial arrangement） 一个输出单元的大小有以下三个量控制：depth, stride 和 zero-padding。 深度(depth) : 卷积核的个数，顾名思义，它控制输出单元的深度，也就是filter的个数，连接同一块区域的神经元个数。又名：depth column 步幅(stride)：它控制在同一深度的相邻两个隐含单元，与他们相连接的输入区域的距离。如果步幅很小（比如 stride = 1）的话，相邻隐含单元的输入区域的重叠部分会很多; 步幅很大则重叠区域变少。 补零(zero-padding) ： 我们可以通过在输入单元周围补零来改变输入单元整体大小，从而控制输出单元的空间大小 感受野（receptive field）：Kernel size 卷积操作（Convolution） 虽然卷积层得名于卷积（convolution）运算，但我们通常在卷积层中使用更加直观的互相关（cross-correlation）运算。在二维卷积层中，一个二维输入数组和一个二维核（kernel）数组通过互相关运算输出一个二维数组。 我们用一个具体例子来解释二维互相关运算的含义。如图5.1所示，输入是一个高和宽均为3的二维数组。我们将该数组的形状记为（3，3）。核数组的高和宽分别为2。该数组在卷积计算中又称卷积核或过滤器（filter）。卷积核窗口（又称卷积窗口）的形状取决于卷积核的高和宽，即2×2 下图阴影部分为第一个输出元素及其计算所使用的输入和核数组元素：0×0+1×1+3×2+4×3=19 池化层（Pooling Layer） 通常在卷积层之后会得到维度很大的特征，将特征切成几个区域，取其最大值或平均值，得到新的、维度较小的特征。 池化（pool）即下采样（downsamples），目的是为了减少特征图。池化操作对每个深度切片独立，规模一般为 2＊2，相对于卷积层进行卷积运算，池化层进行的运算一般有以下几种： 最大池化（Max Pooling）。取4个点的最大值。这是最常用的池化方法。 均值池化（Mean Pooling）。取4个点的均值。 高斯池化。借鉴高斯模糊的方法。不常用。 可训练池化。训练函数 ff ，接受4个点为输入，出入1个点。不常用。 最常见的池化层是规模为2*2， 步幅为2，对输入的每个深度切片进行下采样。每个MAX操作对四个数进行，如下图所示： 池化操作将保存深度大小不变。 如果池化层的输入单元大小不是二的整数倍，一般采取边缘补零（zero-padding）的方式补成2的倍数，然后再池化。 全连接层（Fully-Connected Layer） 把所有局部特征结合变成全局特征，用来计算最后每一类的得分。 对于任意一个卷积层，要把它变成全连接层只需要把权重变成一个巨大的矩阵，其中大部分都是0 除了一些特定区块（因为局部感知），而且好多区块的权值还相同（由于权重共享）。 相反地，对于任何一个全连接层也可以变为卷积层。比如，一个K＝4096 的全连接层，输入层大小为 7∗7∗512，它可以等效为一个 F=7, P=0, S=1, K=4096F=7, P=0, S=1, K=4096 的卷积层。换言之，我们把 filter size 正好设置为整个输入层大小。 实战应用 Pytorch实现 详见构建基本模型（pytorch） 生物交叉应用案例 随着高通量技术的发展，大量的组学数据产出，不断地挑战现有的传统分析方法，如何将这些生物数据转化为有价值的知识是生物信息学面临的重要问题之一，自本世纪初以来，深度学习得到了迅速发展，如今在各个领域都展现了最先进的表现，在生物信息学领域，深度学习目前的应用也已经比较广泛。 卷积神经网络（CNNs）因其出色的空间信息分析能力而成为最成功的图像处理深度学习模型之一。CNNs在基因组学中的应用依赖于卷积层对于图像特征的提取。Zeng等人描述了通过将基因组序列的一个bin理解为图像来实现CNNs从计算机视觉领域到基因组学的迁移(Zeng et al 2016)。 卷积神经网络的亮点是训练过程中自适应特征提取的灵活性。例如，CNNs可用于发现有意义的小变异重复模式，如基因组序列基序，这使得CNNs适用于motif识别和序列的分类。Zhou和Troyanskaya等人早在2015年就通过基于深度学习的算法模型DeepSEA实现了预测单核苷酸敏感性序列的改变的染色质效应，并且通过电子诱变可以解析任何序列中的信息特征，为研究单核苷酸多态性的功能效应提供了有效的方法（Zhou and Troyanskaya 2015）。Xiong等人则研究出了Deep Bayes模型，该模型仅使用DNA序列进行训练，能够准确地对致病变异体进行分类，并对异常剪接在疾病中的作用提供新的假设（Xiong et al 2015）。Alipanahi等人和Zeng等人成功应用CNNs对蛋白质结合的序列特异性进行建模学习（Alipanahi et al 2015），这些应用都已经证明CNNs在针对基序识别的问题上处于领先地位。此外递归神经网络（RNNs）和自动编码器（autoencoder）也在不同的生物领域展现了很优秀的性能。 随着深度学习在基因组学中不断显示出成功，研究人员期望深度学习比简单的统计或机器学习方法具有更高的准确性。为此，当今绝大多数的工作都是从超越经典深度架构的更高级模型，或者采用混合模型来处理基因组问题。Angermueller等人利用两个CNN子模型和一个融合模块来预测DNA甲基化状态。CNN的两个子模型采用不同的输入，因此专注于不同的目的。CpG模块拟合细胞内和跨细胞的CpG位点之间的相关性，而DNA模块检测信息序列模式(基序)（Angermueller et al 2017）。DanQ 是一种混合卷积和递归深度神经网络，用于直接从序列中预测非编码DNA的功能。脱氧核糖核酸序列作为四个碱基的onehot表示输入到一个简单的CNN中，目的是扫描基序位点（Quang and Xie 2016）。 以下两个github项目是对于深度学习在生物领域的应用进行的总结： deeplearning-biology Deep Learning for Genomics: A Concise Overview 接下来主要对本次项目中使用的两个CNN进行结构解析： DeepSEA class DeepSEA(nn.Module): def __init__(self, sequence_length=500, n_genomic_features=2): \"\"\" Parameters ---------- sequence_length : int n_genomic_features : int \"\"\" super(DeepSEA, self).__init__() conv_kernel_size = 8 pool_kernel_size = 4 self.conv_net = nn.Sequential( nn.Conv1d(4, 320, kernel_size=conv_kernel_size), nn.ReLU(inplace=True), nn.MaxPool1d( kernel_size=pool_kernel_size, stride=pool_kernel_size), nn.Dropout(p=0.2), nn.Conv1d(320, 480, kernel_size=conv_kernel_size), nn.ReLU(inplace=True), nn.MaxPool1d( kernel_size=pool_kernel_size, stride=pool_kernel_size), nn.Dropout(p=0.2), nn.Conv1d(480, 960, kernel_size=conv_kernel_size), nn.ReLU(inplace=True), nn.Dropout(p=0.5)) reduce_by = conv_kernel_size - 1 self.n_channels = int( np.floor( (np.floor( (sequence_length - reduce_by) / pool_kernel_size) - reduce_by) / pool_kernel_size) - reduce_by) self.classifier = nn.Sequential( nn.Linear(960 * self.n_channels, n_genomic_features), nn.ReLU(inplace=True), nn.Linear(n_genomic_features, n_genomic_features), nn.Sigmoid() ) def forward(self, x): \"\"\"Forward propagation of a batch. \"\"\" out = self.conv_net(x) reshape_out = out.view(out.size(0), 960 * self.n_channels) predict = self.classifier(reshape_out) return predict DeepSEA( (conv_net): Sequential( (0): Conv1d(4, 320, kernel_size=(8,), stride=(1,)) (1): ReLU(inplace=True) (2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False) (3): Dropout(p=0.2, inplace=False) (4): Conv1d(320, 480, kernel_size=(8,), stride=(1,)) (5): ReLU(inplace=True) (6): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False) (7): Dropout(p=0.2, inplace=False) (8): Conv1d(480, 960, kernel_size=(8,), stride=(1,)) (9): ReLU(inplace=True) (10): Dropout(p=0.5, inplace=False) ) (classifier): Sequential( (0): Linear(in_features=21120, out_features=2, bias=True) (1): ReLU(inplace=True) (2): Linear(in_features=2, out_features=2, bias=True) (3): Sigmoid() ) ) Conv_net （0）一维卷积层 Conv1d https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html?highlight=conv1d#torch.nn.Conv1d class torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) in_channels(int) – 输入信号的通道。在文本分类中，即为词向量的维度 out_channels(int) – 卷积产生的通道。有多少个out_channels，就需要多少个1维卷积 kernel_size(int or tuple) - 卷积核的尺寸，卷积核的大小为(k,)，第二个维度是由in_channels来决定的，所以实际上卷积大小为kernel_size*in_channels stride(int or tuple, optional) - 卷积步长 padding (int or tuple, optional)- 输入的每一条边补充0的层数 dilation(int or tuple, `optional``) – 卷积核元素之间的间距 groups(int, optional) – 从输入通道到输出通道的阻塞连接数 bias(bool, optional) - 如果bias=True，添加偏置 Conv1d(4, 320, kernel_size=(8,), stride=(1,)) （1）激活函数 ReLU层 https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html?highlight=relu#torch.nn.ReLU ReLU(inplace=True) class torch.nn.ReLU(inplace=False) inplace参数，如果inplace = True，则输出会直接覆盖到输入中，可以节省显存 ReLU(x)=max(0, x) 从ReLU函数图像可知，它是分段线性函数，所有的负值和0为0，所有的正值不变，这种操作被称为单侧抑制。ReLU函数图像其实也可以不是这个样子，只要能起到单侧抑制的作用，对原图翻转、镜像都可以。 当训练一个深度分类模型的时候，和目标相关的特征往往也就那么几个，因此通过ReLU实现稀疏后的模型能够更好地挖掘相关特征，拟合训练数据。正因为有了这单侧抑制，才使得神经网络中的神经元也具有了稀疏激活性。尤其体现在深度神经网络模型(如CNN)中，当模型增加N层之后，理论上ReLU神经元的激活率将降低2的N次方倍。 不用simgoid和tanh作为激活函数，而用ReLU作为激活函数的原因是：加速收敛。因为sigmoid和tanh都是饱和(saturating)的。何为饱和？可理解为把这两者的函数曲线和导数曲线plot出来：他们的导数都是倒过来的碗状，也就是越接近目标，对应的导数越小。而ReLu的导数对于大于0的部分恒为1。于是ReLU确实可以在BP的时候能够将梯度很好地传到较前面的网络。 (2): 池化层 MaxPool1d https://pytorch.org/docs/stable/generated/torch.nn.MaxPool1d.html?highlight=maxpool1d#torch.nn.MaxPool1d class torch.nn.MaxPool1d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) kernel_size(int or tuple) - max pooling的窗口大小 stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size padding(int or tuple, optional) - 输入的每一条边补充0的层数 dilation(int or tuple, optional) – 一个控制窗口中元素步幅的参数 return_indices - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助 ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False) (3): 正则化层 Dropout https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html?highlight=dropout#torch.nn.Dropout class torch.nn.Dropout(p=0.5, inplace=False) p:被舍弃的概率，失活概率 inplace:输出是否覆盖输入 Dropout(p=0.2, inplace=False) 后面的六层与前面四层是重复的 CONV1D -> RELU -> MaxPool1d -> Dropout ​ (4): Conv1d(320, 480, kernel_size=(8,), stride=(1,)) ​ (5): ReLU(inplace=True) ​ (6): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False) ​ (7): Dropout(p=0.2, inplace=False) ​ (8): Conv1d(480, 960, kernel_size=(8,), stride=(1,)) ​ (9): ReLU(inplace=True) ​ (10): Dropout(p=0.5, inplace=False) classifier 对卷积层输出的结果进行reshape，最终得到二元分类的预测值 (0): Linear(in_features=21120, out_features=2, bias=True) (1): ReLU(inplace=True) (2): Linear(in_features=2, out_features=2, bias=True) (3): Sigmoid() DeepHsitone import numpy as np import torch import torch.nn as nn import torch.nn.functional as F import torch.optim as optim from torch.autograd import Variable from sklearn import metrics from torch.optim import Optimizer import math from torch.nn.parameter import Parameter class BasicBlock(nn.Module): def __init__(self, in_planes, grow_rate,): super(BasicBlock, self).__init__() self.block = nn.Sequential( nn.BatchNorm2d(in_planes), nn.ReLU(), nn.Conv2d(in_planes, grow_rate, (1,9), 1, (0,4)), #nn.Dropout2d(0.2) ) def forward(self, x): out = self.block(x) return torch.cat([x, out],1) class DenseBlock(nn.Module): def __init__(self, nb_layers, in_planes, grow_rate,): super(DenseBlock, self).__init__() layers = [] for i in range(nb_layers): layers.append(BasicBlock(in_planes + i*grow_rate, grow_rate,)) self.layer = nn.Sequential(*layers) def forward(self, x): return self.layer(x) class ModuleDense(nn.Module): def __init__(self): super(ModuleDense, self).__init__() self.conv1 = nn.Sequential( nn.Conv2d(1,128,(4,9),1,(0,4)), #nn.Dropout2d(0.2), ) self.block1 = DenseBlock(3, 128, 128) self.trans1 = nn.Sequential( nn.BatchNorm2d(128+3*128), nn.ReLU(), nn.Conv2d(128+3*128, 256, (1,1),1), #nn.Dropout2d(0.2), nn.MaxPool2d((1,4)), ) self.block2 = DenseBlock(3,256,256) self.trans2 = nn.Sequential( nn.BatchNorm2d(256+3*256), nn.ReLU(), nn.Conv2d(256+3*256, 512, (1,1),1), #nn.Dropout2d(0.2), nn.MaxPool2d((1,4)), ) self.out_size = 500 // 4 // 4 * 512 def forward(self, seq): n, h, w = seq.size() seq = seq.view(n,1,4,w) out = self.conv1(seq) out = self.block1(out) out = self.trans1(out) out = self.block2(out) out = self.trans2(out) n, c, h, w = out.size() out = out.view(n,c*h*w) return out class NetDeepHistone(nn.Module): def __init__(self): super(NetDeepHistone, self).__init__() print('DeepHistone(Dense) is used.') self.seq_map = ModuleDense() self.seq_len = self.seq_map.out_size seq_len = self.seq_len self.linear_map = nn.Sequential( nn.Dropout(0.5), nn.Linear(int(seq_len),925), nn.BatchNorm1d(925), nn.ReLU(), #nn.Dropout(0.1), nn.Linear(925,5), nn.Sigmoid(), ) def forward(self, seq): flat_seq = self.seq_map(seq) out = self.linear_map(flat_seq) return out class DeepHistone(): def __init__(self,use_gpu,learning_rate=0.001): self.forward_fn = NetDeepHistone() self.criterion = nn.BCELoss() self.optimizer = optim.Adam(self.forward_fn.parameters(), lr=learning_rate, weight_decay = 0) self.use_gpu = use_gpu if self.use_gpu : self.criterion,self.forward_fn = self.criterion.cuda(), self.forward_fn.cuda() def updateLR(self, fold): for param_group in self.optimizer.param_groups: param_group['lr'] *= fold def train_on_batch(self,seq_batch,lab_batch,): self.forward_fn.train() seq_batch = Variable(torch.Tensor(seq_batch)) lab_batch = Variable(torch.Tensor(lab_batch)) if self.use_gpu: seq_batch, lab_batch = seq_batch.cuda(), lab_batch.cuda() output = self.forward_fn(seq_batch) loss = self.criterion(output,lab_batch) self.optimizer.zero_grad() loss.backward() self.optimizer.step() return loss.cpu().data def eval_on_batch(self,seq_batch,lab_batch,): self.forward_fn.eval() seq_batch = Variable(torch.Tensor(seq_batch)) lab_batch = Variable(torch.Tensor(lab_batch)) if self.use_gpu: seq_batch, lab_batch = seq_batch.cuda(), lab_batch.cuda() output = self.forward_fn(seq_batch) loss = self.criterion(output,lab_batch) return loss.cpu().data,output.cpu().data.numpy() def test_on_batch(self, seq_batch): self.forward_fn.eval() seq_batch = Variable(torch.Tensor(seq_batch)) if self.use_gpu: seq_batch = seq_batch.cuda() output = self.forward_fn(seq_batch) pred = output.cpu().data.numpy() return pred def save_model(self, path): torch.save(self.forward_fn.state_dict(), path) def load_model(self, path): self.forward_fn.load_state_dict(torch.load(path)) NetDeepHistone( (seq_map): ModuleDense( (conv1): Sequential( (0): Conv2d(1, 128, kernel_size=(4, 9), stride=(1, 1), padding=(0, 4)) ) (block1): DenseBlock( (layer): Sequential( (0): BasicBlock( (block): Sequential( (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() (2): Conv2d(128, 128, kernel_size=(1, 9), stride=(1, 1), padding=(0, 4)) ) ) (1): BasicBlock( (block): Sequential( (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() (2): Conv2d(256, 128, kernel_size=(1, 9), stride=(1, 1), padding=(0, 4)) ) ) (2): BasicBlock( (block): Sequential( (0): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() (2): Conv2d(384, 128, kernel_size=(1, 9), stride=(1, 1), padding=(0, 4)) ) ) ) ) (trans1): Sequential( (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() (2): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1)) (3): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False) ) (block2): DenseBlock( (layer): Sequential( (0): BasicBlock( (block): Sequential( (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() (2): Conv2d(256, 256, kernel_size=(1, 9), stride=(1, 1), padding=(0, 4)) ) ) (1): BasicBlock( (block): Sequential( (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() (2): Conv2d(512, 256, kernel_size=(1, 9), stride=(1, 1), padding=(0, 4)) ) ) (2): BasicBlock( (block): Sequential( (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() (2): Conv2d(768, 256, kernel_size=(1, 9), stride=(1, 1), padding=(0, 4)) ) ) ) ) (trans2): Sequential( (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU() (2): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1)) (3): MaxPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0, dilation=1, ceil_mode=False) ) ) (linear_map): Sequential( (0): Dropout(p=0.5, inplace=False) (1): Linear(in_features=15872, out_features=925, bias=True) (2): BatchNorm1d(925, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU() (4): Linear(in_features=925, out_features=5, bias=True) (5): Sigmoid() ) ) seq_map 二维卷积层 conv2d class torch.nn.Conv2d(in_channels,out_channels,kernel_size,stride=1,padding=0,dilation=1,groups=,bias=True,padding_mode='zero') in_channels:就是输入的四维张量[N, C, H, W]中的C了，即输入张量的channels数。这个形参是确定权重等可学习参数的shape所必需的。 out_channels:也很好理解，即期望的四维输出张量的channels数，不再多说。 kernel_size:卷积核的大小，一般我们会使用5x5、3x3这种左右两个数相同的卷积核，因此这种情况只需要写kernel_size = 5这样的就行了。如果左右两个数不同，比如3x5的卷积核，那么写作kernel_size = (3, 5)，注意需要写一个tuple，而不能写一个列表（list）。 stride = 1:卷积核在图像窗口上每次平移的间隔，即所谓的步长。这个概念和Tensorflow等其他框架没什么区别，不再多言。 padding = 0:Pytorch与Tensorflow在卷积层实现上最大的差别就在于padding上。   Padding即所谓的图像填充，后面的int型常数代表填充的多少（行数、列数），默认为0。需要注意的是这里的填充包括图像的上下左右，以padding = 1为例，若原始图像大小为32x32，那么padding后的图像大小就变成了34x34，而不是33x33。   Pytorch不同于Tensorflow的地方在于，Tensorflow提供的是padding的模式，比如same、valid，且不同模式对应了不同的输出图像尺寸计算公式。而Pytorch则需要手动输入padding的数量，当然，Pytorch这种实现好处就在于输出图像尺寸计算公式是唯一的，即    当然，上面的公式过于复杂难以记忆。大多数情况下的kernel_size、padding左右两数均相同，且不采用空洞卷积（dilation默认为1），因此只需要记 O = （I - K + 2P）/ S +1这种在深度学习课程里学过的公式就好了。 dilation = 1:这个参数决定了是否采用空洞卷积，默认为1（不采用）。从中文上来讲，这个参数的意义从卷积核上的一个参数到另一个参数需要走过的距离，那当然默认是1了，毕竟不可能两个不同的参数占同一个地方吧（为0）。 groups = 1:决定了是否采用分组卷积，groups参数可以参考groups参数详解 bias = True:即是否要添加偏置参数作为可学习参数的一个，默认为True。 padding_mode = ‘zeros’:即padding的模式，默认采用零填充。 Dense block 归一化 BatchNorm2d() class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) num_features：一般输入参数为batch_sizenum_featuresheight*width，即为其中特征的数量 eps：分母中添加的一个值，目的是为了计算的稳定性，默认为：1e-5 momentum：一个用于运行过程中均值和方差的一个估计参数（我的理解是一个稳定系数，类似于SGD中的momentum的系数） affine：当设为true时，会给定可以学习的系数矩阵gamma和beta 二维池化 MaxPool2d() class torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False) kernel_size(int or tuple) - max pooling的窗口大小， stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是kernel_size padding(int or tuple, optional) - 输入的每一条边补充0的层数 dilation(int or tuple, optional) – 一个控制窗口中元素步幅的参数 return_indices - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助 ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作 trans linear_map (0): Dropout(p=0.5, inplace=False) (1): Linear(in_features=15872, out_features=925, bias=True) (2): BatchNorm1d(925, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): ReLU() (4): Linear(in_features=925, out_features=5, bias=True) (5): Sigmoid() "},"背景知识/深度学习/Transformer.html":{"url":"背景知识/深度学习/Transformer.html","title":"Transformer","keywords":"","body":"Transformer Encoder 1 输入部分 Embedding word2vec 自动初始化 位置编码 RNN：所有的time steps共享一套参数，只更新一套UWV RNN的梯度消失：总梯度和被近距离梯度主导，被远距离梯度忽略不计 天然的时序关系非常符合 Transformer是可以并行化的，所有的单词一起处理，不考虑时序关系，增加了速度，忽略了顺序关系，所以需要位置编码 位置编码公式 偶数位置使用sin，奇数位置使用cos 拓展 位置嵌入的作用 同一个位置，使用sin和cos可以表示绝对位置，绝对位置向量信息中包含着相对位置向量信息 2 注意力机制 基本的注意力机制 人类在观察一张图片时，肯定会有注意力的差别，如图颜色越深代表对此部分的注意力越大，更加关注该区域 比如说提出问题“婴儿在干嘛”，我们需要提取图片中和该问题有关的信息 注意力机制公式 QKV三个矩阵最关键，Q（Query：查询向量）K（Key：键向量）V（Value：值向量） 第一步：Q和K做点乘，得到s 相似度计算：点乘，MLP（网络），cos相似性 点乘：一个向量在另一个向量上投影的长度，是一个标量，可以反映两个向量之间的相似度，点乘结果越大说明距离越近越相似更关注 第二步：对s进行类Softmax()归一化，生成a（相似度，相加为一） 第三步：对V进行加权平均，相加相乘，得到最终结果Attention value Transformer中的注意力机制 如何获取QKV 初始的词向量X，与对应的W矩阵（权重矩阵，可以随机初始化迭代更新）相乘，获取QKV三个向量 Divide by 8 根号下dk ：由于Softmax本身梯度很小，但是S值很大，所以容易发生梯度消失，才需要进行此处理 实际代码使用矩阵进行并行操作 多头注意力机制 相当于将原始数据使用不同的参数在多个空间内进行并行计算， 最后将多个head 的多个输出Z共同输出 残差和LayNorm 处理流程：将词向量x进行位置编码，生成新的词向量X，作为输入，经过self-attention处理，输出Z，然后用Z和X进行对位相加作为残差结果，再进行LayerNorm()处理 残差 为什么残差结构会有用？ 连乘应为1+ (のXc/のXb)*(のXb/のXaout) 确保了梯度不会为零，缓解梯度消失，可以使网络加深 Layer Normalization 为什么使用LN而不使用BN？ BatchNormlization：BN在NLP任务中效果很差，BN本身的作用是在存在多个特征的情况下，对多个特征进行归一化使其能迅速收敛，可以解决内部协变量偏移，缓解了梯度饱和问题，加快收敛 BN针对一个batch中的所有样本的所有特征进行Normliaze，每个特征都是对应的， 缺点： batch_size较小时，效果差，使用一个batch中所有样本的均值和方差来模拟整体的均值和方差，但是如果样本很小不具有代表性 在NLP中，RNN的输入是动态的，与时序有关，如果是一个1*20的向量，就不能用很小的batch来代表整体 LayerNormalization：针对一个样本的所有单词做缩放（均值方差），BN是针对每个单词的不同特征进行缩放，不适合NLP https://academic.oup.com/bib/ 3 Feed Forward前馈神经网络 两层的全连接 ＋ 残差LN Decoder 1 Masked Multi-Head Attention 需要对当前单词和之后的单词做Mask（掩盖） 为什么需要Mask？ 如果不进行mask，在进行训练时，是由所有的单词共同提供的信息进行计算，但是预测时不能看到未来时刻的单词，会产生训练和预测的gap，所以需要掩盖后面的两个单词提供的信息 2 交互层 所有的Encoder生成一个输出，然后这一个输出对每个Decoder进行交互 Encoder生成KV矩阵，Decoder生成Q矩阵，交互层Q矩阵来自于本身，KV矩阵来自于Encoders，进行多头注意力机制 Nt_Transformer K-mers with 1-D convolutions 生物信息学处理DNA序列常用方法：K-mers，就是将一条完整的DNA序列，转化为长度为k的小片段，和1-D卷积层利用slide window提取信息的方法类似 S ：sequence vector(length = l) K : convolution kernel (size = 3) O: output of vector dot（点乘） p：position stride = 1 步长为1 ，每次移动一个单位 使用一个长为3的一维卷积核对序列S做点乘（卷积），从一条DNA序列中提取3-mers的有效信息 核苷酸编码：首先将每个核苷酸转化为embeddings嵌入层（fixed size = dmodel）词向量，对于每条DNA序列产生一个tensor （l，dmodel） 位置编码：采用Transformer中对于词向量的编码方式，奇数使用cos，偶数使用sin，生成K作为整体输入，既包含核苷酸编码的信息又包含相对位置信息 传统k-mers的缺点： 泛化能力差，只能对生成的k-mers组合进行学习，但是生成的k-mers数量有限，比如数据集中有promoter motif TATAAT 但是如果一个相似的motif TATATT不在数据集中就无法进行学习 克服方法： 使用convolutions对序列进行特征提取，能够对TATAAT和TATATT的相似度进行量化，从而更好地进行泛化 大量的参数：4的k次方 Transformer encoder multi-head self-attention mechanism 多头注意力机制能偶线性地将dmodel维度的K，Q，V投射到低维度中进行表示 多头注意力机制能够直接在整条序列上进行操作 多头注意力机制允许不同的head学习输入数据不同的隐藏模式，从而提升性能 详见Transformer self-attention机制能够计算出每个k-mers其他所有k-mers（包括自身）的联系，这样就能建立全局依赖关系（global dependencies） 与CNN和RNN加强模型对于局部区域的保守性（sparse local connectivity）学习的特点不同，Transformer对于整体的依赖关系有着更好的表现 position-wise feedforward network transformer encoder layer：两个transforms中间夹杂着一个ReLU激活函数 多个layers堆叠成一个encoder model=NucleicTransformer(opts.ntoken, opts.nclass, opts.ninp, opts.nhead, opts.nhid, opts.nlayers, opts.kmer_aggregation, kmers=opts.kmers, dropout=opts.dropout).to(device) Parameters default explain gpu_id 0 选择使用的gpu path 数据集存放路径（DNA seq 和labels） epochs 150 batch_size 24 一次训练所抓取的数据样本数量 weight_decay 0 权重衰减（L2正则化，防止过拟合） ntoken 4 表示核苷酸所需要的维度（固定值为4） nclass 2 分类数 ninp 512 nhead 8 multi-head self-attention bhid 1048 mlayers 6 save_freq 1 每训练多少批存储一次结果 dropout 0.1 丢弃概率防止过拟合 warmup_steps 3200 lr_scale 0.1 学习率 nmute 18 突变数 kmers [2，3，4，5，6] n_fold 5 交叉验证 fold 0 选择哪一个fold进行训练 "},"背景知识/深度学习/多模态学习.html":{"url":"背景知识/深度学习/多模态学习.html","title":"多模态学习","keywords":"","body":"多模态学习 基因数据实际上并非只有 DNA 序列这一类遗传方面的数据，还涉及到转录组、表观组修饰、蛋白组等多组学数据，而且数据在彼此之间存在着一定的内在关系。如何处理和整合这些多组学数据就涉及到“多任务和多模态学习”这个问题了。在多模态学习模型的构成中，它有一个总损失函数，它的值是各个模态数据损失函数之和或者加权和，这取决于各个模态之间损失函数的结果是否差异巨大。下面图7.a-c 是一个多任务和多模态学习的示意图。这类模型的训练往往比较困难，因为需要同时优化学习网络中多个不同的损失函数，并且往往还得做出合适的取舍，每一个取舍都要有合理的内在理由。而且如果不同的类型的数据之间，出现了较为严重的权重失衡的话——比如出现”一超无强”的情况，那么最终的模型可能仅能代表一小撮数据的结果，这就会让模型出现严重偏差。 由于当前的技术已经使得来自具有异构特征集的多平台或多视图输入的数据可用，多视图深度学习似乎是未来深度学习研究的一个令人激动的方向，该研究利用跨数据集的信息，捕捉它们的高级关联用于预测、聚类以及处理不完整的数据。在许多应用程序中，我们可以从不同类型的数据中处理相同的问题，例如在音频和视频数据都可用的计算机视觉中。基因组学是一个可以自然吸收各种类型数据的领域。例如，通过最先进的高通量测序技术，同一组肿瘤样品的丰富类型的基因组数据(例如，DNA甲基化、基因表达、miRNA表达数据)已经可以联合使用。因此，自然会想到利用基因组学中的多视图信息来实现比单视图更好的预测。Li等人回顾了一些具有指导意义的多视图生物数据集成方法（Li et al 2016b）。例如，多视图学习可以通过连接特征、集成方法或多模态学习(为每个视图选择特定的深层网络作为主模型的子网络，然后将它们集成到更高层中)来实现。Liang等人提出了一个多模态的综合基因表达，脱氧核糖核酸甲基化，微小核糖核酸和药物反应的数据，以群集癌症患者和确定癌症亚型。它们的叠加高斯约束玻尔兹曼机(gRBM)通过对比发散训练，不同模态通过叠加隐藏层集成，共同特征从多个单一模态的固有特征中流出(Liang et al 2015)。 "},"背景知识/深度学习/迁移学习.html":{"url":"背景知识/深度学习/迁移学习.html","title":"迁移学习","keywords":"","body":"迁移学习 迁移学习是一种解决训练数据稀缺问题的机器学习方法。因为数据稀缺或者数据缺失的情况下，从头训练整个模型可能是不可行的。那么一个取而代之的方法就是使用相似结构的任务，以及由它训练得到的模型的大多数参数来初始化我们的目标模型。你可以理解为，这是一种将先验知识整合到新模型中的机器学习方法，它可以在一定程度上解决训练数据不足的问题。 在基因组学中，远程调控的预测模型就应用到了迁移学习。不过迁移学习在组学方面的应用还缺少深入的研究，比如目前依然不清楚应该如何选择合适的源模型、以及源模型中有哪些参数适合共享到目标模型中等。 生物信息中的迁移学习 "}}